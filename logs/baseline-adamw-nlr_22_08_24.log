Tracking run with wandb version 0.17.7
Run data is saved locally in /content/wandb/run-20240822_214429-ndu8tbwn
Syncing run good-energy-22 to Weights & Biases (docs)
View project at https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers
View run at https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/ndu8tbwn
Start Training
| epoch 1 | 200/2222 batches | lr 0.0010 | ms/batch 30.87 | loss 2.7892 | bpc 4.0440
| epoch 1 | 400/2222 batches | lr 0.0010 | ms/batch 23.87 | loss 2.2220 | bpc 3.2445
| epoch 1 | 600/2222 batches | lr 0.0010 | ms/batch 21.54 | loss 2.0332 | bpc 2.9740
| epoch 1 | 800/2222 batches | lr 0.0010 | ms/batch 20.40 | loss 1.8842 | bpc 2.7557
| epoch 1 | 1000/2222 batches | lr 0.0010 | ms/batch 19.75 | loss 1.7124 | bpc 2.5109
| epoch 1 | 1200/2222 batches | lr 0.0010 | ms/batch 19.33 | loss 1.7143 | bpc 2.5140
| epoch 1 | 1400/2222 batches | lr 0.0010 | ms/batch 19.04 | loss 1.6398 | bpc 2.4030
| epoch 1 | 1600/2222 batches | lr 0.0010 | ms/batch 18.84 | loss 1.8437 | bpc 2.6903
| epoch 1 | 1800/2222 batches | lr 0.0010 | ms/batch 18.69 | loss 1.5314 | bpc 2.2468
| epoch 1 | 2000/2222 batches | lr 0.0010 | ms/batch 18.58 | loss 1.4188 | bpc 2.1113
| epoch 1 | 2200/2222 batches | lr 0.0010 | ms/batch 18.48 | loss 1.4512 | bpc 2.1409
--------------------------------------------------------------------------------
| end of epoch 1 | time: 43.18s | train loss 1.8360 | train bpc 2.6885 | valid loss 8.7252 | valid bpc 12.6192 
--------------------------------------------------------------------------------
Start Training
| epoch 2 | 200/2222 batches | lr 0.0010 | ms/batch 17.39 | loss 1.4842 | bpc 2.1857
| epoch 2 | 400/2222 batches | lr 0.0010 | ms/batch 17.27 | loss 1.4308 | bpc 2.1385
| epoch 2 | 600/2222 batches | lr 0.0010 | ms/batch 17.21 | loss 1.3676 | bpc 2.0407
| epoch 2 | 800/2222 batches | lr 0.0010 | ms/batch 17.17 | loss 1.3557 | bpc 2.0062
| epoch 2 | 1000/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.3027 | bpc 1.9298
| epoch 2 | 1200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.3771 | bpc 2.0346
| epoch 2 | 1400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.3923 | bpc 2.0503
| epoch 2 | 1600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.6916 | bpc 2.4713
| epoch 2 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.3302 | bpc 1.9569
| epoch 2 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.2490 | bpc 1.8658
| epoch 2 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 1.3067 | bpc 1.9313
--------------------------------------------------------------------------------
| end of epoch 2 | time: 39.34s | train loss 1.3890 | train bpc 2.0544 | valid loss 8.5516 | valid bpc 12.3698 
--------------------------------------------------------------------------------
Start Training
| epoch 3 | 200/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.3519 | bpc 1.9926
| epoch 3 | 400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.2938 | bpc 1.9379
| epoch 3 | 600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.2595 | bpc 1.8823
| epoch 3 | 800/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.2521 | bpc 1.8569
| epoch 3 | 1000/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.2108 | bpc 1.7991
| epoch 3 | 1200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.2961 | bpc 1.9201
| epoch 3 | 1400/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 1.3270 | bpc 1.9558
| epoch 3 | 1600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 1.6390 | bpc 2.3932
| epoch 3 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 1.2694 | bpc 1.8675
| epoch 3 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 1.1930 | bpc 1.7816
| epoch 3 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 1.2565 | bpc 1.8593
--------------------------------------------------------------------------------
| end of epoch 3 | time: 39.54s | train loss 1.3041 | train bpc 1.9310 | valid loss 8.0860 | valid bpc 11.6934 
--------------------------------------------------------------------------------
Start Training
| epoch 4 | 200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.2983 | bpc 1.9138
| epoch 4 | 400/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.2436 | bpc 1.8674
| epoch 4 | 600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.2080 | bpc 1.8107
| epoch 4 | 800/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.2055 | bpc 1.7912
| epoch 4 | 1000/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.1671 | bpc 1.7380
| epoch 4 | 1200/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.2554 | bpc 1.8606
| epoch 4 | 1400/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.2947 | bpc 1.9107
| epoch 4 | 1600/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.6092 | bpc 2.3504
| epoch 4 | 1800/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.2356 | bpc 1.8210
| epoch 4 | 2000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1636 | bpc 1.7427
| epoch 4 | 2200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.2275 | bpc 1.8197
--------------------------------------------------------------------------------
| end of epoch 4 | time: 39.68s | train loss 1.2642 | train bpc 1.8749 | valid loss 8.3000 | valid bpc 12.0066 
--------------------------------------------------------------------------------
Start Training
| epoch 5 | 200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.2686 | bpc 1.8734
| epoch 5 | 400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 1.2146 | bpc 1.8280
| epoch 5 | 600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.1813 | bpc 1.7732
| epoch 5 | 800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1764 | bpc 1.7503
| epoch 5 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.1399 | bpc 1.7009
| epoch 5 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.2283 | bpc 1.8234
| epoch 5 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.2714 | bpc 1.8782
| epoch 5 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.5912 | bpc 2.3256
| epoch 5 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.2143 | bpc 1.7915
| epoch 5 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 1.1447 | bpc 1.7170
| epoch 5 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.2103 | bpc 1.7982
--------------------------------------------------------------------------------
| end of epoch 5 | time: 39.56s | train loss 1.2399 | train bpc 1.8416 | valid loss 8.0584 | valid bpc 11.6599 
--------------------------------------------------------------------------------
Start Training
| epoch 6 | 200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.2503 | bpc 1.8493
| epoch 6 | 400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.1920 | bpc 1.7997
| epoch 6 | 600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1598 | bpc 1.7466
| epoch 6 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1549 | bpc 1.7219
| epoch 6 | 1000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1193 | bpc 1.6726
| epoch 6 | 1200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.2096 | bpc 1.7981
| epoch 6 | 1400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.2587 | bpc 1.8611
| epoch 6 | 1600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.5767 | bpc 2.3054
| epoch 6 | 1800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1986 | bpc 1.7700
| epoch 6 | 2000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1301 | bpc 1.6977
| epoch 6 | 2200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1969 | bpc 1.7789
--------------------------------------------------------------------------------
| end of epoch 6 | time: 39.40s | train loss 1.2224 | train bpc 1.8182 | valid loss 8.3122 | valid bpc 12.0288 
--------------------------------------------------------------------------------
Start Training
| epoch 7 | 200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.2344 | bpc 1.8273
| epoch 7 | 400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1763 | bpc 1.7795
| epoch 7 | 600/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1453 | bpc 1.7265
| epoch 7 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1397 | bpc 1.7017
| epoch 7 | 1000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1051 | bpc 1.6515
| epoch 7 | 1200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1982 | bpc 1.7835
| epoch 7 | 1400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.2459 | bpc 1.8437
| epoch 7 | 1600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.5652 | bpc 2.2892
| epoch 7 | 1800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1899 | bpc 1.7573
| epoch 7 | 2000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1190 | bpc 1.6838
| epoch 7 | 2200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1860 | bpc 1.7643
--------------------------------------------------------------------------------
| end of epoch 7 | time: 39.58s | train loss 1.2096 | train bpc 1.8009 | valid loss 8.0884 | valid bpc 11.7050 
--------------------------------------------------------------------------------
Start Training
| epoch 8 | 200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.2209 | bpc 1.8087
| epoch 8 | 400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1625 | bpc 1.7618
| epoch 8 | 600/2222 batches | lr 0.0010 | ms/batch 17.17 | loss 1.1340 | bpc 1.7132
| epoch 8 | 800/2222 batches | lr 0.0010 | ms/batch 17.15 | loss 1.1267 | bpc 1.6843
| epoch 8 | 1000/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.0922 | bpc 1.6349
| epoch 8 | 1200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1873 | bpc 1.7687
| epoch 8 | 1400/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.2356 | bpc 1.8302
| epoch 8 | 1600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.5584 | bpc 2.2800
| epoch 8 | 1800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1789 | bpc 1.7417
| epoch 8 | 2000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1125 | bpc 1.6741
| epoch 8 | 2200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1785 | bpc 1.7535
--------------------------------------------------------------------------------
| end of epoch 8 | time: 39.46s | train loss 1.1989 | train bpc 1.7865 | valid loss 7.7945 | valid bpc 11.2796 
--------------------------------------------------------------------------------
Start Training
| epoch 9 | 200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.2144 | bpc 1.7998
| epoch 9 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.1529 | bpc 1.7487
| epoch 9 | 600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1244 | bpc 1.6987
| epoch 9 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1170 | bpc 1.6716
| epoch 9 | 1000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0828 | bpc 1.6222
| epoch 9 | 1200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1765 | bpc 1.7534
| epoch 9 | 1400/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.2284 | bpc 1.8213
| epoch 9 | 1600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.5506 | bpc 2.2687
| epoch 9 | 1800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1700 | bpc 1.7299
| epoch 9 | 2000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1060 | bpc 1.6660
| epoch 9 | 2200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1703 | bpc 1.7431
--------------------------------------------------------------------------------
| end of epoch 9 | time: 39.42s | train loss 1.1904 | train bpc 1.7751 | valid loss 8.4485 | valid bpc 12.2251 
--------------------------------------------------------------------------------
Start Training
| epoch 10 | 200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.2050 | bpc 1.7870
| epoch 10 | 400/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1451 | bpc 1.7373
| epoch 10 | 600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1151 | bpc 1.6870
| epoch 10 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1086 | bpc 1.6598
| epoch 10 | 1000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0741 | bpc 1.6104
| epoch 10 | 1200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1689 | bpc 1.7445
| epoch 10 | 1400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.2231 | bpc 1.8139
| epoch 10 | 1600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.5451 | bpc 2.2612
| epoch 10 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1654 | bpc 1.7232
| epoch 10 | 2000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0996 | bpc 1.6566
| epoch 10 | 2200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1644 | bpc 1.7346
--------------------------------------------------------------------------------
| end of epoch 10 | time: 39.36s | train loss 1.1832 | train bpc 1.7652 | valid loss 8.0352 | valid bpc 11.6262 
--------------------------------------------------------------------------------
Start Training
| epoch 11 | 200/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.1996 | bpc 1.7792
| epoch 11 | 400/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.1366 | bpc 1.7264
| epoch 11 | 600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1088 | bpc 1.6778
| epoch 11 | 800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1010 | bpc 1.6495
| epoch 11 | 1000/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0673 | bpc 1.6011
| epoch 11 | 1200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1625 | bpc 1.7353
| epoch 11 | 1400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.2169 | bpc 1.8046
| epoch 11 | 1600/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.5392 | bpc 2.2530
| epoch 11 | 1800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1599 | bpc 1.7156
| epoch 11 | 2000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0952 | bpc 1.6505
| epoch 11 | 2200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1604 | bpc 1.7290
--------------------------------------------------------------------------------
| end of epoch 11 | time: 39.40s | train loss 1.1771 | train bpc 1.7566 | valid loss 8.2371 | valid bpc 11.9185 
--------------------------------------------------------------------------------
Start Training
| epoch 12 | 200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.1934 | bpc 1.7708
| epoch 12 | 400/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1308 | bpc 1.7185
| epoch 12 | 600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1026 | bpc 1.6694
| epoch 12 | 800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0946 | bpc 1.6397
| epoch 12 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0617 | bpc 1.5930
| epoch 12 | 1200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1573 | bpc 1.7273
| epoch 12 | 1400/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.2117 | bpc 1.7980
| epoch 12 | 1600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.5341 | bpc 2.2451
| epoch 12 | 1800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1542 | bpc 1.7082
| epoch 12 | 2000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0901 | bpc 1.6436
| epoch 12 | 2200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1562 | bpc 1.7232
--------------------------------------------------------------------------------
| end of epoch 12 | time: 39.45s | train loss 1.1716 | train bpc 1.7489 | valid loss 8.3485 | valid bpc 12.0762 
--------------------------------------------------------------------------------
Start Training
| epoch 13 | 200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1889 | bpc 1.7641
| epoch 13 | 400/2222 batches | lr 0.0010 | ms/batch 17.17 | loss 1.1259 | bpc 1.7111
| epoch 13 | 600/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0982 | bpc 1.6636
| epoch 13 | 800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0880 | bpc 1.6310
| epoch 13 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0555 | bpc 1.5841
| epoch 13 | 1200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1524 | bpc 1.7207
| epoch 13 | 1400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.2078 | bpc 1.7927
| epoch 13 | 1600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.5299 | bpc 2.2397
| epoch 13 | 1800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1490 | bpc 1.7007
| epoch 13 | 2000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0875 | bpc 1.6407
| epoch 13 | 2200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1526 | bpc 1.7181
--------------------------------------------------------------------------------
| end of epoch 13 | time: 39.35s | train loss 1.1669 | train bpc 1.7426 | valid loss 8.1162 | valid bpc 11.7425 
--------------------------------------------------------------------------------
Start Training
| epoch 14 | 200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.1817 | bpc 1.7552
| epoch 14 | 400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1214 | bpc 1.7053
| epoch 14 | 600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0928 | bpc 1.6566
| epoch 14 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0831 | bpc 1.6245
| epoch 14 | 1000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0494 | bpc 1.5758
| epoch 14 | 1200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1475 | bpc 1.7146
| epoch 14 | 1400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.2034 | bpc 1.7870
| epoch 14 | 1600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.5258 | bpc 2.2340
| epoch 14 | 1800/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1472 | bpc 1.6981
| epoch 14 | 2000/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0811 | bpc 1.6313
| epoch 14 | 2200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1486 | bpc 1.7130
--------------------------------------------------------------------------------
| end of epoch 14 | time: 39.39s | train loss 1.1621 | train bpc 1.7361 | valid loss 8.2265 | valid bpc 11.9008 
--------------------------------------------------------------------------------
Start Training
| epoch 15 | 200/2222 batches | lr 0.0010 | ms/batch 17.26 | loss 1.1794 | bpc 1.7517
| epoch 15 | 400/2222 batches | lr 0.0010 | ms/batch 17.18 | loss 1.1162 | bpc 1.6985
| epoch 15 | 600/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.0894 | bpc 1.6516
| epoch 15 | 800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0792 | bpc 1.6186
| epoch 15 | 1000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0467 | bpc 1.5725
| epoch 15 | 1200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1429 | bpc 1.7083
| epoch 15 | 1400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.2006 | bpc 1.7833
| epoch 15 | 1600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.5227 | bpc 2.2294
| epoch 15 | 1800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1434 | bpc 1.6931
| epoch 15 | 2000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0798 | bpc 1.6309
| epoch 15 | 2200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1451 | bpc 1.7080
--------------------------------------------------------------------------------
| end of epoch 15 | time: 39.36s | train loss 1.1588 | train bpc 1.7316 | valid loss 8.2306 | valid bpc 11.9085 
--------------------------------------------------------------------------------
Start Training
| epoch 16 | 200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1759 | bpc 1.7473
| epoch 16 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.1129 | bpc 1.6940
| epoch 16 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0847 | bpc 1.6454
| epoch 16 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0743 | bpc 1.6127
| epoch 16 | 1000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0406 | bpc 1.5636
| epoch 16 | 1200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1395 | bpc 1.7044
| epoch 16 | 1400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1982 | bpc 1.7803
| epoch 16 | 1600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.5203 | bpc 2.2260
| epoch 16 | 1800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1393 | bpc 1.6874
| epoch 16 | 2000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0760 | bpc 1.6260
| epoch 16 | 2200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1436 | bpc 1.7063
--------------------------------------------------------------------------------
| end of epoch 16 | time: 39.47s | train loss 1.1550 | train bpc 1.7267 | valid loss 8.5712 | valid bpc 12.3994 
--------------------------------------------------------------------------------
Start Training
| epoch 17 | 200/2222 batches | lr 0.0010 | ms/batch 17.16 | loss 1.1735 | bpc 1.7438
| epoch 17 | 400/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1073 | bpc 1.6882
| epoch 17 | 600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0812 | bpc 1.6412
| epoch 17 | 800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0689 | bpc 1.6050
| epoch 17 | 1000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0401 | bpc 1.5640
| epoch 17 | 1200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1347 | bpc 1.6970
| epoch 17 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.1939 | bpc 1.7748
| epoch 17 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.5182 | bpc 2.2241
| epoch 17 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1366 | bpc 1.6838
| epoch 17 | 2000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0732 | bpc 1.6226
| epoch 17 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.1397 | bpc 1.7016
--------------------------------------------------------------------------------
| end of epoch 17 | time: 39.31s | train loss 1.1516 | train bpc 1.7225 | valid loss 8.5694 | valid bpc 12.3964 
--------------------------------------------------------------------------------
Start Training
| epoch 18 | 200/2222 batches | lr 0.0010 | ms/batch 17.21 | loss 1.1690 | bpc 1.7381
| epoch 18 | 400/2222 batches | lr 0.0010 | ms/batch 17.15 | loss 1.1067 | bpc 1.6873
| epoch 18 | 600/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0756 | bpc 1.6339
| epoch 18 | 800/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0667 | bpc 1.6026
| epoch 18 | 1000/2222 batches | lr 0.0010 | ms/batch 17.16 | loss 1.0348 | bpc 1.5564
| epoch 18 | 1200/2222 batches | lr 0.0010 | ms/batch 17.15 | loss 1.1315 | bpc 1.6930
| epoch 18 | 1400/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.1935 | bpc 1.7745
| epoch 18 | 1600/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.5140 | bpc 2.2173
| epoch 18 | 1800/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.1331 | bpc 1.6795
| epoch 18 | 2000/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0708 | bpc 1.6200
| epoch 18 | 2200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1373 | bpc 1.6986
--------------------------------------------------------------------------------
| end of epoch 18 | time: 39.45s | train loss 1.1486 | train bpc 1.7185 | valid loss 8.3450 | valid bpc 12.0726 
--------------------------------------------------------------------------------
Start Training
| epoch 19 | 200/2222 batches | lr 0.0010 | ms/batch 17.19 | loss 1.1672 | bpc 1.7368
| epoch 19 | 400/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.1015 | bpc 1.6795
| epoch 19 | 600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0740 | bpc 1.6324
| epoch 19 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0644 | bpc 1.5996
| epoch 19 | 1000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0326 | bpc 1.5531
| epoch 19 | 1200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1285 | bpc 1.6895
| epoch 19 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.1893 | bpc 1.7686
| epoch 19 | 1600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.5116 | bpc 2.2143
| epoch 19 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1301 | bpc 1.6746
| epoch 19 | 2000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0700 | bpc 1.6198
| epoch 19 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.1349 | bpc 1.6946
--------------------------------------------------------------------------------
| end of epoch 19 | time: 39.32s | train loss 1.1459 | train bpc 1.7150 | valid loss 8.4485 | valid bpc 12.2231 
--------------------------------------------------------------------------------
Start Training
| epoch 20 | 200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1635 | bpc 1.7319
| epoch 20 | 400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1000 | bpc 1.6783
| epoch 20 | 600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.0717 | bpc 1.6282
| epoch 20 | 800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0604 | bpc 1.5947
| epoch 20 | 1000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0300 | bpc 1.5500
| epoch 20 | 1200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1264 | bpc 1.6865
| epoch 20 | 1400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1882 | bpc 1.7673
| epoch 20 | 1600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.5094 | bpc 2.2111
| epoch 20 | 1800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1292 | bpc 1.6740
| epoch 20 | 2000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0667 | bpc 1.6149
| epoch 20 | 2200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1318 | bpc 1.6912
--------------------------------------------------------------------------------
| end of epoch 20 | time: 39.55s | train loss 1.1436 | train bpc 1.7119 | valid loss 8.4485 | valid bpc 12.2246 
--------------------------------------------------------------------------------
Start Training
| epoch 21 | 200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1605 | bpc 1.7270
| epoch 21 | 400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0963 | bpc 1.6733
| epoch 21 | 600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0681 | bpc 1.6244
| epoch 21 | 800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0583 | bpc 1.5921
| epoch 21 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0278 | bpc 1.5478
| epoch 21 | 1200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1257 | bpc 1.6858
| epoch 21 | 1400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1845 | bpc 1.7626
| epoch 21 | 1600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.5063 | bpc 2.2066
| epoch 21 | 1800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1263 | bpc 1.6697
| epoch 21 | 2000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0653 | bpc 1.6141
| epoch 21 | 2200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1307 | bpc 1.6902
--------------------------------------------------------------------------------
| end of epoch 21 | time: 39.51s | train loss 1.1410 | train bpc 1.7087 | valid loss 8.3802 | valid bpc 12.1247 
--------------------------------------------------------------------------------
Start Training
| epoch 22 | 200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1599 | bpc 1.7267
| epoch 22 | 400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0935 | bpc 1.6703
| epoch 22 | 600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0664 | bpc 1.6218
| epoch 22 | 800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0560 | bpc 1.5890
| epoch 22 | 1000/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0241 | bpc 1.5425
| epoch 22 | 1200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1207 | bpc 1.6786
| epoch 22 | 1400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1841 | bpc 1.7619
| epoch 22 | 1600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.5056 | bpc 2.2056
| epoch 22 | 1800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1259 | bpc 1.6695
| epoch 22 | 2000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0622 | bpc 1.6092
| epoch 22 | 2200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1288 | bpc 1.6870
--------------------------------------------------------------------------------
| end of epoch 22 | time: 39.66s | train loss 1.1389 | train bpc 1.7058 | valid loss 8.4640 | valid bpc 12.2449 
--------------------------------------------------------------------------------
Start Training
| epoch 23 | 200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1584 | bpc 1.7255
| epoch 23 | 400/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0908 | bpc 1.6659
| epoch 23 | 600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0649 | bpc 1.6193
| epoch 23 | 800/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0531 | bpc 1.5850
| epoch 23 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0203 | bpc 1.5365
| epoch 23 | 1200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1191 | bpc 1.6770
| epoch 23 | 1400/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1832 | bpc 1.7610
| epoch 23 | 1600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.5031 | bpc 2.2027
| epoch 23 | 1800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1234 | bpc 1.6659
| epoch 23 | 2000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0622 | bpc 1.6095
| epoch 23 | 2200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1264 | bpc 1.6842
--------------------------------------------------------------------------------
| end of epoch 23 | time: 39.77s | train loss 1.1369 | train bpc 1.7032 | valid loss 8.2627 | valid bpc 11.9542 
--------------------------------------------------------------------------------
Start Training
| epoch 24 | 200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1556 | bpc 1.7202
| epoch 24 | 400/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0897 | bpc 1.6655
| epoch 24 | 600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0623 | bpc 1.6170
| epoch 24 | 800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0512 | bpc 1.5831
| epoch 24 | 1000/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0191 | bpc 1.5355
| epoch 24 | 1200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1167 | bpc 1.6739
| epoch 24 | 1400/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1804 | bpc 1.7575
| epoch 24 | 1600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.5024 | bpc 2.2012
| epoch 24 | 1800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1181 | bpc 1.6581
| epoch 24 | 2000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0600 | bpc 1.6066
| epoch 24 | 2200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1255 | bpc 1.6834
--------------------------------------------------------------------------------
| end of epoch 24 | time: 39.60s | train loss 1.1348 | train bpc 1.7004 | valid loss 8.4738 | valid bpc 12.2599 
--------------------------------------------------------------------------------
Start Training
| epoch 25 | 200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1538 | bpc 1.7187
| epoch 25 | 400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0867 | bpc 1.6616
| epoch 25 | 600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0617 | bpc 1.6169
| epoch 25 | 800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0484 | bpc 1.5787
| epoch 25 | 1000/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0178 | bpc 1.5344
| epoch 25 | 1200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1150 | bpc 1.6712
| epoch 25 | 1400/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1792 | bpc 1.7563
| epoch 25 | 1600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.5011 | bpc 2.1999
| epoch 25 | 1800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1176 | bpc 1.6579
| epoch 25 | 2000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0591 | bpc 1.6061
| epoch 25 | 2200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1249 | bpc 1.6835
--------------------------------------------------------------------------------
| end of epoch 25 | time: 39.41s | train loss 1.1334 | train bpc 1.6989 | valid loss 8.5391 | valid bpc 12.3553 
--------------------------------------------------------------------------------
Start Training
| epoch 26 | 200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 1.1516 | bpc 1.7158
| epoch 26 | 400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.0840 | bpc 1.6585
| epoch 26 | 600/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0593 | bpc 1.6126
| epoch 26 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0453 | bpc 1.5756
| epoch 26 | 1000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0158 | bpc 1.5315
| epoch 26 | 1200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1133 | bpc 1.6698
| epoch 26 | 1400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1769 | bpc 1.7532
| epoch 26 | 1600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.4978 | bpc 2.1954
| epoch 26 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1167 | bpc 1.6562
| epoch 26 | 2000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0567 | bpc 1.6031
| epoch 26 | 2200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1216 | bpc 1.6780
--------------------------------------------------------------------------------
| end of epoch 26 | time: 39.34s | train loss 1.1310 | train bpc 1.6958 | valid loss 8.9117 | valid bpc 12.8930 
--------------------------------------------------------------------------------
Start Training
| epoch 27 | 200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1512 | bpc 1.7153
| epoch 27 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.0827 | bpc 1.6569
| epoch 27 | 600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0558 | bpc 1.6082
| epoch 27 | 800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0446 | bpc 1.5745
| epoch 27 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0129 | bpc 1.5273
| epoch 27 | 1200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1118 | bpc 1.6679
| epoch 27 | 1400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1741 | bpc 1.7493
| epoch 27 | 1600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.4982 | bpc 2.1965
| epoch 27 | 1800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1135 | bpc 1.6523
| epoch 27 | 2000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0565 | bpc 1.6032
| epoch 27 | 2200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1220 | bpc 1.6792
--------------------------------------------------------------------------------
| end of epoch 27 | time: 39.42s | train loss 1.1295 | train bpc 1.6939 | valid loss 8.5807 | valid bpc 12.4139 
--------------------------------------------------------------------------------
Start Training
| epoch 28 | 200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1498 | bpc 1.7143
| epoch 28 | 400/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.0808 | bpc 1.6541
| epoch 28 | 600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0564 | bpc 1.6103
| epoch 28 | 800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0437 | bpc 1.5733
| epoch 28 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0110 | bpc 1.5257
| epoch 28 | 1200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1100 | bpc 1.6651
| epoch 28 | 1400/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1742 | bpc 1.7501
| epoch 28 | 1600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.4952 | bpc 2.1919
| epoch 28 | 1800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1146 | bpc 1.6540
| epoch 28 | 2000/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0545 | bpc 1.6010
| epoch 28 | 2200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1194 | bpc 1.6756
--------------------------------------------------------------------------------
| end of epoch 28 | time: 39.41s | train loss 1.1282 | train bpc 1.6925 | valid loss 8.8178 | valid bpc 12.7579 
--------------------------------------------------------------------------------
Start Training
| epoch 29 | 200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1470 | bpc 1.7090
| epoch 29 | 400/2222 batches | lr 0.0010 | ms/batch 17.16 | loss 1.0805 | bpc 1.6553
| epoch 29 | 600/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.0527 | bpc 1.6052
| epoch 29 | 800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0408 | bpc 1.5698
| epoch 29 | 1000/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0102 | bpc 1.5235
| epoch 29 | 1200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1083 | bpc 1.6629
| epoch 29 | 1400/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1723 | bpc 1.7468
| epoch 29 | 1600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.4929 | bpc 2.1889
| epoch 29 | 1800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1141 | bpc 1.6536
| epoch 29 | 2000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0527 | bpc 1.5992
| epoch 29 | 2200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1184 | bpc 1.6741
--------------------------------------------------------------------------------
| end of epoch 29 | time: 39.47s | train loss 1.1265 | train bpc 1.6901 | valid loss 8.8084 | valid bpc 12.7444 
--------------------------------------------------------------------------------
Start Training
| epoch 30 | 200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1468 | bpc 1.7088
| epoch 30 | 400/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0757 | bpc 1.6483
| epoch 30 | 600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0515 | bpc 1.6037
| epoch 30 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0380 | bpc 1.5667
| epoch 30 | 1000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0082 | bpc 1.5210
| epoch 30 | 1200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1048 | bpc 1.6578
| epoch 30 | 1400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1729 | bpc 1.7482
| epoch 30 | 1600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.4929 | bpc 2.1889
| epoch 30 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1113 | bpc 1.6495
| epoch 30 | 2000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0514 | bpc 1.5966
| epoch 30 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.1192 | bpc 1.6757
--------------------------------------------------------------------------------
| end of epoch 30 | time: 39.31s | train loss 1.1249 | train bpc 1.6879 | valid loss 8.5991 | valid bpc 12.4416 
--------------------------------------------------------------------------------
Start Training
| epoch 31 | 200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 1.1448 | bpc 1.7060
| epoch 31 | 400/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0757 | bpc 1.6485
| epoch 31 | 600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0488 | bpc 1.6000
| epoch 31 | 800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0379 | bpc 1.5657
| epoch 31 | 1000/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0062 | bpc 1.5193
| epoch 31 | 1200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1039 | bpc 1.6566
| epoch 31 | 1400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1713 | bpc 1.7464
| epoch 31 | 1600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.4917 | bpc 2.1873
| epoch 31 | 1800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1104 | bpc 1.6483
| epoch 31 | 2000/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0506 | bpc 1.5964
| epoch 31 | 2200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1170 | bpc 1.6732
--------------------------------------------------------------------------------
| end of epoch 31 | time: 39.37s | train loss 1.1236 | train bpc 1.6863 | valid loss 8.6030 | valid bpc 12.4473 
--------------------------------------------------------------------------------
Start Training
| epoch 32 | 200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1431 | bpc 1.7046
| epoch 32 | 400/2222 batches | lr 0.0010 | ms/batch 17.18 | loss 1.0758 | bpc 1.6489
| epoch 32 | 600/2222 batches | lr 0.0010 | ms/batch 17.16 | loss 1.0487 | bpc 1.6008
| epoch 32 | 800/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.0364 | bpc 1.5639
| epoch 32 | 1000/2222 batches | lr 0.0010 | ms/batch 17.15 | loss 1.0054 | bpc 1.5184
| epoch 32 | 1200/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.1035 | bpc 1.6565
| epoch 32 | 1400/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.1701 | bpc 1.7448
| epoch 32 | 1600/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.4908 | bpc 2.1860
| epoch 32 | 1800/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1081 | bpc 1.6447
| epoch 32 | 2000/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0505 | bpc 1.5953
| epoch 32 | 2200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1159 | bpc 1.6715
--------------------------------------------------------------------------------
| end of epoch 32 | time: 39.47s | train loss 1.1227 | train bpc 1.6853 | valid loss 8.6573 | valid bpc 12.5259 
--------------------------------------------------------------------------------
Start Training
| epoch 33 | 200/2222 batches | lr 0.0010 | ms/batch 17.19 | loss 1.1415 | bpc 1.7014
| epoch 33 | 400/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.0738 | bpc 1.6464
| epoch 33 | 600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0460 | bpc 1.5972
| epoch 33 | 800/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0355 | bpc 1.5632
| epoch 33 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0052 | bpc 1.5184
| epoch 33 | 1200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1017 | bpc 1.6539
| epoch 33 | 1400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1683 | bpc 1.7429
| epoch 33 | 1600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.4885 | bpc 2.1820
| epoch 33 | 1800/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1099 | bpc 1.6481
| epoch 33 | 2000/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0493 | bpc 1.5937
| epoch 33 | 2200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1160 | bpc 1.6721
--------------------------------------------------------------------------------
| end of epoch 33 | time: 39.55s | train loss 1.1215 | train bpc 1.6837 | valid loss 9.0523 | valid bpc 13.0964 
--------------------------------------------------------------------------------
Start Training
| epoch 34 | 200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1419 | bpc 1.7031
| epoch 34 | 400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0709 | bpc 1.6422
| epoch 34 | 600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.0458 | bpc 1.5964
| epoch 34 | 800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.0332 | bpc 1.5605
| epoch 34 | 1000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0020 | bpc 1.5138
| epoch 34 | 1200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0998 | bpc 1.6525
| epoch 34 | 1400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1680 | bpc 1.7422
| epoch 34 | 1600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.4886 | bpc 2.1834
| epoch 34 | 1800/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1070 | bpc 1.6433
| epoch 34 | 2000/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0482 | bpc 1.5930
| epoch 34 | 2200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1131 | bpc 1.6681
--------------------------------------------------------------------------------
| end of epoch 34 | time: 39.40s | train loss 1.1200 | train bpc 1.6819 | valid loss 8.8670 | valid bpc 12.8288 
--------------------------------------------------------------------------------
Start Training
| epoch 35 | 200/2222 batches | lr 0.0010 | ms/batch 17.22 | loss 1.1405 | bpc 1.7012
| epoch 35 | 400/2222 batches | lr 0.0010 | ms/batch 17.16 | loss 1.0698 | bpc 1.6400
| epoch 35 | 600/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.0434 | bpc 1.5935
| epoch 35 | 800/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.0320 | bpc 1.5582
| epoch 35 | 1000/2222 batches | lr 0.0010 | ms/batch 17.15 | loss 1.0022 | bpc 1.5142
| epoch 35 | 1200/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.0987 | bpc 1.6499
| epoch 35 | 1400/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.1657 | bpc 1.7387
| epoch 35 | 1600/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.4863 | bpc 2.1805
| epoch 35 | 1800/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1061 | bpc 1.6423
| epoch 35 | 2000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0463 | bpc 1.5907
| epoch 35 | 2200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1134 | bpc 1.6689
--------------------------------------------------------------------------------
| end of epoch 35 | time: 39.44s | train loss 1.1187 | train bpc 1.6799 | valid loss 8.8467 | valid bpc 12.7997 
--------------------------------------------------------------------------------
Start Training
| epoch 36 | 200/2222 batches | lr 0.0010 | ms/batch 17.25 | loss 1.1383 | bpc 1.6992
| epoch 36 | 400/2222 batches | lr 0.0010 | ms/batch 17.18 | loss 1.0695 | bpc 1.6405
| epoch 36 | 600/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.0426 | bpc 1.5931
| epoch 36 | 800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0296 | bpc 1.5557
| epoch 36 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0006 | bpc 1.5119
| epoch 36 | 1200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0974 | bpc 1.6482
| epoch 36 | 1400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1653 | bpc 1.7386
| epoch 36 | 1600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.4869 | bpc 2.1815
| epoch 36 | 1800/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1043 | bpc 1.6393
| epoch 36 | 2000/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0465 | bpc 1.5910
| epoch 36 | 2200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1113 | bpc 1.6659
--------------------------------------------------------------------------------
| end of epoch 36 | time: 39.37s | train loss 1.1177 | train bpc 1.6790 | valid loss 8.6347 | valid bpc 12.4935 
--------------------------------------------------------------------------------
Start Training
| epoch 37 | 200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1391 | bpc 1.6997
| epoch 37 | 400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0665 | bpc 1.6371
| epoch 37 | 600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0403 | bpc 1.5888
| epoch 37 | 800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0318 | bpc 1.5593
| epoch 37 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 0.9989 | bpc 1.5097
| epoch 37 | 1200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0958 | bpc 1.6463
| epoch 37 | 1400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1654 | bpc 1.7388
| epoch 37 | 1600/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.4861 | bpc 2.1798
| epoch 37 | 1800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1018 | bpc 1.6358
| epoch 37 | 2000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0463 | bpc 1.5902
| epoch 37 | 2200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1107 | bpc 1.6651
--------------------------------------------------------------------------------
| end of epoch 37 | time: 39.41s | train loss 1.1168 | train bpc 1.6777 | valid loss 8.8810 | valid bpc 12.8490 
--------------------------------------------------------------------------------
Start Training
| epoch 38 | 200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1366 | bpc 1.6962
| epoch 38 | 400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0662 | bpc 1.6371
| epoch 38 | 600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0402 | bpc 1.5891
| epoch 38 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0295 | bpc 1.5554
| epoch 38 | 1000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 0.9976 | bpc 1.5089
| epoch 38 | 1200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0943 | bpc 1.6440
| epoch 38 | 1400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1649 | bpc 1.7386
| epoch 38 | 1600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.4838 | bpc 2.1768
| epoch 38 | 1800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1036 | bpc 1.6383
| epoch 38 | 2000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0446 | bpc 1.5881
| epoch 38 | 2200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1086 | bpc 1.6627
--------------------------------------------------------------------------------
| end of epoch 38 | time: 39.53s | train loss 1.1157 | train bpc 1.6763 | valid loss 8.8589 | valid bpc 12.8176 
--------------------------------------------------------------------------------
Start Training
| epoch 39 | 200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1347 | bpc 1.6947
| epoch 39 | 400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0659 | bpc 1.6363
| epoch 39 | 600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.0399 | bpc 1.5882
| epoch 39 | 800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0261 | bpc 1.5520
| epoch 39 | 1000/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 0.9971 | bpc 1.5085
| epoch 39 | 1200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0953 | bpc 1.6462
| epoch 39 | 1400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1612 | bpc 1.7338
| epoch 39 | 1600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.4840 | bpc 2.1763
| epoch 39 | 1800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1025 | bpc 1.6368
| epoch 39 | 2000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0438 | bpc 1.5869
| epoch 39 | 2200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1099 | bpc 1.6648
--------------------------------------------------------------------------------
| end of epoch 39 | time: 39.69s | train loss 1.1148 | train bpc 1.6752 | valid loss 8.8022 | valid bpc 12.7352 
--------------------------------------------------------------------------------
Start Training
| epoch 40 | 200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 1.1338 | bpc 1.6926
| epoch 40 | 400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 1.0648 | bpc 1.6354
| epoch 40 | 600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.0390 | bpc 1.5883
| epoch 40 | 800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.0260 | bpc 1.5514
| epoch 40 | 1000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 0.9965 | bpc 1.5077
| epoch 40 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.0938 | bpc 1.6445
| epoch 40 | 1400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1619 | bpc 1.7347
| epoch 40 | 1600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.4818 | bpc 2.1739
| epoch 40 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1009 | bpc 1.6349
| epoch 40 | 2000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0432 | bpc 1.5859
| epoch 40 | 2200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1091 | bpc 1.6629
--------------------------------------------------------------------------------
| end of epoch 40 | time: 39.67s | train loss 1.1138 | train bpc 1.6739 | valid loss 8.9255 | valid bpc 12.9145 
--------------------------------------------------------------------------------
Start Training
| epoch 41 | 200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1330 | bpc 1.6921
| epoch 41 | 400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.0635 | bpc 1.6333
| epoch 41 | 600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.0375 | bpc 1.5865
| epoch 41 | 800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0245 | bpc 1.5497
| epoch 41 | 1000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 0.9953 | bpc 1.5055
| epoch 41 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.0933 | bpc 1.6442
| epoch 41 | 1400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1597 | bpc 1.7315
| epoch 41 | 1600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.4828 | bpc 2.1751
| epoch 41 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1007 | bpc 1.6347
| epoch 41 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.0416 | bpc 1.5844
| epoch 41 | 2200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1082 | bpc 1.6621
--------------------------------------------------------------------------------
| end of epoch 41 | time: 39.37s | train loss 1.1129 | train bpc 1.6728 | valid loss 9.2699 | valid bpc 13.4099 
--------------------------------------------------------------------------------
Start Training
| epoch 42 | 200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.1344 | bpc 1.6936
| epoch 42 | 400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.0618 | bpc 1.6318
| epoch 42 | 600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0374 | bpc 1.5864
| epoch 42 | 800/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0234 | bpc 1.5485
| epoch 42 | 1000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 0.9936 | bpc 1.5039
| epoch 42 | 1200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0921 | bpc 1.6415
| epoch 42 | 1400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1605 | bpc 1.7335
| epoch 42 | 1600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.4819 | bpc 2.1744
| epoch 42 | 1800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1001 | bpc 1.6335
| epoch 42 | 2000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0416 | bpc 1.5840
| epoch 42 | 2200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1068 | bpc 1.6603
--------------------------------------------------------------------------------
| end of epoch 42 | time: 39.43s | train loss 1.1123 | train bpc 1.6721 | valid loss 8.8857 | valid bpc 12.8584 
--------------------------------------------------------------------------------
Start Training
| epoch 43 | 200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.1324 | bpc 1.6916
| epoch 43 | 400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0612 | bpc 1.6309
| epoch 43 | 600/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.0350 | bpc 1.5826
| epoch 43 | 800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0230 | bpc 1.5475
| epoch 43 | 1000/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 0.9938 | bpc 1.5039
| epoch 43 | 1200/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.0909 | bpc 1.6398
| epoch 43 | 1400/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.1593 | bpc 1.7315
| epoch 43 | 1600/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.4812 | bpc 2.1737
| epoch 43 | 1800/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0980 | bpc 1.6300
| epoch 43 | 2200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1071 | bpc 1.6613
--------------------------------------------------------------------------------
| end of epoch 43 | time: 39.48s | train loss 1.1113 | train bpc 1.6708 | valid loss 9.0592 | valid bpc 13.1066 
--------------------------------------------------------------------------------
Start Training
| epoch 44 | 200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1323 | bpc 1.6910
| epoch 44 | 400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0603 | bpc 1.6298
| epoch 44 | 600/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0348 | bpc 1.5817
| epoch 44 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0221 | bpc 1.5467
| epoch 44 | 1000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 0.9923 | bpc 1.5021
| epoch 44 | 1200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0896 | bpc 1.6383
| epoch 44 | 1400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1583 | bpc 1.7306
| epoch 44 | 1600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.4805 | bpc 2.1725
| epoch 44 | 1800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0975 | bpc 1.6298
| epoch 44 | 2000/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0406 | bpc 1.5830
| epoch 44 | 2200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.1066 | bpc 1.6610
--------------------------------------------------------------------------------
| end of epoch 44 | time: 39.39s | train loss 1.1106 | train bpc 1.6699 | valid loss 9.0563 | valid bpc 13.1029 
--------------------------------------------------------------------------------
Start Training
| epoch 45 | 200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.1311 | bpc 1.6892
| epoch 45 | 400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0588 | bpc 1.6284
| epoch 45 | 600/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 1.0333 | bpc 1.5805
| epoch 45 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 1.0206 | bpc 1.5440
| epoch 45 | 1000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 0.9928 | bpc 1.5028
| epoch 45 | 1200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0889 | bpc 1.6370
| epoch 45 | 1400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1577 | bpc 1.7307
| epoch 45 | 1600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.4796 | bpc 2.1706
| epoch 45 | 1800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0972 | bpc 1.6291
| epoch 45 | 2000/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0405 | bpc 1.5830
| epoch 45 | 2200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.1041 | bpc 1.6574
--------------------------------------------------------------------------------
| end of epoch 45 | time: 39.53s | train loss 1.1098 | train bpc 1.6689 | valid loss 8.9045 | valid bpc 12.8832 
--------------------------------------------------------------------------------
Start Training
| epoch 46 | 200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1289 | bpc 1.6861
| epoch 46 | 400/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.0592 | bpc 1.6286
| epoch 46 | 600/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0331 | bpc 1.5801
| epoch 46 | 800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0215 | bpc 1.5464
| epoch 46 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 0.9899 | bpc 1.4990
| epoch 46 | 1200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0884 | bpc 1.6368
| epoch 46 | 1400/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 1.1576 | bpc 1.7306
| epoch 46 | 1600/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.4783 | bpc 2.1695
| epoch 46 | 1800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0972 | bpc 1.6293
| epoch 46 | 2000/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0391 | bpc 1.5813
| epoch 46 | 2200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1045 | bpc 1.6577
--------------------------------------------------------------------------------
| end of epoch 46 | time: 39.44s | train loss 1.1090 | train bpc 1.6680 | valid loss 8.9253 | valid bpc 12.9148 
--------------------------------------------------------------------------------
Start Training
| epoch 47 | 200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.1302 | bpc 1.6880
| epoch 47 | 400/2222 batches | lr 0.0010 | ms/batch 17.16 | loss 1.0566 | bpc 1.6252
| epoch 47 | 600/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.0322 | bpc 1.5791
| epoch 47 | 800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0193 | bpc 1.5425
| epoch 47 | 1000/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 0.9904 | bpc 1.5002
| epoch 47 | 1200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0869 | bpc 1.6349
| epoch 47 | 1400/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1577 | bpc 1.7297
| epoch 47 | 1600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.4777 | bpc 2.1691
| epoch 47 | 1800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0964 | bpc 1.6283
| epoch 47 | 2000/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0381 | bpc 1.5798
| epoch 47 | 2200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1033 | bpc 1.6557
--------------------------------------------------------------------------------
| end of epoch 47 | time: 39.44s | train loss 1.1083 | train bpc 1.6670 | valid loss 9.0090 | valid bpc 13.0348 
--------------------------------------------------------------------------------
Start Training
| epoch 48 | 200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.1285 | bpc 1.6855
| epoch 48 | 400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0575 | bpc 1.6268
| epoch 48 | 600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0307 | bpc 1.5770
| epoch 48 | 800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 1.0201 | bpc 1.5437
| epoch 48 | 1000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 0.9892 | bpc 1.4988
| epoch 48 | 1200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0870 | bpc 1.6354
| epoch 48 | 1400/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1559 | bpc 1.7268
| epoch 48 | 1600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.4770 | bpc 2.1683
| epoch 48 | 1800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0957 | bpc 1.6275
| epoch 48 | 2000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0382 | bpc 1.5799
| epoch 48 | 2200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1023 | bpc 1.6559
--------------------------------------------------------------------------------
| end of epoch 48 | time: 39.46s | train loss 1.1076 | train bpc 1.6662 | valid loss 8.9572 | valid bpc 12.9596 
--------------------------------------------------------------------------------
Start Training
| epoch 49 | 200/2222 batches | lr 0.0010 | ms/batch 17.18 | loss 1.1269 | bpc 1.6840
| epoch 49 | 400/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 1.0561 | bpc 1.6251
| epoch 49 | 600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0299 | bpc 1.5765
| epoch 49 | 800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0177 | bpc 1.5407
| epoch 49 | 1000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 0.9888 | bpc 1.4988
| epoch 49 | 1200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0850 | bpc 1.6325
| epoch 49 | 1400/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1553 | bpc 1.7264
| epoch 49 | 1600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.4777 | bpc 2.1690
| epoch 49 | 1800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0958 | bpc 1.6279
| epoch 49 | 2000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0366 | bpc 1.5786
| epoch 49 | 2200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1031 | bpc 1.6550
--------------------------------------------------------------------------------
| end of epoch 49 | time: 39.46s | train loss 1.1068 | train bpc 1.6652 | valid loss 8.9925 | valid bpc 13.0125 
--------------------------------------------------------------------------------
Start Training
| epoch 50 | 200/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 1.1271 | bpc 1.6840
| epoch 50 | 400/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0543 | bpc 1.6230
| epoch 50 | 600/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0305 | bpc 1.5773
| epoch 50 | 800/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.0179 | bpc 1.5417
| epoch 50 | 1000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 0.9881 | bpc 1.4974
| epoch 50 | 1200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0857 | bpc 1.6336
| epoch 50 | 1400/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.1550 | bpc 1.7269
| epoch 50 | 1600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 1.4757 | bpc 2.1670
| epoch 50 | 1800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0946 | bpc 1.6262
| epoch 50 | 2000/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.0374 | bpc 1.5784
| epoch 50 | 2200/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 1.1022 | bpc 1.6559
--------------------------------------------------------------------------------
| end of epoch 50 | time: 39.46s | train loss 1.1064 | train bpc 1.6651 | valid loss 8.9231 | valid bpc 12.9113 
--------------------------------------------------------------------------------
Run history:

epoch	
lr	
train_bpc	
train_loss	
val_bpc	
val_loss	

Run summary:

epoch	50
lr	0.001
train_bpc	1.66509
train_loss	1.10643
val_bpc	12.91131
val_loss	8.92315

View run good-energy-22 at: https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/ndu8tbwn
View project at: https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
Find logs at: ./wandb/run-20240822_214429-ndu8tbwn/logs
Start Test Validation
Evaluating batch 1/124
Evaluating batch 2/124
Evaluating batch 3/124
Evaluating batch 4/124
Evaluating batch 5/124
Evaluating batch 6/124
Evaluating batch 7/124
Evaluating batch 8/124
Evaluating batch 9/124
Evaluating batch 10/124
Evaluating batch 11/124
Evaluating batch 12/124
Evaluating batch 13/124
Evaluating batch 14/124
Evaluating batch 15/124
Evaluating batch 16/124
Evaluating batch 17/124
Evaluating batch 18/124
Evaluating batch 19/124
Evaluating batch 20/124
Evaluating batch 21/124
Evaluating batch 22/124
Evaluating batch 23/124
Evaluating batch 24/124
Evaluating batch 25/124
Evaluating batch 26/124
Evaluating batch 27/124
Evaluating batch 28/124
Evaluating batch 29/124
Evaluating batch 30/124
Evaluating batch 31/124
Evaluating batch 32/124
Evaluating batch 33/124
Evaluating batch 34/124
Evaluating batch 35/124
Evaluating batch 36/124
Evaluating batch 37/124
Evaluating batch 38/124
Evaluating batch 39/124
Evaluating batch 40/124
Evaluating batch 41/124
Evaluating batch 42/124
Evaluating batch 43/124
Evaluating batch 44/124
Evaluating batch 45/124
Evaluating batch 46/124
Evaluating batch 47/124
Evaluating batch 48/124
Evaluating batch 49/124
Evaluating batch 50/124
Evaluating batch 51/124
Evaluating batch 52/124
Evaluating batch 53/124
Evaluating batch 54/124
Evaluating batch 55/124
Evaluating batch 56/124
Evaluating batch 57/124
Evaluating batch 58/124
Evaluating batch 59/124
Evaluating batch 60/124
Evaluating batch 61/124
Evaluating batch 62/124
Evaluating batch 63/124
Evaluating batch 64/124
Evaluating batch 65/124
Evaluating batch 66/124
Evaluating batch 67/124
Evaluating batch 68/124
Evaluating batch 69/124
Evaluating batch 70/124
Evaluating batch 71/124
Evaluating batch 72/124
Evaluating batch 73/124
Evaluating batch 74/124
Evaluating batch 75/124
Evaluating batch 76/124
Evaluating batch 77/124
Evaluating batch 78/124
Evaluating batch 79/124
Evaluating batch 80/124
Evaluating batch 81/124
Evaluating batch 82/124
Evaluating batch 83/124
Evaluating batch 84/124
Evaluating batch 85/124
Evaluating batch 86/124
Evaluating batch 87/124
Evaluating batch 88/124
Evaluating batch 89/124
Evaluating batch 90/124
Evaluating batch 91/124
Evaluating batch 92/124
Evaluating batch 93/124
Evaluating batch 94/124
Evaluating batch 95/124
Evaluating batch 96/124
Evaluating batch 97/124
Evaluating batch 98/124
Evaluating batch 99/124
Evaluating batch 100/124
Evaluating batch 101/124
Evaluating batch 102/124
Evaluating batch 103/124
Evaluating batch 104/124
Evaluating batch 105/124
Evaluating batch 106/124
Evaluating batch 107/124
Evaluating batch 108/124
Evaluating batch 109/124
Evaluating batch 110/124
Evaluating batch 111/124
Evaluating batch 112/124
Evaluating batch 113/124
Evaluating batch 114/124
Evaluating batch 115/124
Evaluating batch 116/124
Evaluating batch 117/124
Evaluating batch 118/124
Evaluating batch 119/124
Evaluating batch 120/124
Evaluating batch 121/124
Evaluating batch 122/124
Evaluating batch 123/124
Evaluating batch 124/124
================================================================================
| End of training | test loss 7.280705451965332 | test bpc 10.558947563171387 
================================================================================
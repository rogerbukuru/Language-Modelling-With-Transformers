Tracking run with wandb version 0.17.7
Run data is saved locally in /content/wandb/run-20240822_210932-q423zdup
Syncing run stellar-thunder-21 to Weights & Biases (docs)
View project at https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers
View run at https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/q423zdup
Start Training
| epoch 1 | 200/2222 batches | lr 0.0010 | ms/batch 28.85 | loss 3.3402 | bpc 4.8390
| epoch 1 | 400/2222 batches | lr 0.0010 | ms/batch 22.72 | loss 3.3101 | bpc 4.8101
| epoch 1 | 600/2222 batches | lr 0.0010 | ms/batch 20.69 | loss 3.2846 | bpc 4.7697
| epoch 1 | 800/2222 batches | lr 0.0010 | ms/batch 19.69 | loss 3.2687 | bpc 4.7411
| epoch 1 | 1000/2222 batches | lr 0.0010 | ms/batch 19.13 | loss 3.2592 | bpc 4.7266
| epoch 1 | 1200/2222 batches | lr 0.0010 | ms/batch 18.75 | loss 3.2706 | bpc 4.7417
| epoch 1 | 1400/2222 batches | lr 0.0010 | ms/batch 18.48 | loss 3.2552 | bpc 4.7169
| epoch 1 | 1600/2222 batches | lr 0.0010 | ms/batch 18.30 | loss 3.3261 | bpc 4.8130
| epoch 1 | 1800/2222 batches | lr 0.0010 | ms/batch 18.17 | loss 3.2611 | bpc 4.7219
| epoch 1 | 2000/2222 batches | lr 0.0010 | ms/batch 18.06 | loss 3.2639 | bpc 4.7384
| epoch 1 | 2200/2222 batches | lr 0.0010 | ms/batch 17.98 | loss 3.2508 | bpc 4.7132
--------------------------------------------------------------------------------
| end of epoch 1 | time: 42.08s | train loss 3.2803 | train bpc 4.7566 | valid loss 7.9538 | valid bpc 11.4827 
--------------------------------------------------------------------------------
Start Training
| epoch 2 | 200/2222 batches | lr 0.0010 | ms/batch 17.50 | loss 3.2886 | bpc 4.7651
| epoch 2 | 400/2222 batches | lr 0.0010 | ms/batch 17.39 | loss 3.3117 | bpc 4.8127
| epoch 2 | 600/2222 batches | lr 0.0010 | ms/batch 17.39 | loss 3.2859 | bpc 4.7715
| epoch 2 | 800/2222 batches | lr 0.0010 | ms/batch 17.39 | loss 3.2675 | bpc 4.7391
| epoch 2 | 1000/2222 batches | lr 0.0010 | ms/batch 17.36 | loss 3.2565 | bpc 4.7227
| epoch 2 | 1200/2222 batches | lr 0.0010 | ms/batch 17.31 | loss 3.2700 | bpc 4.7407
| epoch 2 | 1400/2222 batches | lr 0.0010 | ms/batch 17.28 | loss 3.2565 | bpc 4.7191
| epoch 2 | 1600/2222 batches | lr 0.0010 | ms/batch 17.26 | loss 3.3250 | bpc 4.8112
| epoch 2 | 1800/2222 batches | lr 0.0010 | ms/batch 17.22 | loss 3.2609 | bpc 4.7215
| epoch 2 | 2000/2222 batches | lr 0.0010 | ms/batch 17.20 | loss 3.2633 | bpc 4.7372
| epoch 2 | 2200/2222 batches | lr 0.0010 | ms/batch 17.17 | loss 3.2523 | bpc 4.7157
--------------------------------------------------------------------------------
| end of epoch 2 | time: 39.62s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9553 | valid bpc 11.4846 
--------------------------------------------------------------------------------
Start Training
| epoch 3 | 200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2885 | bpc 4.7652
| epoch 3 | 400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 3.3104 | bpc 4.8104
| epoch 3 | 600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 3.2842 | bpc 4.7692
| epoch 3 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 3.2683 | bpc 4.7403
| epoch 3 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 3.2590 | bpc 4.7266
| epoch 3 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 3.2696 | bpc 4.7403
| epoch 3 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 3.2548 | bpc 4.7164
| epoch 3 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 3.3258 | bpc 4.8123
| epoch 3 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 3.2603 | bpc 4.7208
| epoch 3 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 3.2652 | bpc 4.7402
| epoch 3 | 2200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 3.2524 | bpc 4.7155
--------------------------------------------------------------------------------
| end of epoch 3 | time: 38.99s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9557 | valid bpc 11.4852 
--------------------------------------------------------------------------------
Start Training
| epoch 4 | 200/2222 batches | lr 0.0010 | ms/batch 17.19 | loss 3.2881 | bpc 4.7649
| epoch 4 | 400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 3.3100 | bpc 4.8097
| epoch 4 | 600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2840 | bpc 4.7689
| epoch 4 | 800/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 3.2687 | bpc 4.7411
| epoch 4 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 3.2577 | bpc 4.7242
| epoch 4 | 1200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 3.2696 | bpc 4.7406
| epoch 4 | 1400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2552 | bpc 4.7170
| epoch 4 | 1600/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 3.3264 | bpc 4.8134
| epoch 4 | 1800/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2589 | bpc 4.7185
| epoch 4 | 2000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2654 | bpc 4.7408
| epoch 4 | 2200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2508 | bpc 4.7130
--------------------------------------------------------------------------------
| end of epoch 4 | time: 39.39s | train loss 3.2753 | train bpc 4.7494 | valid loss 7.9544 | valid bpc 11.4835 
--------------------------------------------------------------------------------
Start Training
| epoch 5 | 200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2881 | bpc 4.7650
| epoch 5 | 400/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.3105 | bpc 4.8108
| epoch 5 | 600/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2848 | bpc 4.7699
| epoch 5 | 800/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2675 | bpc 4.7394
| epoch 5 | 1000/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2586 | bpc 4.7257
| epoch 5 | 1200/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2698 | bpc 4.7407
| epoch 5 | 1400/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2550 | bpc 4.7168
| epoch 5 | 1600/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3241 | bpc 4.8099
| epoch 5 | 1800/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2607 | bpc 4.7212
| epoch 5 | 2000/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2629 | bpc 4.7368
| epoch 5 | 2200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2516 | bpc 4.7143
--------------------------------------------------------------------------------
| end of epoch 5 | time: 39.19s | train loss 3.2752 | train bpc 4.7492 | valid loss 7.9545 | valid bpc 11.4833 
--------------------------------------------------------------------------------
Start Training
| epoch 6 | 200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2901 | bpc 4.7678
| epoch 6 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3108 | bpc 4.8113
| epoch 6 | 600/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2838 | bpc 4.7685
| epoch 6 | 800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2669 | bpc 4.7384
| epoch 6 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2578 | bpc 4.7247
| epoch 6 | 1200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2701 | bpc 4.7410
| epoch 6 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2545 | bpc 4.7160
| epoch 6 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3260 | bpc 4.8130
| epoch 6 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2610 | bpc 4.7212
| epoch 6 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2634 | bpc 4.7378
| epoch 6 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2526 | bpc 4.7160
--------------------------------------------------------------------------------
| end of epoch 6 | time: 39.32s | train loss 3.2755 | train bpc 4.7497 | valid loss 7.9538 | valid bpc 11.4825 
--------------------------------------------------------------------------------
Start Training
| epoch 7 | 200/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2880 | bpc 4.7645
| epoch 7 | 400/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3115 | bpc 4.8123
| epoch 7 | 600/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2846 | bpc 4.7698
| epoch 7 | 800/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2681 | bpc 4.7402
| epoch 7 | 1000/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2579 | bpc 4.7247
| epoch 7 | 1200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2714 | bpc 4.7427
| epoch 7 | 1400/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2563 | bpc 4.7186
| epoch 7 | 1600/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.3258 | bpc 4.8125
| epoch 7 | 1800/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2597 | bpc 4.7198
| epoch 7 | 2000/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2649 | bpc 4.7399
| epoch 7 | 2200/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2511 | bpc 4.7133
--------------------------------------------------------------------------------
| end of epoch 7 | time: 39.17s | train loss 3.2757 | train bpc 4.7499 | valid loss 7.9544 | valid bpc 11.4832 
--------------------------------------------------------------------------------
Start Training
| epoch 8 | 200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2877 | bpc 4.7639
| epoch 8 | 400/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3113 | bpc 4.8120
| epoch 8 | 600/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2845 | bpc 4.7696
| epoch 8 | 800/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2682 | bpc 4.7404
| epoch 8 | 1000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2571 | bpc 4.7236
| epoch 8 | 1200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2693 | bpc 4.7402
| epoch 8 | 1400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2554 | bpc 4.7173
| epoch 8 | 1600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3259 | bpc 4.8127
| epoch 8 | 1800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2597 | bpc 4.7195
| epoch 8 | 2000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2649 | bpc 4.7399
| epoch 8 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2523 | bpc 4.7153
--------------------------------------------------------------------------------
| end of epoch 8 | time: 39.24s | train loss 3.2754 | train bpc 4.7495 | valid loss 7.9555 | valid bpc 11.4851 
--------------------------------------------------------------------------------
Start Training
| epoch 9 | 200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2879 | bpc 4.7649
| epoch 9 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3109 | bpc 4.8110
| epoch 9 | 600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2844 | bpc 4.7697
| epoch 9 | 800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2678 | bpc 4.7394
| epoch 9 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2584 | bpc 4.7258
| epoch 9 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2694 | bpc 4.7399
| epoch 9 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2545 | bpc 4.7158
| epoch 9 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3256 | bpc 4.8122
| epoch 9 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2602 | bpc 4.7205
| epoch 9 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2640 | bpc 4.7383
| epoch 9 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2525 | bpc 4.7160
--------------------------------------------------------------------------------
| end of epoch 9 | time: 39.55s | train loss 3.2754 | train bpc 4.7495 | valid loss 7.9528 | valid bpc 11.4809 
--------------------------------------------------------------------------------
Start Training
| epoch 10 | 200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2879 | bpc 4.7646
| epoch 10 | 400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3119 | bpc 4.8126
| epoch 10 | 600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2843 | bpc 4.7694
| epoch 10 | 800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2687 | bpc 4.7409
| epoch 10 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2586 | bpc 4.7258
| epoch 10 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2696 | bpc 4.7402
| epoch 10 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2557 | bpc 4.7178
| epoch 10 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3259 | bpc 4.8126
| epoch 10 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2613 | bpc 4.7220
| epoch 10 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2630 | bpc 4.7371
| epoch 10 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2521 | bpc 4.7151
--------------------------------------------------------------------------------
| end of epoch 10 | time: 39.53s | train loss 3.2756 | train bpc 4.7499 | valid loss 7.9550 | valid bpc 11.4843 
--------------------------------------------------------------------------------
Start Training
| epoch 11 | 200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2882 | bpc 4.7648
| epoch 11 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3114 | bpc 4.8123
| epoch 11 | 600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2854 | bpc 4.7709
| epoch 11 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2692 | bpc 4.7416
| epoch 11 | 1000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2571 | bpc 4.7237
| epoch 11 | 1200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2704 | bpc 4.7412
| epoch 11 | 1400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2549 | bpc 4.7165
| epoch 11 | 1600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3255 | bpc 4.8121
| epoch 11 | 1800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2608 | bpc 4.7214
| epoch 11 | 2000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2637 | bpc 4.7378
| epoch 11 | 2200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2521 | bpc 4.7153
--------------------------------------------------------------------------------
| end of epoch 11 | time: 39.56s | train loss 3.2756 | train bpc 4.7497 | valid loss 7.9562 | valid bpc 11.4856 
--------------------------------------------------------------------------------
Start Training
| epoch 12 | 200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2884 | bpc 4.7654
| epoch 12 | 400/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3113 | bpc 4.8121
| epoch 12 | 600/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2850 | bpc 4.7701
| epoch 12 | 800/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2675 | bpc 4.7392
| epoch 12 | 1000/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2588 | bpc 4.7261
| epoch 12 | 1200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2715 | bpc 4.7429
| epoch 12 | 1400/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2557 | bpc 4.7178
| epoch 12 | 1600/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3251 | bpc 4.8115
| epoch 12 | 1800/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2595 | bpc 4.7195
| epoch 12 | 2000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2636 | bpc 4.7378
| epoch 12 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2521 | bpc 4.7151
--------------------------------------------------------------------------------
| end of epoch 12 | time: 39.55s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9565 | valid bpc 11.4867 
--------------------------------------------------------------------------------
Start Training
| epoch 13 | 200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2875 | bpc 4.7639
| epoch 13 | 400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3113 | bpc 4.8118
| epoch 13 | 600/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2857 | bpc 4.7713
| epoch 13 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2685 | bpc 4.7409
| epoch 13 | 1000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2581 | bpc 4.7250
| epoch 13 | 1200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2707 | bpc 4.7417
| epoch 13 | 1400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2550 | bpc 4.7170
| epoch 13 | 1600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3257 | bpc 4.8123
| epoch 13 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2601 | bpc 4.7204
| epoch 13 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2647 | bpc 4.7396
| epoch 13 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2517 | bpc 4.7144
--------------------------------------------------------------------------------
| end of epoch 13 | time: 39.31s | train loss 3.2757 | train bpc 4.7499 | valid loss 7.9542 | valid bpc 11.4829 
--------------------------------------------------------------------------------
Start Training
| epoch 14 | 200/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2870 | bpc 4.7634
| epoch 14 | 400/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.3108 | bpc 4.8111
| epoch 14 | 600/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2852 | bpc 4.7705
| epoch 14 | 800/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2674 | bpc 4.7392
| epoch 14 | 1000/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2580 | bpc 4.7248
| epoch 14 | 1200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2705 | bpc 4.7415
| epoch 14 | 1400/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2556 | bpc 4.7175
| epoch 14 | 1600/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3259 | bpc 4.8127
| epoch 14 | 1800/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2600 | bpc 4.7202
| epoch 14 | 2000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2650 | bpc 4.7399
| epoch 14 | 2200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2515 | bpc 4.7143
--------------------------------------------------------------------------------
| end of epoch 14 | time: 39.22s | train loss 3.2755 | train bpc 4.7497 | valid loss 7.9555 | valid bpc 11.4847 
--------------------------------------------------------------------------------
Start Training
| epoch 15 | 200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2876 | bpc 4.7641
| epoch 15 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3111 | bpc 4.8116
| epoch 15 | 600/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2851 | bpc 4.7705
| epoch 15 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2685 | bpc 4.7408
| epoch 15 | 1000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2573 | bpc 4.7239
| epoch 15 | 1200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2695 | bpc 4.7398
| epoch 15 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2554 | bpc 4.7176
| epoch 15 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3252 | bpc 4.8113
| epoch 15 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2598 | bpc 4.7200
| epoch 15 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2654 | bpc 4.7404
| epoch 15 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2513 | bpc 4.7140
--------------------------------------------------------------------------------
| end of epoch 15 | time: 39.24s | train loss 3.2754 | train bpc 4.7496 | valid loss 7.9542 | valid bpc 11.4830 
--------------------------------------------------------------------------------
Start Training
| epoch 16 | 200/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 3.2873 | bpc 4.7634
| epoch 16 | 400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 3.3105 | bpc 4.8108
| epoch 16 | 600/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2857 | bpc 4.7713
| epoch 16 | 800/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2680 | bpc 4.7400
| epoch 16 | 1000/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2577 | bpc 4.7245
| epoch 16 | 1200/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2707 | bpc 4.7417
| epoch 16 | 1400/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2545 | bpc 4.7161
| epoch 16 | 1600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.3245 | bpc 4.8105
| epoch 16 | 1800/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2620 | bpc 4.7230
| epoch 16 | 2000/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2660 | bpc 4.7415
| epoch 16 | 2200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2519 | bpc 4.7145
--------------------------------------------------------------------------------
| end of epoch 16 | time: 39.18s | train loss 3.2756 | train bpc 4.7499 | valid loss 7.9540 | valid bpc 11.4829 
--------------------------------------------------------------------------------
Start Training
| epoch 17 | 200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2880 | bpc 4.7647
| epoch 17 | 400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3108 | bpc 4.8113
| epoch 17 | 600/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 3.2843 | bpc 4.7691
| epoch 17 | 800/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 3.2677 | bpc 4.7397
| epoch 17 | 1000/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 3.2584 | bpc 4.7254
| epoch 17 | 1200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 3.2696 | bpc 4.7401
| epoch 17 | 1400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2550 | bpc 4.7171
| epoch 17 | 1600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3267 | bpc 4.8136
| epoch 17 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2602 | bpc 4.7205
| epoch 17 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2652 | bpc 4.7401
| epoch 17 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2510 | bpc 4.7135
--------------------------------------------------------------------------------
| end of epoch 17 | time: 39.30s | train loss 3.2755 | train bpc 4.7496 | valid loss 7.9543 | valid bpc 11.4830 
--------------------------------------------------------------------------------
Start Training
| epoch 18 | 200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2878 | bpc 4.7642
| epoch 18 | 400/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.3111 | bpc 4.8119
| epoch 18 | 600/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2854 | bpc 4.7707
| epoch 18 | 800/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2678 | bpc 4.7397
| epoch 18 | 1000/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2583 | bpc 4.7254
| epoch 18 | 1200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2706 | bpc 4.7417
| epoch 18 | 1400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2549 | bpc 4.7166
| epoch 18 | 1600/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3258 | bpc 4.8127
| epoch 18 | 1800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2613 | bpc 4.7219
| epoch 18 | 2000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2640 | bpc 4.7388
| epoch 18 | 2200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2524 | bpc 4.7153
--------------------------------------------------------------------------------
| end of epoch 18 | time: 39.25s | train loss 3.2757 | train bpc 4.7499 | valid loss 7.9560 | valid bpc 11.4856 
--------------------------------------------------------------------------------
Start Training
| epoch 19 | 200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2872 | bpc 4.7635
| epoch 19 | 400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3104 | bpc 4.8106
| epoch 19 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2851 | bpc 4.7706
| epoch 19 | 800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2672 | bpc 4.7386
| epoch 19 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2596 | bpc 4.7274
| epoch 19 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2691 | bpc 4.7393
| epoch 19 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2559 | bpc 4.7182
| epoch 19 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3251 | bpc 4.8115
| epoch 19 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2605 | bpc 4.7209
| epoch 19 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2650 | bpc 4.7401
| epoch 19 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2531 | bpc 4.7166
--------------------------------------------------------------------------------
| end of epoch 19 | time: 39.28s | train loss 3.2756 | train bpc 4.7497 | valid loss 7.9541 | valid bpc 11.4829 
--------------------------------------------------------------------------------
Start Training
| epoch 20 | 200/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2882 | bpc 4.7650
| epoch 20 | 400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3104 | bpc 4.8106
| epoch 20 | 600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2851 | bpc 4.7703
| epoch 20 | 800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2680 | bpc 4.7399
| epoch 20 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2588 | bpc 4.7261
| epoch 20 | 1200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2706 | bpc 4.7416
| epoch 20 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2559 | bpc 4.7181
| epoch 20 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3250 | bpc 4.8117
| epoch 20 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2596 | bpc 4.7193
| epoch 20 | 2000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2650 | bpc 4.7401
| epoch 20 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2504 | bpc 4.7124
--------------------------------------------------------------------------------
| end of epoch 20 | time: 39.24s | train loss 3.2755 | train bpc 4.7497 | valid loss 7.9529 | valid bpc 11.4812 
--------------------------------------------------------------------------------
Start Training
| epoch 21 | 200/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2874 | bpc 4.7636
| epoch 21 | 400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3098 | bpc 4.8098
| epoch 21 | 600/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2849 | bpc 4.7702
| epoch 21 | 800/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2670 | bpc 4.7384
| epoch 21 | 1000/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2596 | bpc 4.7273
| epoch 21 | 1200/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2713 | bpc 4.7426
| epoch 21 | 1400/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2548 | bpc 4.7164
| epoch 21 | 1600/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3259 | bpc 4.8126
| epoch 21 | 1800/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2620 | bpc 4.7229
| epoch 21 | 2000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2646 | bpc 4.7396
| epoch 21 | 2200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2513 | bpc 4.7140
--------------------------------------------------------------------------------
| end of epoch 21 | time: 39.18s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9544 | valid bpc 11.4836 
--------------------------------------------------------------------------------
Start Training
| epoch 22 | 200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2886 | bpc 4.7655
| epoch 22 | 400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3113 | bpc 4.8120
| epoch 22 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2845 | bpc 4.7699
| epoch 22 | 800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2672 | bpc 4.7387
| epoch 22 | 1000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2584 | bpc 4.7253
| epoch 22 | 1200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2700 | bpc 4.7408
| epoch 22 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2562 | bpc 4.7187
| epoch 22 | 1600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3259 | bpc 4.8124
| epoch 22 | 1800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2597 | bpc 4.7198
| epoch 22 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2641 | bpc 4.7385
| epoch 22 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2518 | bpc 4.7148
--------------------------------------------------------------------------------
| end of epoch 22 | time: 39.32s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9548 | valid bpc 11.4837 
--------------------------------------------------------------------------------
Start Training
| epoch 23 | 200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2883 | bpc 4.7649
| epoch 23 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3112 | bpc 4.8116
| epoch 23 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2844 | bpc 4.7693
| epoch 23 | 800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2690 | bpc 4.7413
| epoch 23 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2579 | bpc 4.7250
| epoch 23 | 1200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2704 | bpc 4.7417
| epoch 23 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2556 | bpc 4.7175
| epoch 23 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3243 | bpc 4.8104
| epoch 23 | 1800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2597 | bpc 4.7197
| epoch 23 | 2000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2660 | bpc 4.7413
| epoch 23 | 2200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2512 | bpc 4.7139
--------------------------------------------------------------------------------
| end of epoch 23 | time: 39.23s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9549 | valid bpc 11.4842 
--------------------------------------------------------------------------------
Start Training
| epoch 24 | 200/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2881 | bpc 4.7647
| epoch 24 | 400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3114 | bpc 4.8120
| epoch 24 | 600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2843 | bpc 4.7692
| epoch 24 | 800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2677 | bpc 4.7398
| epoch 24 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2587 | bpc 4.7259
| epoch 24 | 1200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2707 | bpc 4.7419
| epoch 24 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2540 | bpc 4.7155
| epoch 24 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3254 | bpc 4.8119
| epoch 24 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2604 | bpc 4.7206
| epoch 24 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2662 | bpc 4.7420
| epoch 24 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2515 | bpc 4.7141
--------------------------------------------------------------------------------
| end of epoch 24 | time: 39.31s | train loss 3.2757 | train bpc 4.7499 | valid loss 7.9540 | valid bpc 11.4826 
--------------------------------------------------------------------------------
Start Training
| epoch 25 | 200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2879 | bpc 4.7647
| epoch 25 | 400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3104 | bpc 4.8103
| epoch 25 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2849 | bpc 4.7703
| epoch 25 | 800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2692 | bpc 4.7418
| epoch 25 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2580 | bpc 4.7250
| epoch 25 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2707 | bpc 4.7417
| epoch 25 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2562 | bpc 4.7185
| epoch 25 | 1600/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 3.3243 | bpc 4.8103
| epoch 25 | 1800/2222 batches | lr 0.0010 | ms/batch 17.11 | loss 3.2609 | bpc 4.7215
| epoch 25 | 2000/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 3.2636 | bpc 4.7378
| epoch 25 | 2200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 3.2518 | bpc 4.7148
--------------------------------------------------------------------------------
| end of epoch 25 | time: 39.48s | train loss 3.2755 | train bpc 4.7497 | valid loss 7.9532 | valid bpc 11.4815 
--------------------------------------------------------------------------------
Start Training
| epoch 26 | 200/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2885 | bpc 4.7655
| epoch 26 | 400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3103 | bpc 4.8105
| epoch 26 | 600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2856 | bpc 4.7712
| epoch 26 | 800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2672 | bpc 4.7385
| epoch 26 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2582 | bpc 4.7251
| epoch 26 | 1200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2709 | bpc 4.7421
| epoch 26 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2551 | bpc 4.7170
| epoch 26 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3254 | bpc 4.8120
| epoch 26 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2594 | bpc 4.7192
| epoch 26 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2647 | bpc 4.7397
| epoch 26 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2515 | bpc 4.7142
--------------------------------------------------------------------------------
| end of epoch 26 | time: 39.32s | train loss 3.2754 | train bpc 4.7495 | valid loss 7.9555 | valid bpc 11.4849 
--------------------------------------------------------------------------------
Start Training
| epoch 27 | 200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2893 | bpc 4.7667
| epoch 27 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3097 | bpc 4.8095
| epoch 27 | 600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2851 | bpc 4.7705
| epoch 27 | 800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2682 | bpc 4.7402
| epoch 27 | 1000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2595 | bpc 4.7271
| epoch 27 | 1200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2714 | bpc 4.7428
| epoch 27 | 1400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2545 | bpc 4.7162
| epoch 27 | 1600/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3238 | bpc 4.8096
| epoch 27 | 1800/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2622 | bpc 4.7233
| epoch 27 | 2000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2651 | bpc 4.7402
| epoch 27 | 2200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2503 | bpc 4.7123
--------------------------------------------------------------------------------
| end of epoch 27 | time: 39.20s | train loss 3.2757 | train bpc 4.7499 | valid loss 7.9535 | valid bpc 11.4823 
--------------------------------------------------------------------------------
Start Training
| epoch 28 | 200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2893 | bpc 4.7663
| epoch 28 | 400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3114 | bpc 4.8123
| epoch 28 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2850 | bpc 4.7702
| epoch 28 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2684 | bpc 4.7405
| epoch 28 | 1000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2578 | bpc 4.7246
| epoch 28 | 1200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2706 | bpc 4.7415
| epoch 28 | 1400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2537 | bpc 4.7153
| epoch 28 | 1600/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.3260 | bpc 4.8124
| epoch 28 | 1800/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2607 | bpc 4.7215
| epoch 28 | 2000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2652 | bpc 4.7402
| epoch 28 | 2200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2514 | bpc 4.7140
--------------------------------------------------------------------------------
| end of epoch 28 | time: 39.16s | train loss 3.2757 | train bpc 4.7500 | valid loss 7.9557 | valid bpc 11.4851 
--------------------------------------------------------------------------------
Start Training
| epoch 29 | 200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2876 | bpc 4.7642
| epoch 29 | 400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3102 | bpc 4.8103
| epoch 29 | 600/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2846 | bpc 4.7697
| epoch 29 | 800/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2674 | bpc 4.7392
| epoch 29 | 1000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2570 | bpc 4.7234
| epoch 29 | 1200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2705 | bpc 4.7415
| epoch 29 | 1400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2546 | bpc 4.7163
| epoch 29 | 1600/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3269 | bpc 4.8139
| epoch 29 | 1800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2612 | bpc 4.7221
| epoch 29 | 2000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2649 | bpc 4.7396
| epoch 29 | 2200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2524 | bpc 4.7156
--------------------------------------------------------------------------------
| end of epoch 29 | time: 39.21s | train loss 3.2755 | train bpc 4.7496 | valid loss 7.9544 | valid bpc 11.4834 
--------------------------------------------------------------------------------
Start Training
| epoch 30 | 200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2882 | bpc 4.7649
| epoch 30 | 400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3125 | bpc 4.8137
| epoch 30 | 600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2843 | bpc 4.7692
| epoch 30 | 800/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2675 | bpc 4.7394
| epoch 30 | 1000/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2578 | bpc 4.7245
| epoch 30 | 1200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2709 | bpc 4.7419
| epoch 30 | 1400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2556 | bpc 4.7177
| epoch 30 | 1600/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.3254 | bpc 4.8119
| epoch 30 | 1800/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2605 | bpc 4.7208
| epoch 30 | 2000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2638 | bpc 4.7384
| epoch 30 | 2200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2503 | bpc 4.7125
--------------------------------------------------------------------------------
| end of epoch 30 | time: 39.19s | train loss 3.2755 | train bpc 4.7496 | valid loss 7.9557 | valid bpc 11.4850 
--------------------------------------------------------------------------------
Start Training
| epoch 31 | 200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2888 | bpc 4.7660
| epoch 31 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3101 | bpc 4.8102
| epoch 31 | 600/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2848 | bpc 4.7699
| epoch 31 | 800/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2684 | bpc 4.7406
| epoch 31 | 1000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2580 | bpc 4.7249
| epoch 31 | 1200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2715 | bpc 4.7430
| epoch 31 | 1400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2546 | bpc 4.7164
| epoch 31 | 1600/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3243 | bpc 4.8101
| epoch 31 | 1800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2601 | bpc 4.7204
| epoch 31 | 2000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2650 | bpc 4.7400
| epoch 31 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2514 | bpc 4.7140
--------------------------------------------------------------------------------
| end of epoch 31 | time: 39.21s | train loss 3.2755 | train bpc 4.7497 | valid loss 7.9546 | valid bpc 11.4834 
--------------------------------------------------------------------------------
Start Training
| epoch 32 | 200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 3.2873 | bpc 4.7635
| epoch 32 | 400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3100 | bpc 4.8104
| epoch 32 | 600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2858 | bpc 4.7713
| epoch 32 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2684 | bpc 4.7406
| epoch 32 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2583 | bpc 4.7253
| epoch 32 | 1200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2698 | bpc 4.7405
| epoch 32 | 1400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2552 | bpc 4.7173
| epoch 32 | 1600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3257 | bpc 4.8124
| epoch 32 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2608 | bpc 4.7213
| epoch 32 | 2000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2643 | bpc 4.7388
| epoch 32 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2520 | bpc 4.7147
--------------------------------------------------------------------------------
| end of epoch 32 | time: 39.21s | train loss 3.2755 | train bpc 4.7496 | valid loss 7.9563 | valid bpc 11.4860 
--------------------------------------------------------------------------------
Start Training
| epoch 33 | 200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2873 | bpc 4.7636
| epoch 33 | 400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3107 | bpc 4.8111
| epoch 33 | 600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2852 | bpc 4.7706
| epoch 33 | 800/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2691 | bpc 4.7416
| epoch 33 | 1000/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2578 | bpc 4.7246
| epoch 33 | 1200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2710 | bpc 4.7423
| epoch 33 | 1400/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2552 | bpc 4.7171
| epoch 33 | 1600/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3253 | bpc 4.8117
| epoch 33 | 1800/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2599 | bpc 4.7202
| epoch 33 | 2000/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2660 | bpc 4.7415
| epoch 33 | 2200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2522 | bpc 4.7151
--------------------------------------------------------------------------------
| end of epoch 33 | time: 39.15s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9556 | valid bpc 11.4851 
--------------------------------------------------------------------------------
Start Training
| epoch 34 | 200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2884 | bpc 4.7650
| epoch 34 | 400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3116 | bpc 4.8127
| epoch 34 | 600/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2850 | bpc 4.7702
| epoch 34 | 800/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2684 | bpc 4.7405
| epoch 34 | 1000/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2583 | bpc 4.7254
| epoch 34 | 1200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2702 | bpc 4.7413
| epoch 34 | 1400/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2558 | bpc 4.7179
| epoch 34 | 1600/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.3256 | bpc 4.8122
| epoch 34 | 1800/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2599 | bpc 4.7200
| epoch 34 | 2000/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2653 | bpc 4.7406
| epoch 34 | 2200/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2514 | bpc 4.7140
--------------------------------------------------------------------------------
| end of epoch 34 | time: 39.05s | train loss 3.2758 | train bpc 4.7501 | valid loss 7.9538 | valid bpc 11.4823 
--------------------------------------------------------------------------------
Start Training
| epoch 35 | 200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2875 | bpc 4.7641
| epoch 35 | 400/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.3105 | bpc 4.8104
| epoch 35 | 600/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 3.2840 | bpc 4.7688
| epoch 35 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 3.2681 | bpc 4.7402
| epoch 35 | 1000/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2594 | bpc 4.7270
| epoch 35 | 1200/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2707 | bpc 4.7417
| epoch 35 | 1400/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2555 | bpc 4.7177
| epoch 35 | 1600/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.3248 | bpc 4.8109
| epoch 35 | 1800/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2607 | bpc 4.7213
| epoch 35 | 2000/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2665 | bpc 4.7421
| epoch 35 | 2200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2510 | bpc 4.7134
--------------------------------------------------------------------------------
| end of epoch 35 | time: 39.16s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9541 | valid bpc 11.4830 
--------------------------------------------------------------------------------
Start Training
| epoch 36 | 200/2222 batches | lr 0.0010 | ms/batch 17.13 | loss 3.2885 | bpc 4.7652
| epoch 36 | 400/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 3.3108 | bpc 4.8115
| epoch 36 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2851 | bpc 4.7702
| epoch 36 | 800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2673 | bpc 4.7393
| epoch 36 | 1000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2593 | bpc 4.7265
| epoch 36 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2698 | bpc 4.7405
| epoch 36 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2557 | bpc 4.7179
| epoch 36 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3252 | bpc 4.8116
| epoch 36 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2601 | bpc 4.7203
| epoch 36 | 2000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2652 | bpc 4.7405
| epoch 36 | 2200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2519 | bpc 4.7146
--------------------------------------------------------------------------------
| end of epoch 36 | time: 39.23s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9520 | valid bpc 11.4802 
--------------------------------------------------------------------------------
Start Training
| epoch 37 | 200/2222 batches | lr 0.0010 | ms/batch 17.16 | loss 3.2879 | bpc 4.7643
| epoch 37 | 400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3111 | bpc 4.8118
| epoch 37 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2850 | bpc 4.7701
| epoch 37 | 800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2687 | bpc 4.7411
| epoch 37 | 1000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2577 | bpc 4.7243
| epoch 37 | 1200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2707 | bpc 4.7415
| epoch 37 | 1400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2555 | bpc 4.7179
| epoch 37 | 1600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3251 | bpc 4.8113
| epoch 37 | 1800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2604 | bpc 4.7209
| epoch 37 | 2000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2627 | bpc 4.7367
| epoch 37 | 2200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2511 | bpc 4.7134
--------------------------------------------------------------------------------
| end of epoch 37 | time: 39.36s | train loss 3.2754 | train bpc 4.7495 | valid loss 7.9570 | valid bpc 11.4871 
--------------------------------------------------------------------------------
Start Training
| epoch 38 | 200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2885 | bpc 4.7653
| epoch 38 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3106 | bpc 4.8113
| epoch 38 | 600/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2852 | bpc 4.7705
| epoch 38 | 800/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2688 | bpc 4.7412
| epoch 38 | 1000/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2585 | bpc 4.7256
| epoch 38 | 1200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2701 | bpc 4.7411
| epoch 38 | 1400/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2556 | bpc 4.7173
| epoch 38 | 1600/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3256 | bpc 4.8121
| epoch 38 | 1800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2612 | bpc 4.7221
| epoch 38 | 2000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2636 | bpc 4.7379
| epoch 38 | 2200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2517 | bpc 4.7142
--------------------------------------------------------------------------------
| end of epoch 38 | time: 39.24s | train loss 3.2757 | train bpc 4.7500 | valid loss 7.9541 | valid bpc 11.4828 
--------------------------------------------------------------------------------
Start Training
| epoch 39 | 200/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2884 | bpc 4.7650
| epoch 39 | 400/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.3121 | bpc 4.8132
| epoch 39 | 600/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2842 | bpc 4.7692
| epoch 39 | 800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2684 | bpc 4.7406
| epoch 39 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2582 | bpc 4.7250
| epoch 39 | 1200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2682 | bpc 4.7383
| epoch 39 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2556 | bpc 4.7175
| epoch 39 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3261 | bpc 4.8128
| epoch 39 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2614 | bpc 4.7223
| epoch 39 | 2000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2638 | bpc 4.7385
| epoch 39 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2515 | bpc 4.7140
--------------------------------------------------------------------------------
| end of epoch 39 | time: 39.23s | train loss 3.2755 | train bpc 4.7497 | valid loss 7.9582 | valid bpc 11.4887 
--------------------------------------------------------------------------------
Start Training
| epoch 40 | 200/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 3.2898 | bpc 4.7674
| epoch 40 | 400/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.3098 | bpc 4.8096
| epoch 40 | 600/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2853 | bpc 4.7707
| epoch 40 | 800/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2664 | bpc 4.7375
| epoch 40 | 1000/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2585 | bpc 4.7257
| epoch 40 | 1200/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2707 | bpc 4.7419
| epoch 40 | 1400/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2555 | bpc 4.7175
| epoch 40 | 1600/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3264 | bpc 4.8132
| epoch 40 | 1800/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2587 | bpc 4.7184
| epoch 40 | 2000/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2647 | bpc 4.7395
| epoch 40 | 2200/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2517 | bpc 4.7145
--------------------------------------------------------------------------------
| end of epoch 40 | time: 39.14s | train loss 3.2755 | train bpc 4.7497 | valid loss 7.9557 | valid bpc 11.4850 
--------------------------------------------------------------------------------
Start Training
| epoch 41 | 200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2879 | bpc 4.7647
| epoch 41 | 400/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.3112 | bpc 4.8116
| epoch 41 | 600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2863 | bpc 4.7722
| epoch 41 | 800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2678 | bpc 4.7396
| epoch 41 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2579 | bpc 4.7248
| epoch 41 | 1200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2707 | bpc 4.7418
| epoch 41 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2544 | bpc 4.7159
| epoch 41 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3257 | bpc 4.8124
| epoch 41 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2608 | bpc 4.7212
| epoch 41 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2653 | bpc 4.7405
| epoch 41 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2507 | bpc 4.7130
--------------------------------------------------------------------------------
| end of epoch 41 | time: 39.53s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9535 | valid bpc 11.4824 
--------------------------------------------------------------------------------
Start Training
| epoch 42 | 200/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 3.2885 | bpc 4.7657
| epoch 42 | 400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 3.3097 | bpc 4.8093
| epoch 42 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 3.2858 | bpc 4.7716
| epoch 42 | 800/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2678 | bpc 4.7398
| epoch 42 | 1000/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2575 | bpc 4.7240
| epoch 42 | 1200/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2709 | bpc 4.7420
| epoch 42 | 1400/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2548 | bpc 4.7165
| epoch 42 | 1600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.3253 | bpc 4.8119
| epoch 42 | 1800/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2594 | bpc 4.7192
| epoch 42 | 2000/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2671 | bpc 4.7432
| epoch 42 | 2200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2508 | bpc 4.7130
--------------------------------------------------------------------------------
| end of epoch 42 | time: 39.34s | train loss 3.2755 | train bpc 4.7497 | valid loss 7.9561 | valid bpc 11.4859 
--------------------------------------------------------------------------------
Start Training
| epoch 43 | 200/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2876 | bpc 4.7641
| epoch 43 | 400/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 3.3107 | bpc 4.8113
| epoch 43 | 600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2848 | bpc 4.7699
| epoch 43 | 800/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2681 | bpc 4.7402
| epoch 43 | 1000/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2589 | bpc 4.7263
| epoch 43 | 1200/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2719 | bpc 4.7435
| epoch 43 | 1400/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2546 | bpc 4.7164
| epoch 43 | 1600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.3249 | bpc 4.8111
| epoch 43 | 1800/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2601 | bpc 4.7204
| epoch 43 | 2000/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2659 | bpc 4.7415
| epoch 43 | 2200/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2510 | bpc 4.7134
--------------------------------------------------------------------------------
| end of epoch 43 | time: 39.52s | train loss 3.2756 | train bpc 4.7499 | valid loss 7.9524 | valid bpc 11.4803 
--------------------------------------------------------------------------------
Start Training
| epoch 44 | 200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2877 | bpc 4.7642
| epoch 44 | 400/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.3105 | bpc 4.8109
| epoch 44 | 600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2863 | bpc 4.7721
| epoch 44 | 800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2681 | bpc 4.7401
| epoch 44 | 1000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2579 | bpc 4.7250
| epoch 44 | 1200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2710 | bpc 4.7420
| epoch 44 | 1400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2557 | bpc 4.7178
| epoch 44 | 1600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3256 | bpc 4.8122
| epoch 44 | 1800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2587 | bpc 4.7182
| epoch 44 | 2000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2641 | bpc 4.7387
| epoch 44 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2509 | bpc 4.7133
--------------------------------------------------------------------------------
| end of epoch 44 | time: 39.37s | train loss 3.2754 | train bpc 4.7495 | valid loss 7.9592 | valid bpc 11.4902 
--------------------------------------------------------------------------------
Start Training
| epoch 45 | 200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2888 | bpc 4.7659
| epoch 45 | 400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3095 | bpc 4.8091
| epoch 45 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2856 | bpc 4.7712
| epoch 45 | 800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2683 | bpc 4.7405
| epoch 45 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2580 | bpc 4.7248
| epoch 45 | 1200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2708 | bpc 4.7419
| epoch 45 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2555 | bpc 4.7174
| epoch 45 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3251 | bpc 4.8116
| epoch 45 | 1800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2598 | bpc 4.7200
| epoch 45 | 2000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2652 | bpc 4.7403
| epoch 45 | 2200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2518 | bpc 4.7147
--------------------------------------------------------------------------------
| end of epoch 45 | time: 39.23s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9546 | valid bpc 11.4838 
--------------------------------------------------------------------------------
Start Training
| epoch 46 | 200/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2881 | bpc 4.7647
| epoch 46 | 400/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.3106 | bpc 4.8112
| epoch 46 | 600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2856 | bpc 4.7708
| epoch 46 | 800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2683 | bpc 4.7404
| epoch 46 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2588 | bpc 4.7260
| epoch 46 | 1200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2713 | bpc 4.7427
| epoch 46 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2551 | bpc 4.7169
| epoch 46 | 1600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3254 | bpc 4.8118
| epoch 46 | 1800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2603 | bpc 4.7206
| epoch 46 | 2000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2650 | bpc 4.7399
| epoch 46 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2527 | bpc 4.7161
--------------------------------------------------------------------------------
| end of epoch 46 | time: 39.21s | train loss 3.2759 | train bpc 4.7502 | valid loss 7.9550 | valid bpc 11.4840 
--------------------------------------------------------------------------------
Start Training
| epoch 47 | 200/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2875 | bpc 4.7638
| epoch 47 | 400/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.3107 | bpc 4.8112
| epoch 47 | 600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2848 | bpc 4.7700
| epoch 47 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2669 | bpc 4.7384
| epoch 47 | 1000/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2583 | bpc 4.7251
| epoch 47 | 1200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2703 | bpc 4.7413
| epoch 47 | 1400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2546 | bpc 4.7161
| epoch 47 | 1600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3269 | bpc 4.8142
| epoch 47 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2608 | bpc 4.7213
| epoch 47 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2642 | bpc 4.7389
| epoch 47 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2515 | bpc 4.7140
--------------------------------------------------------------------------------
| end of epoch 47 | time: 39.26s | train loss 3.2754 | train bpc 4.7496 | valid loss 7.9558 | valid bpc 11.4852 
--------------------------------------------------------------------------------
Start Training
| epoch 48 | 200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2880 | bpc 4.7646
| epoch 48 | 400/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3097 | bpc 4.8096
| epoch 48 | 600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2841 | bpc 4.7689
| epoch 48 | 800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2681 | bpc 4.7402
| epoch 48 | 1000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2577 | bpc 4.7245
| epoch 48 | 1200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2707 | bpc 4.7416
| epoch 48 | 1400/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2562 | bpc 4.7184
| epoch 48 | 1600/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3257 | bpc 4.8124
| epoch 48 | 1800/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2597 | bpc 4.7198
| epoch 48 | 2000/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2647 | bpc 4.7394
| epoch 48 | 2200/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2534 | bpc 4.7171
--------------------------------------------------------------------------------
| end of epoch 48 | time: 39.12s | train loss 3.2756 | train bpc 4.7498 | valid loss 7.9531 | valid bpc 11.4815 
--------------------------------------------------------------------------------
Start Training
| epoch 49 | 200/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 3.2889 | bpc 4.7656
| epoch 49 | 400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 3.3108 | bpc 4.8115
| epoch 49 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 3.2856 | bpc 4.7711
| epoch 49 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 3.2685 | bpc 4.7406
| epoch 49 | 1000/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 3.2586 | bpc 4.7257
| epoch 49 | 1200/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2695 | bpc 4.7401
| epoch 49 | 1400/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2544 | bpc 4.7158
| epoch 49 | 1600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.3248 | bpc 4.8107
| epoch 49 | 1800/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2606 | bpc 4.7214
| epoch 49 | 2000/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2637 | bpc 4.7380
| epoch 49 | 2200/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2529 | bpc 4.7162
--------------------------------------------------------------------------------
| end of epoch 49 | time: 39.11s | train loss 3.2756 | train bpc 4.7497 | valid loss 7.9573 | valid bpc 11.4876 
--------------------------------------------------------------------------------
Start Training
| epoch 50 | 200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2871 | bpc 4.7630
| epoch 50 | 400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3119 | bpc 4.8129
| epoch 50 | 600/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2846 | bpc 4.7699
| epoch 50 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2683 | bpc 4.7404
| epoch 50 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2584 | bpc 4.7256
| epoch 50 | 1200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2713 | bpc 4.7427
| epoch 50 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2547 | bpc 4.7163
| epoch 50 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3259 | bpc 4.8128
| epoch 50 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2581 | bpc 4.7173
| epoch 50 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2645 | bpc 4.7393
| epoch 50 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2521 | bpc 4.7152
--------------------------------------------------------------------------------
| end of epoch 50 | time: 39.25s | train loss 3.2755 | train bpc 4.7496 | valid loss 7.9531 | valid bpc 11.4813 
--------------------------------------------------------------------------------
Run history:

epoch	
lr	
train_bpc	
train_loss	
val_bpc	
val_loss	

Run summary:

epoch	50
lr	0.001
train_bpc	4.74961
train_loss	3.27548
val_bpc	11.48131
val_loss	7.95308

View run stellar-thunder-21 at: https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/q423zdup
View project at: https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
Find logs at: ./wandb/run-20240822_210932-q423zdup/logs
Start Test Validation
Evaluating batch 1/124
Evaluating batch 2/124
Evaluating batch 3/124
Evaluating batch 4/124
Evaluating batch 5/124
Evaluating batch 6/124
Evaluating batch 7/124
Evaluating batch 8/124
Evaluating batch 9/124
Evaluating batch 10/124
Evaluating batch 11/124
Evaluating batch 12/124
Evaluating batch 13/124
Evaluating batch 14/124
Evaluating batch 15/124
Evaluating batch 16/124
Evaluating batch 17/124
Evaluating batch 18/124
Evaluating batch 19/124
Evaluating batch 20/124
Evaluating batch 21/124
Evaluating batch 22/124
Evaluating batch 23/124
Evaluating batch 24/124
Evaluating batch 25/124
Evaluating batch 26/124
Evaluating batch 27/124
Evaluating batch 28/124
Evaluating batch 29/124
Evaluating batch 30/124
Evaluating batch 31/124
Evaluating batch 32/124
Evaluating batch 33/124
Evaluating batch 34/124
Evaluating batch 35/124
Evaluating batch 36/124
Evaluating batch 37/124
Evaluating batch 38/124
Evaluating batch 39/124
Evaluating batch 40/124
Evaluating batch 41/124
Evaluating batch 42/124
Evaluating batch 43/124
Evaluating batch 44/124
Evaluating batch 45/124
Evaluating batch 46/124
Evaluating batch 47/124
Evaluating batch 48/124
Evaluating batch 49/124
Evaluating batch 50/124
Evaluating batch 51/124
Evaluating batch 52/124
Evaluating batch 53/124
Evaluating batch 54/124
Evaluating batch 55/124
Evaluating batch 56/124
Evaluating batch 57/124
Evaluating batch 58/124
Evaluating batch 59/124
Evaluating batch 60/124
Evaluating batch 61/124
Evaluating batch 62/124
Evaluating batch 63/124
Evaluating batch 64/124
Evaluating batch 65/124
Evaluating batch 66/124
Evaluating batch 67/124
Evaluating batch 68/124
Evaluating batch 69/124
Evaluating batch 70/124
Evaluating batch 71/124
Evaluating batch 72/124
Evaluating batch 73/124
Evaluating batch 74/124
Evaluating batch 75/124
Evaluating batch 76/124
Evaluating batch 77/124
Evaluating batch 78/124
Evaluating batch 79/124
Evaluating batch 80/124
Evaluating batch 81/124
Evaluating batch 82/124
Evaluating batch 83/124
Evaluating batch 84/124
Evaluating batch 85/124
Evaluating batch 86/124
Evaluating batch 87/124
Evaluating batch 88/124
Evaluating batch 89/124
Evaluating batch 90/124
Evaluating batch 91/124
Evaluating batch 92/124
Evaluating batch 93/124
Evaluating batch 94/124
Evaluating batch 95/124
Evaluating batch 96/124
Evaluating batch 97/124
Evaluating batch 98/124
Evaluating batch 99/124
Evaluating batch 100/124
Evaluating batch 101/124
Evaluating batch 102/124
Evaluating batch 103/124
Evaluating batch 104/124
Evaluating batch 105/124
Evaluating batch 106/124
Evaluating batch 107/124
Evaluating batch 108/124
Evaluating batch 109/124
Evaluating batch 110/124
Evaluating batch 111/124
Evaluating batch 112/124
Evaluating batch 113/124
Evaluating batch 114/124
Evaluating batch 115/124
Evaluating batch 116/124
Evaluating batch 117/124
Evaluating batch 118/124
Evaluating batch 119/124
Evaluating batch 120/124
Evaluating batch 121/124
Evaluating batch 122/124
Evaluating batch 123/124
Evaluating batch 124/124
================================================================================
| End of training | test loss 6.820382118225098 | test bpc 9.8591890335083 
================================================================================
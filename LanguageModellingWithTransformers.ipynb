{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Author: Roger Bukuru**\n",
        "\n",
        "## How to guide\n",
        "- Start by placing the isiXhosa data which include the train, validation and test sets within the same directory as this notebook.\n",
        "- Sequentially run each code block, the code blocks perform the following tasks:\n",
        "   - Installs any missing packages\n",
        "   - Setups the Transformer and Language Model Architectures\n",
        "   - Setups the training step function, loss, and validation functions\n",
        "   - Hyperparameter Tuning and Training (A GPU is required for this)\n",
        "      - Beware that this takes a long time to run\n",
        "   - Using the best parameters obtained you should manually update the hyperparameter values within the\n",
        "      - \"Performing Extensions using Best Hyperparameters\"\n",
        "      - Using these optimal parameters you can perform further intermediate and advanced extension testing by change the boolean values of these fields:\n",
        "        -  pre_norm = False\n",
        "        - tie_weights = False\n",
        "        - useAdamWOptimization = False\n",
        "        - useLearningRateSchedule = False\n",
        "      By default an optimizer with the above settings is executed, this is our baseline optimizer.\n",
        "\n"
      ],
      "metadata": {
        "id": "mFuXuT0juTNG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M95I8Uv2P4iX",
        "outputId": "800beb7d-7f2a-4ca7-df30-07120cdacccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.13.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.8.4)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax) (1.26.4)\n",
            "Requirement already satisfied: jax>=0.4.19 in /usr/local/lib/python3.10/dist-packages (from flax) (0.4.26)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.6.0)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.64)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (1.13.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.16.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.86)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (3.20.3)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (4.10.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.4.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.20.0)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.26)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax) (1.13.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax) (0.1.86)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from optax) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax) (4.12.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (1.13.1)\n",
            "a GPU is connected.\n",
            "gpu\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install flax\n",
        "!pip install jax\n",
        "!pip install optax\n",
        "\n",
        "import os\n",
        "import math\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import functools\n",
        "import optax\n",
        "import itertools\n",
        "import random\n",
        "import time\n",
        "import wandb\n",
        "import optax\n",
        "\n",
        "\n",
        "from jax import grad, jit, vmap\n",
        "\n",
        "\n",
        "if os.environ[\"COLAB_GPU\"] and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"a GPU is connected.\")\n",
        "else:\n",
        "    print(\"Only CPU accelerator is connected.\")\n",
        "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
        "\n",
        "from jax.lib import xla_bridge\n",
        "print(xla_bridge.get_backend().platform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "fEaPqDDJoF9P"
      },
      "outputs": [],
      "source": [
        "# MLOps Tool for experiment tracking\n",
        "#wandb.require(\"core\")\n",
        "#wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer and Language Model Setup**"
      ],
      "metadata": {
        "id": "W4CPNVlKmm3C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "LlSNh_Q8YzHC"
      },
      "outputs": [],
      "source": [
        "class SequenceToQKV(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, X):\n",
        "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "\n",
        "\n",
        "    q_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    k_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    v_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "\n",
        "    Q = q_layer(X)\n",
        "    K = k_layer(X)\n",
        "    V = v_layer(X)\n",
        "\n",
        "    return Q, K, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "DGnfegRIV35G"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  d_m: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "    self.d_k  = self.d_m // self.num_heads\n",
        "\n",
        "    self.W_q = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "    self.W_k = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "    self.W_v = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "    self.Wo  = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "    self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
        "\n",
        "  def scaled_dot_product_attention(self,query, key, value, mask=None):\n",
        "    d_k = key.shape[-1]\n",
        "    T_k = key.shape[-2]\n",
        "    T_q = query.shape[-2]\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits/jnp.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "    attention = jnp.matmul(attention_weights, value)\n",
        "    return attention, attention_weights\n",
        "\n",
        "  def __call__(self, X, mask=None):\n",
        "\n",
        "    # get the batch size, sequence length and embedding size\n",
        "    Q, K, V   = self.sequence_to_qkv(X)\n",
        "    B, T, d_m = K.shape\n",
        "    assert d_m == self.d_m\n",
        "\n",
        "\n",
        "    q_heads = Q.reshape(B, -1, self.num_heads, self.d_k).swapaxes(1,2)\n",
        "    k_heads = K.reshape(B, -1, self.num_heads, self.d_k).swapaxes(1,2)\n",
        "    v_heads = V.reshape(B, -1, self.num_heads, self.d_k).swapaxes(1,2)\n",
        "\n",
        "    # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, d_m) - re-assemble all head outputs\n",
        "    attention, attention_weights = self.scaled_dot_product_attention(\n",
        "        q_heads, k_heads, v_heads, mask\n",
        "        )\n",
        "\n",
        "    attention = attention.swapaxes(1,2).reshape(B, -1, d_m)\n",
        "    X_new = self.Wo(attention)\n",
        "    return X_new, attention_weights\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "UJYeRDkihwTe"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    d_m: int\n",
        "    max_seq_length: int\n",
        "\n",
        "    def setup(self):\n",
        "        pe = jnp.zeros((self.max_seq_length, d_m))\n",
        "        positions = jnp.arange(0, self.max_seq_length)[: jnp.newaxis]\n",
        "\n",
        "        i = jnp.arange(0, d_m, 2)\n",
        "        div_term = jnp.exp(i * (-math.log(10000.0) / d_m))\n",
        "        frequencies = positions * div_term\n",
        "\n",
        "        pe = pe.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "        pe = pe.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "        self.pe  = pe\n",
        "\n",
        "    def __call__(self, x):\n",
        "      return x + self.pe[:x.shape[0], :]\n",
        "\n",
        "def positionalEncoding(d_m,sequence_length):\n",
        "      assert d_m % 2 == 0, \"token_embedding should be divisible by two\"\n",
        "\n",
        "      pe = jnp.zeros((sequence_length, d_m))\n",
        "      positions = jnp.arange(0, sequence_length)[:, jnp.newaxis]\n",
        "\n",
        "      i = jnp.arange(0, d_m, 2)\n",
        "      div_term = jnp.exp(i * (-math.log(10000.0) / d_m))\n",
        "      frequencies = positions * div_term\n",
        "\n",
        "      pe = pe.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "      pe = pe.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "      return pe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "pHHDQuCTlsI9"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\"\n",
        "  A 2-layer MLP which widens then narrows the input.\n",
        "\n",
        "  Args:\n",
        "    widening_factor [optional, default=4]: The size of the hidden layer will be d_model * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  widening_factor: int = 4\n",
        "  init_scale: float = 0.25\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      x: [B, T, d_m]\n",
        "    '''\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self.widening_factor * d_m\n",
        "\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "    )\n",
        "    layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
        "    layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))\n",
        "    x = layer2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "8lw0xw1nneNX"
      },
      "outputs": [],
      "source": [
        "class AddNorm(nn.Module):\n",
        "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, processed_x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: Sequence of tokens before feeding into MHA or FF blocks, with shape [B, T, d_m]\n",
        "      x: Sequence of after being processed by MHA or FF blocks, with shape [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      add_norm_x: Transformed tokens with shape [B, T, d_m]\n",
        "    '''\n",
        "\n",
        "    added = x + processed_x\n",
        "    normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
        "    return normalised(added)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "BiYFxJUFnm4Y"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  \"\"\" The transformer decoder blocker that combines, the MulitHead-Attention Layer, AddNorm and FeedForward Layers\"\"\"\n",
        "\n",
        "  d_m: int\n",
        "  num_heads: int\n",
        "  ff_widening_factor: int\n",
        "  dropout_rate: float\n",
        "  training: bool\n",
        "  pre_norm: bool\n",
        "\n",
        "  def setup(self):\n",
        "    self.mha = MultiHeadAttention(d_m = self.d_m, num_heads = self.num_heads)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.ff = FeedForward(widening_factor=self.ff_widening_factor)\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.dropout = nn.Dropout(rate=self.dropout_rate)\n",
        "\n",
        "  def __call__(self, X, mask=None, key=None):\n",
        "\n",
        "      x_output = None\n",
        "      attention_weights = None\n",
        "\n",
        "      if not self.pre_norm:\n",
        "        attention_output, attention_weights = self.mha(X=X,mask=mask)\n",
        "        add_norm1_output = self.add_norm1(X, self.dropout(attention_output, deterministic=not self.training, rng=key))\n",
        "        ff_output = self.ff(add_norm1_output)\n",
        "        add_norm2_output = self.add_norm2(add_norm1_output, self.dropout(ff_output, deterministic=not self.training, rng=key))\n",
        "        x_output = add_norm2_output\n",
        "      else:\n",
        "        X = self.add_norm1(X,X)\n",
        "        attention_output, attention_weights = self.mha(X=X,mask=mask)\n",
        "        add_norm2_output = self.add_norm2(X, self.dropout(attention_output, deterministic=not self.training, rng=key))\n",
        "        ff_output = self.ff(add_norm2_output)\n",
        "        x_output = self.dropout(ff_output, deterministic=not self.training, rng=key)\n",
        "\n",
        "      return x_output, attention_weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "dtlY9erPq5v9"
      },
      "outputs": [],
      "source": [
        "class LM(nn.Module):\n",
        "    \"\"\"The Language model that combines the one or more transformer block(s) with the output embedding, for language modelling\"\"\"\n",
        "    num_heads:int\n",
        "    num_layers:int\n",
        "    d_m:int\n",
        "    ff_widening_factor:int\n",
        "    vocab_size: int\n",
        "    dropout_rate: float\n",
        "    training: bool\n",
        "    pre_norm: bool\n",
        "    tie_weights: bool\n",
        "\n",
        "    def setup(self):\n",
        "        self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m) # convert tokens to embeddings\n",
        "        self.decoder_blocks = [DecoderBlock(self.d_m, self.num_heads, self.ff_widening_factor, self.dropout_rate, self.training, self.pre_norm) for _ in range(self.num_layers)]\n",
        "        self.pred_layer = nn.Dense(self.vocab_size)\n",
        "        self.dropout = nn.Dropout(rate=self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, mask=None, key=None):\n",
        "        x = self.embedding(x)\n",
        "        sequence_len = x.shape[-2]\n",
        "        positions = positionalEncoding(self.d_m,sequence_len)\n",
        "        x = x + positions\n",
        "        attention_weights = []\n",
        "        for decoder_block in self.decoder_blocks:\n",
        "            key, subkey = jax.random.split(key)\n",
        "            out, attention_weight = decoder_block(x, mask, key=subkey)\n",
        "            out = self.dropout(out, deterministic=not self.training, rng=key)\n",
        "            x = out\n",
        "            attention_weights.append(attention_weight)\n",
        "\n",
        "        logits = None\n",
        "        if self.tie_weights:\n",
        "            logits = nn.log_softmax(x @ self.embedding.embedding.T)\n",
        "        else:\n",
        "          logits = nn.log_softmax(self.pred_layer(x))\n",
        "\n",
        "        return logits, attention_weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGgxk4Ikpre4"
      },
      "source": [
        "# **Training Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Iy6Ea5iQFAFg"
      },
      "outputs": [],
      "source": [
        "class CharacterBasedAsciiDatasetForLLM:\n",
        "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Tokenize by splitting the text into characters\n",
        "        characters = list(corpus)\n",
        "        self.vocab_size = len(set(characters))  # Number of unique words\n",
        "\n",
        "        # Create a mapping from characters to unique IDs\n",
        "        self.character_to_id = {character: i for i, character in enumerate(set(characters))}\n",
        "\n",
        "        # Store the inverse mapping from IDs to characters\n",
        "        self.id_to_character = {i: character for character, i in self.character_to_id.items()}\n",
        "\n",
        "        # Convert the characters in the corpus to their corresponding IDs\n",
        "        corpus = np.array([self.character_to_id[character] for character in characters]).astype(np.int32)\n",
        "\n",
        "        crop_len = sequence_length + 1\n",
        "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "        if ragged:\n",
        "            corpus = corpus[:-ragged]\n",
        "        corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "        if num_batches < 10:\n",
        "            raise ValueError(\n",
        "                f\"Only {num_batches} batches; consider a shorter \"\n",
        "                \"sequence or a smaller batch.\"\n",
        "            )\n",
        "\n",
        "        self.num_batches = num_batches  # Store the number of batches\n",
        "        self._ds = CharacterBasedAsciiDatasetForLLM._infinite_shuffle(\n",
        "            corpus, batch_size * 10\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Yield next mini-batch.\"\"\"\n",
        "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "        batch = np.stack(batch)\n",
        "        # Create the language modeling observation/target pairs.\n",
        "        return dict(\n",
        "            input=batch[:, :-1], target=batch[:, 1:]\n",
        "        )\n",
        "\n",
        "    def ids_to_characters(self, ids):\n",
        "        \"\"\"Convert a sequence of character IDs to characters.\"\"\"\n",
        "        return [self.id_to_character[id] for id in ids]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    @staticmethod\n",
        "    def _infinite_shuffle(iterable, buffer_size):\n",
        "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
        "        ds = itertools.cycle(iterable)\n",
        "        buf = [next(ds) for _ in range(buffer_size)]\n",
        "        random.shuffle(buf)\n",
        "        while True:\n",
        "            item = next(ds)\n",
        "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
        "            result, buf[idx] = buf[idx], item\n",
        "            yield result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Cv9yYXRDpy9k"
      },
      "outputs": [],
      "source": [
        "def sequence_loss_fn(logits, targets):\n",
        "  \"\"\"Compute the loss on data wrt params.\"\"\"\n",
        "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "  assert logits.shape == target_labels.shape\n",
        "  mask = jnp.greater(targets, 0)\n",
        "  loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "  sequence_loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
        "\n",
        "  return sequence_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "pEZEqMqgQny0"
      },
      "outputs": [],
      "source": [
        "def compute_bpc(logits, batch):\n",
        "  # Compute BPC (bits-per-character)\n",
        "  log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
        "  target_log_probs = jnp.take_along_axis(log_probs, batch['target'][..., None], axis=-1).squeeze(-1)\n",
        "  bpc = -jnp.mean(target_log_probs) / jnp.log(2)\n",
        "  return bpc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "0s9jpE06qNg1"
      },
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnums=(4, 5))\n",
        "def train_step(params, optimizer_state, batch, key, apply_fn, update_fn):\n",
        "  def loss_fn(params):\n",
        "    T = batch['input'].shape[1]\n",
        "    logits,_ = apply_fn(params, batch['input'], jnp.tril(np.ones((T, T))), key)\n",
        "    loss   = sequence_loss_fn(logits, batch['target'])\n",
        "    return loss, logits\n",
        "\n",
        "  (loss,logits), gradients = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
        "  bpc = compute_bpc(logits, batch)\n",
        "  updates, optimizer_state = update_fn(gradients, optimizer_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, optimizer_state, loss, bpc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "go7U4M_FfWSv"
      },
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnums=(2, ))\n",
        "def generate_prediction(params, input, apply_fn):\n",
        "  logits = apply_fn(params, input)\n",
        "  argmax_out = jnp.argmax(logits, axis=-1)\n",
        "  return argmax_out[0][-1].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "CF_gzkBcQrO3"
      },
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnums=(4))\n",
        "def validation_step(params, batch, mask, key, apply_fn):\n",
        "    \"\"\"\n",
        "    Perform a validation step, computing both the loss and bits-per-character (BPC).\n",
        "\n",
        "    Args:\n",
        "        params: Model parameters.\n",
        "        batch: Validation batch containing 'input' and 'target'.\n",
        "        mask: Mask for the sequence.\n",
        "        key: JAX PRNG key.\n",
        "        model_apply: The model's apply function.\n",
        "\n",
        "    Returns:\n",
        "        loss: The computed validation loss.\n",
        "        bpc: The computed bits-per-character for the given batch.\n",
        "    \"\"\"\n",
        "    # Compute logits from the model\n",
        "    logits,_ = apply_fn(params, batch['input'], mask, key)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = sequence_loss_fn(logits, batch['target'])\n",
        "\n",
        "    # Compute bpc\n",
        "    bpc = bpc = compute_bpc(logits, batch)\n",
        "\n",
        "    return loss, bpc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo6qHwL1ydUF"
      },
      "source": [
        "# **Hyperparameter-Tuning Training and Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDdU7xDE5FSw"
      },
      "outputs": [],
      "source": [
        "# start a new wandb run to track this script\n",
        "#wandb.init(\n",
        "#    # set the wandb project where this run will be logged\n",
        "#    project=\"Language Modelling with Transformers\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "#    config={\n",
        "#    \"batch_size\": 0.02,\n",
        "#    \"seq_length\": 128,\n",
        "#    \"d_m\": 64,\n",
        "#    \"num_heads\": 4,\n",
        "#    \"num_layers_list\": 1,\n",
        "#   \"ff_widening_factor\": 4,\n",
        "#    \"LR\": 2e-3,\n",
        "#    \"dropout_rate\": 0.1\n",
        "#    }\n",
        "#)\n",
        "\n",
        "\n",
        "# Define search ranges for hyperparameters\n",
        "batch_sizes = [32, 64]\n",
        "seq_lengths = [64, 128]\n",
        "d_ms = [128, 256]\n",
        "num_heads_list = [4, 8]\n",
        "num_layers_list = [2, 4]\n",
        "ff_widening_factors = [4, 8]\n",
        "LRs = [1e-3, 2e-3]\n",
        "dropout_rates = [0.1, 0.2]\n",
        "\n",
        "best_val_bpc = np.inf\n",
        "best_hyperparameters = {}\n",
        "best_params = None\n",
        "MAX_STEPS = 50 # Do 50 epochs\n",
        "\n",
        "# Loop over all combinations of hyperparameters\n",
        "for bs in batch_sizes:\n",
        "    for seq_len in seq_lengths:\n",
        "        for d_m in d_ms:\n",
        "            for nh in num_heads_list:\n",
        "                for nl in num_layers_list:\n",
        "                    for ff_wf in ff_widening_factors:\n",
        "                        for lr in LRs:\n",
        "                            for dr in dropout_rates:\n",
        "                                print(f\"Training with BS={bs}, Seq_Len={seq_len}, DM={d_m}, NH={nh}, NL={nl}, FF_WF={ff_wf}, LR={lr}, DR={dr}\")\n",
        "\n",
        "                                # Update dataset with new sequence length\n",
        "                                train_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.train\", bs, seq_len)\n",
        "                                vocab_size = train_dataset.vocab_size\n",
        "\n",
        "                                # Update LM parameters\n",
        "                                lm = LM(\n",
        "                                    num_heads=nh,\n",
        "                                    num_layers=nl,\n",
        "                                    d_m=d_m,\n",
        "                                    vocab_size=vocab_size,\n",
        "                                    ff_widening_factor=ff_wf,\n",
        "                                    dropout_rate=dr,\n",
        "                                    training=True,\n",
        "                                    pre_norm=False,\n",
        "                                    tie_weights=False\n",
        "                                )\n",
        "\n",
        "\n",
        "                                # Update validation dataset with new sequence length\n",
        "                                val_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.valid\", bs, seq_len)\n",
        "\n",
        "                                # Reinitialize optimizer with new learning rate\n",
        "                                optimizer = optax.adam(lr, b1=0.9, b2=0.99)\n",
        "\n",
        "                                # Initialize parameters\n",
        "                                key = jax.random.PRNGKey(90)\n",
        "                                key, init_key = jax.random.split(key)\n",
        "                                batch = next(train_dataset)  # Get a batch to initialize parameters\n",
        "                                mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
        "                                params = lm.init(key, batch['input'], mask, init_key)\n",
        "                                optimizer_state = optimizer.init(params)\n",
        "\n",
        "                                # Training Loop\n",
        "                                for epoch in range(1, MAX_STEPS + 1):\n",
        "                                    print(\"Start Training\")\n",
        "                                    start_time = time.time()\n",
        "                                    train_losses = []\n",
        "                                    train_bpcs = []\n",
        "\n",
        "                                    for batch_idx in range(len(train_dataset)):\n",
        "                                        key, train_key = jax.random.split(key)\n",
        "                                        batch = next(train_dataset)\n",
        "                                        params, optimizer_state, train_loss, train_bpc = train_step(\n",
        "                                            params, optimizer_state, batch, train_key, lm.apply, optimizer.update)\n",
        "                                        train_losses.append(train_loss)\n",
        "                                        train_bpcs.append(train_bpc)\n",
        "\n",
        "                                        # Log intermediate training progress every 200 batches\n",
        "                                        if (batch_idx + 1) % 200 == 0:\n",
        "                                            avg_train_loss = jnp.mean(jnp.array(train_losses[-200:]))\n",
        "                                            avg_train_bpc = jnp.mean(jnp.array(train_bpcs[-200:]))\n",
        "                                            elapsed_time = (time.time() - start_time) / (batch_idx + 1) * 1000\n",
        "                                            print(f\"| epoch {epoch} | {batch_idx + 1}/{len(train_dataset)} batches | \"\n",
        "                                                  f\"lr {lr:.4f} | ms/batch {elapsed_time:.2f} | loss {avg_train_loss:.4f} | \"\n",
        "                                                  f\"bpc {avg_train_bpc:.4f}\")\n",
        "\n",
        "                                    # Average training loss and BPC for the epoch\n",
        "                                    avg_train_loss = jnp.mean(jnp.array(train_losses))\n",
        "                                    avg_train_bpc = jnp.mean(jnp.array(train_bpcs))\n",
        "\n",
        "                                    # Validation Phase\n",
        "                                    val_losses = []\n",
        "                                    val_bpcs = []\n",
        "                                    for batch_idx in range(len(val_batch)):\n",
        "                                      val_batch = next(val_dataset)\n",
        "                                      mask = jnp.tril(jnp.ones((val_batch['input'].shape[1], val_batch['input'].shape[1])))\n",
        "                                      key, val_key = jax.random.split(key)\n",
        "                                      val_loss, val_bpc = validation_step(params, val_batch, mask, val_key, lm.apply)\n",
        "                                      val_losses.append(val_loss)\n",
        "                                      val_bpcs.append(val_bpc)\n",
        "\n",
        "                                    # Average validation loss and BPC for the epoch\n",
        "                                    avg_val_loss = jnp.mean(jnp.array(val_losses))\n",
        "                                    avg_val_bpc = jnp.mean(jnp.array(val_bpcs))\n",
        "\n",
        "                                    # Calculate epoch time\n",
        "                                    epoch_time = time.time() - start_time\n",
        "\n",
        "                                    # Log end of epoch results\n",
        "                                    print(\"--------------------------------------------------------------------------------\")\n",
        "                                    print(f\"| end of epoch {epoch} | time: {epoch_time:.2f}s | train loss {avg_train_loss:.4f} | train bpc {avg_train_bpc:.4f} | \"\n",
        "                                          f\"valid loss {avg_val_loss:.4f} | valid bpc {avg_val_bpc:.4f} \")\n",
        "                                    print(\"--------------------------------------------------------------------------------\")\n",
        "\n",
        "                                    # Store best hyperparameters\n",
        "                                    if val_bpc < best_val_bpc:\n",
        "                                        best_val_bpc = val_bpc\n",
        "                                        best_hyperparameters = {\n",
        "                                            \"batch_size\": bs,\n",
        "                                            \"seq_length\": seq_len,\n",
        "                                            \"d_m\": d_m,\n",
        "                                            \"num_heads\": nh,\n",
        "                                            \"num_layers\": nl,\n",
        "                                            \"ff_widening_factor\": ff_wf,\n",
        "                                            \"LR\": lr,\n",
        "                                            \"dropout_rate\": dr\n",
        "                                        }\n",
        "                                        best_params = params  # Store the best model parameters\n",
        "\n",
        "# Reinitialize test dataset with the best batch size and sequence length\n",
        "test_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.test\", best_hyperparameters[\"batch_size\"], best_hyperparameters[\"seq_length\"])\n",
        "vocab_size = test_dataset.vocab_size\n",
        "\n",
        "# After training, evaluate on the entire test set using the best hyperparameters and parameters\n",
        "# Reinitialize LM with the best hyperparameters\n",
        "lm = LM(\n",
        "    num_heads=best_hyperparameters[\"num_heads\"],\n",
        "    num_layers=best_hyperparameters[\"num_layers\"],\n",
        "    d_m=best_hyperparameters[\"d_m\"],\n",
        "    vocab_size=vocab_size,\n",
        "    ff_widening_factor=best_hyperparameters[\"ff_widening_factor\"],\n",
        "    dropout_rate=best_hyperparameters[\"dropout_rate\"],\n",
        "    training=False,\n",
        "    pre_norm=False,\n",
        "    tie_weights=False\n",
        ")\n",
        "\n",
        "# Evaluate on all batches in the test dataset\n",
        "test_losses = []\n",
        "test_bpcs = []\n",
        "for test_batch in test_dataset:\n",
        "    mask = jnp.tril(jnp.ones((test_batch['input'].shape[1], test_batch['input'].shape[1])))\n",
        "    key, test_key = jax.random.split(key)\n",
        "    test_loss, test_bpc = validation_step(best_params, test_batch, mask, test_key, lm.apply)\n",
        "    test_losses.append(test_loss)\n",
        "    test_bpcs.append(test_bpc)\n",
        "\n",
        "# Compute average test loss and BPC across all batches\n",
        "avg_test_loss = jnp.mean(jnp.array(test_losses))\n",
        "avg_test_bpc = jnp.mean(jnp.array(test_bpcs))\n",
        "\n",
        "# Final Test BPC and Loss log\n",
        "print(\"================================================================================\")\n",
        "print(f\"| End of training | test loss {avg_test_loss:.2f} | test bpc {avg_test_bpc:.2f} \")\n",
        "print(\"================================================================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vo_fAoEy5c0"
      },
      "source": [
        "# **Performing Extensions using Best Hyperparameters**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "K_zOlfhoGxd-"
      },
      "outputs": [],
      "source": [
        "# We use the optimal hyperparameters obtained from the code above.\n",
        "\n",
        "batch_size = 32\n",
        "seq_length = 128\n",
        "d_m        = 256\n",
        "num_heads  = 4\n",
        "num_layers = 2\n",
        "ff_widening_factor = 4\n",
        "LR = 1e-3\n",
        "dropout_rate=0.2\n",
        "training=True\n",
        "MAX_STEPS = 50 # number of epocs\n",
        "\n",
        "# These ar manually adjusted as we test the different extensions\n",
        "pre_norm = False\n",
        "tie_weights = False\n",
        "useAdamWOptimization = False\n",
        "useLearningRateSchedule = False\n",
        "WARMUP_STEPS = 10\n",
        "optimizer = None\n",
        "\n",
        "# set up the data\n",
        "train_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.train\", batch_size, seq_length)\n",
        "val_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.valid\", batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size\n",
        "batch = next(train_dataset)\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "\n",
        "# initialise model\n",
        "\n",
        "lm = LM(num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            d_m=d_m,\n",
        "            vocab_size=vocab_size,\n",
        "            ff_widening_factor=ff_widening_factor,\n",
        "            dropout_rate = dropout_rate,\n",
        "            training=training,\n",
        "            pre_norm = pre_norm,\n",
        "            tie_weights=tie_weights\n",
        "      )\n",
        "mask   = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
        "key    = jax.random.PRNGKey(90)\n",
        "params = lm.init(key, batch['input'], mask,key)\n",
        "\n",
        "# set up the learning rate schedule function\n",
        "learning_rate_fn = optax.warmup_cosine_decay_schedule(\n",
        "    init_value = 0.0,\n",
        "    peak_value = LR,\n",
        "    warmup_steps = WARMUP_STEPS,\n",
        "    decay_steps = MAX_STEPS - WARMUP_STEPS,\n",
        "    end_value = 0.0\n",
        ")\n",
        "# set up the optimizer based on the experiment\n",
        "if useAdamWOptimization:\n",
        "    if useLearningRateSchedule:\n",
        "      optimizer = optax.adamw(\n",
        "          learning_rate=learning_rate_fn,\n",
        "          b1=0.9,\n",
        "          b2=0.99,\n",
        "          weight_decay=1e-4) # Weight decay for regularization\n",
        "    else:\n",
        "      optimizer = optax.adamw(\n",
        "          LR,\n",
        "          b1=0.9,\n",
        "          b2=0.99,\n",
        "          weight_decay=1e-4, # Weight decay for regularization\n",
        "          )\n",
        "else:\n",
        "  if useLearningRateSchedule:\n",
        "    optimizer = optax.adam(\n",
        "        learning_rate=learning_rate_fn,\n",
        "        b1=0.9,\n",
        "        b2=0.99)\n",
        "  else:\n",
        "    optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
        "optimizer_state = optimizer.init(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwFMdzY9O1nx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# start a new wandb run to track this script\n",
        "#wandb.init(\n",
        "#    # set the wandb project where this run will be logged\n",
        "#    project=\"Language Modelling with Transformers\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "#    config={\n",
        "#    \"batch_size\": 0.02,\n",
        "#    \"seq_length\": 128,\n",
        "#    \"d_m\": 64,\n",
        "#    \"num_heads\": 4,\n",
        "#    \"num_layers_list\": 1,\n",
        "#    \"ff_widening_factor\": 4,\n",
        "#    \"LR\": 2e-3,\n",
        "#    \"dropout_rate\": 0.1\n",
        "#    }\n",
        "#)\n",
        "\n",
        "best_val_bpc = np.inf\n",
        "best_params = None\n",
        "VOCAB_SIZE = vocab_size\n",
        "\n",
        "#checkpoint_dir = \"checkpoints\"\n",
        "#if not os.path.exists(checkpoint_dir):\n",
        "#    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, MAX_STEPS + 1):\n",
        "    print(\"Start Training\")\n",
        "    start_time = time.time()\n",
        "    train_losses = []\n",
        "    train_bpcs = []\n",
        "\n",
        "    for batch_idx in range(len(train_dataset)):\n",
        "        key, train_key = jax.random.split(key)\n",
        "        batch = next(train_dataset)\n",
        "        params, optimizer_state, train_loss, train_bpc = train_step(\n",
        "            params, optimizer_state, batch, train_key, lm.apply, optimizer.update)\n",
        "        train_losses.append(train_loss)\n",
        "        train_bpcs.append(train_bpc)\n",
        "\n",
        "        # Log intermediate training progress every 200 batches\n",
        "        if (batch_idx + 1) % 200 == 0:\n",
        "            avg_train_loss = jnp.mean(jnp.array(train_losses[-200:]))\n",
        "            avg_train_bpc = jnp.mean(jnp.array(train_bpcs[-200:]))\n",
        "            elapsed_time = (time.time() - start_time) / (batch_idx + 1) * 1000\n",
        "            print(f\"| epoch {epoch} | {batch_idx + 1}/{len(train_dataset)} batches | \"\n",
        "                  f\"lr {LR:.4f} | ms/batch {elapsed_time:.2f} | loss {avg_train_loss:.4f} | \"\n",
        "                  f\"bpc {avg_train_bpc:.4f}\")\n",
        "\n",
        "    # Average training loss and BPC for the epoch\n",
        "    avg_train_loss = jnp.mean(jnp.array(train_losses))\n",
        "    avg_train_bpc = jnp.mean(jnp.array(train_bpcs))\n",
        "\n",
        "    # Validation Phase\n",
        "    val_losses = []\n",
        "    val_bpcs = []\n",
        "    for batch_idx in range(len(val_dataset)):\n",
        "      val_batch = next(val_dataset)\n",
        "      mask = jnp.tril(jnp.ones((val_batch['input'].shape[1], val_batch['input'].shape[1])))\n",
        "      key, val_key = jax.random.split(key)\n",
        "      val_loss, val_bpc = validation_step(params, val_batch, mask, val_key, lm.apply)\n",
        "      val_losses.append(val_loss)\n",
        "      val_bpcs.append(val_bpc)\n",
        "\n",
        "    # Average validation loss and BPC for the epoch\n",
        "    avg_val_loss = jnp.mean(jnp.array(val_losses))\n",
        "    avg_val_bpc = jnp.mean(jnp.array(val_bpcs))\n",
        "\n",
        "    # Calculate epoch time\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # Log end of epoch results\n",
        "    print(\"--------------------------------------------------------------------------------\")\n",
        "    print(f\"| end of epoch {epoch} | time: {epoch_time:.2f}s | train loss {avg_train_loss:.4f} | train bpc {avg_train_bpc:.4f} | \"\n",
        "          f\"valid loss {avg_val_loss:.4f} | valid bpc {avg_val_bpc:.4f} \")\n",
        "    print(\"--------------------------------------------------------------------------------\")\n",
        "    #wandb.log({\n",
        "     #          \"train_loss\": avg_train_loss,\n",
        "     #          \"train_bpc\": avg_train_bpc,\n",
        "     #         \"val_loss\": avg_val_loss,\n",
        "     #          \"val_bpc\": avg_val_bpc,\n",
        "     #          \"epoch\": epoch,\n",
        "     #          \"lr\": LR,\n",
        "     #          })\n",
        "\n",
        "    # Store best hyperparameters\n",
        "    if val_bpc < best_val_bpc:\n",
        "        best_val_bpc = val_bpc\n",
        "        best_params = params  # Store the best model parameters\n",
        "#wandb.finish()\n",
        "\n",
        "lm = LM(num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            d_m=d_m,\n",
        "            vocab_size=vocab_size,\n",
        "            ff_widening_factor=ff_widening_factor,\n",
        "            dropout_rate = dropout_rate,\n",
        "            training=False,\n",
        "            pre_norm = pre_norm,\n",
        "            tie_weights=tie_weights\n",
        "      )\n",
        "print(f\"Start Test Validation\")\n",
        "# Reinitialize test dataset with the best batch size and sequence length\n",
        "test_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.test\", batch_size, seq_length)\n",
        "\n",
        "# Evaluate on all batches in the test dataset\n",
        "test_losses = []\n",
        "test_bpcs = []\n",
        "for batch_idx in range(len(test_dataset)):\n",
        "    print(f\"Evaluating batch {batch_idx + 1}/{len(test_dataset)}\")\n",
        "    test_batch = next(test_dataset)\n",
        "    mask = jnp.tril(jnp.ones((test_batch['input'].shape[1], test_batch['input'].shape[1])))\n",
        "    key, test_key = jax.random.split(key)\n",
        "    test_loss, test_bpc = validation_step(best_params, test_batch, mask, test_key, lm.apply)\n",
        "    test_losses.append(test_loss)\n",
        "    test_bpcs.append(test_bpc)\n",
        "\n",
        "# Compute average test loss and BPC across all batches\n",
        "avg_test_loss = jnp.mean(jnp.array(test_losses))\n",
        "avg_test_bpc = jnp.mean(jnp.array(test_bpcs))\n",
        "\n",
        "# Final Test BPC and Loss log\n",
        "print(\"================================================================================\")\n",
        "print(f\"| End of training | test loss {avg_test_loss} | test bpc {avg_test_bpc} \")\n",
        "print(\"================================================================================\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
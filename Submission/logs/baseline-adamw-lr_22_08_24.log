Tracking run with wandb version 0.17.7
Run data is saved locally in /content/wandb/run-20240822_223105-xe0vn41d
Syncing run efficient-firebrand-23 to Weights & Biases (docs)
View project at https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers
View run at https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/xe0vn41d
Start Training
| epoch 1 | 200/2222 batches | lr 0.0010 | ms/batch 30.03 | loss 3.3438 | bpc 4.8434
| epoch 1 | 400/2222 batches | lr 0.0010 | ms/batch 22.98 | loss 3.3127 | bpc 4.8126
| epoch 1 | 600/2222 batches | lr 0.0010 | ms/batch 20.69 | loss 3.2859 | bpc 4.7704
| epoch 1 | 800/2222 batches | lr 0.0010 | ms/batch 19.52 | loss 3.2704 | bpc 4.7425
| epoch 1 | 1000/2222 batches | lr 0.0010 | ms/batch 18.83 | loss 3.2594 | bpc 4.7260
| epoch 1 | 1200/2222 batches | lr 0.0010 | ms/batch 18.38 | loss 3.2722 | bpc 4.7432
| epoch 1 | 1400/2222 batches | lr 0.0010 | ms/batch 18.08 | loss 3.2563 | bpc 4.7178
| epoch 1 | 1600/2222 batches | lr 0.0010 | ms/batch 17.85 | loss 3.3256 | bpc 4.8117
| epoch 1 | 1800/2222 batches | lr 0.0010 | ms/batch 17.68 | loss 3.2617 | bpc 4.7220
| epoch 1 | 2000/2222 batches | lr 0.0010 | ms/batch 17.53 | loss 3.2688 | bpc 4.7442
| epoch 1 | 2200/2222 batches | lr 0.0010 | ms/batch 17.44 | loss 3.2536 | bpc 4.7162
--------------------------------------------------------------------------------
| end of epoch 1 | time: 40.87s | train loss 3.2821 | train bpc 4.7582 | valid loss 7.9031 | valid bpc 11.4089 
--------------------------------------------------------------------------------
Start Training
| epoch 2 | 200/2222 batches | lr 0.0010 | ms/batch 16.37 | loss 3.2910 | bpc 4.7680
| epoch 2 | 400/2222 batches | lr 0.0010 | ms/batch 16.39 | loss 3.3131 | bpc 4.8132
| epoch 2 | 600/2222 batches | lr 0.0010 | ms/batch 16.45 | loss 3.2871 | bpc 4.7720
| epoch 2 | 800/2222 batches | lr 0.0010 | ms/batch 16.48 | loss 3.2686 | bpc 4.7400
| epoch 2 | 1000/2222 batches | lr 0.0010 | ms/batch 16.49 | loss 3.2595 | bpc 4.7261
| epoch 2 | 1200/2222 batches | lr 0.0010 | ms/batch 16.51 | loss 3.2722 | bpc 4.7430
| epoch 2 | 1400/2222 batches | lr 0.0010 | ms/batch 16.55 | loss 3.2568 | bpc 4.7185
| epoch 2 | 1600/2222 batches | lr 0.0010 | ms/batch 16.56 | loss 3.3263 | bpc 4.8128
| epoch 2 | 1800/2222 batches | lr 0.0010 | ms/batch 16.58 | loss 3.2627 | bpc 4.7233
| epoch 2 | 2000/2222 batches | lr 0.0010 | ms/batch 16.60 | loss 3.2660 | bpc 4.7401
| epoch 2 | 2200/2222 batches | lr 0.0010 | ms/batch 16.62 | loss 3.2553 | bpc 4.7188
--------------------------------------------------------------------------------
| end of epoch 2 | time: 38.46s | train loss 3.2774 | train bpc 4.7515 | valid loss 7.9036 | valid bpc 11.4093 
--------------------------------------------------------------------------------
Start Training
| epoch 3 | 200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 3.2901 | bpc 4.7671
| epoch 3 | 400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 3.3127 | bpc 4.8124
| epoch 3 | 600/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2869 | bpc 4.7719
| epoch 3 | 800/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2691 | bpc 4.7406
| epoch 3 | 1000/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2610 | bpc 4.7285
| epoch 3 | 1200/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2719 | bpc 4.7426
| epoch 3 | 1400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2563 | bpc 4.7180
| epoch 3 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3268 | bpc 4.8133
| epoch 3 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2624 | bpc 4.7230
| epoch 3 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2662 | bpc 4.7405
| epoch 3 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2538 | bpc 4.7165
--------------------------------------------------------------------------------
| end of epoch 3 | time: 39.33s | train loss 3.2773 | train bpc 4.7513 | valid loss 7.9053 | valid bpc 11.4117 
--------------------------------------------------------------------------------
Start Training
| epoch 4 | 200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2906 | bpc 4.7676
| epoch 4 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3118 | bpc 4.8112
| epoch 4 | 600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2880 | bpc 4.7733
| epoch 4 | 800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2695 | bpc 4.7414
| epoch 4 | 1000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2600 | bpc 4.7267
| epoch 4 | 1200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2714 | bpc 4.7421
| epoch 4 | 1400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2555 | bpc 4.7167
| epoch 4 | 1600/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3264 | bpc 4.8128
| epoch 4 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2628 | bpc 4.7238
| epoch 4 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2675 | bpc 4.7424
| epoch 4 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2529 | bpc 4.7154
--------------------------------------------------------------------------------
| end of epoch 4 | time: 39.34s | train loss 3.2773 | train bpc 4.7513 | valid loss 7.9022 | valid bpc 11.4073 
--------------------------------------------------------------------------------
Start Training
| epoch 5 | 200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2902 | bpc 4.7669
| epoch 5 | 400/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.3119 | bpc 4.8114
| epoch 5 | 600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2870 | bpc 4.7721
| epoch 5 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2686 | bpc 4.7400
| epoch 5 | 1000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2604 | bpc 4.7275
| epoch 5 | 1200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2717 | bpc 4.7423
| epoch 5 | 1400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2555 | bpc 4.7167
| epoch 5 | 1600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3271 | bpc 4.8135
| epoch 5 | 1800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2622 | bpc 4.7229
| epoch 5 | 2000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2681 | bpc 4.7432
| epoch 5 | 2200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2544 | bpc 4.7175
--------------------------------------------------------------------------------
| end of epoch 5 | time: 39.44s | train loss 3.2773 | train bpc 4.7514 | valid loss 7.9043 | valid bpc 11.4104 
--------------------------------------------------------------------------------
Start Training
| epoch 6 | 200/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 3.2905 | bpc 4.7676
| epoch 6 | 400/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3119 | bpc 4.8116
| epoch 6 | 600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2861 | bpc 4.7707
| epoch 6 | 800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2676 | bpc 4.7385
| epoch 6 | 1000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2608 | bpc 4.7280
| epoch 6 | 1200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2722 | bpc 4.7432
| epoch 6 | 1400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2565 | bpc 4.7182
| epoch 6 | 1600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3262 | bpc 4.8125
| epoch 6 | 1800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2626 | bpc 4.7232
| epoch 6 | 2000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2671 | bpc 4.7417
| epoch 6 | 2200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2560 | bpc 4.7197
--------------------------------------------------------------------------------
| end of epoch 6 | time: 39.50s | train loss 3.2774 | train bpc 4.7514 | valid loss 7.9026 | valid bpc 11.4079 
--------------------------------------------------------------------------------
Start Training
| epoch 7 | 200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2899 | bpc 4.7667
| epoch 7 | 400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3129 | bpc 4.8128
| epoch 7 | 600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2862 | bpc 4.7709
| epoch 7 | 800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2690 | bpc 4.7403
| epoch 7 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2601 | bpc 4.7269
| epoch 7 | 1200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2717 | bpc 4.7424
| epoch 7 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2562 | bpc 4.7177
| epoch 7 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3254 | bpc 4.8115
| epoch 7 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2628 | bpc 4.7237
| epoch 7 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2674 | bpc 4.7421
| epoch 7 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2536 | bpc 4.7164
--------------------------------------------------------------------------------
| end of epoch 7 | time: 39.56s | train loss 3.2772 | train bpc 4.7511 | valid loss 7.9053 | valid bpc 11.4117 
--------------------------------------------------------------------------------
Start Training
| epoch 8 | 200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2913 | bpc 4.7686
| epoch 8 | 400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3115 | bpc 4.8106
| epoch 8 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2874 | bpc 4.7727
| epoch 8 | 800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2697 | bpc 4.7416
| epoch 8 | 1000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2591 | bpc 4.7256
| epoch 8 | 1200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2721 | bpc 4.7431
| epoch 8 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2578 | bpc 4.7201
| epoch 8 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3259 | bpc 4.8121
| epoch 8 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2623 | bpc 4.7227
| epoch 8 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2670 | bpc 4.7417
| epoch 8 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2546 | bpc 4.7177
--------------------------------------------------------------------------------
| end of epoch 8 | time: 39.63s | train loss 3.2774 | train bpc 4.7515 | valid loss 7.9025 | valid bpc 11.4077 
--------------------------------------------------------------------------------
Start Training
| epoch 9 | 200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2904 | bpc 4.7673
| epoch 9 | 400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3120 | bpc 4.8117
| epoch 9 | 600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2868 | bpc 4.7716
| epoch 9 | 800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2698 | bpc 4.7417
| epoch 9 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2590 | bpc 4.7254
| epoch 9 | 1200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2734 | bpc 4.7448
| epoch 9 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2563 | bpc 4.7179
| epoch 9 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3261 | bpc 4.8124
| epoch 9 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2618 | bpc 4.7221
| epoch 9 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2671 | bpc 4.7416
| epoch 9 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2527 | bpc 4.7153
--------------------------------------------------------------------------------
| end of epoch 9 | time: 39.58s | train loss 3.2772 | train bpc 4.7511 | valid loss 7.9047 | valid bpc 11.4112 
--------------------------------------------------------------------------------
Start Training
| epoch 10 | 200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2907 | bpc 4.7678
| epoch 10 | 400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3116 | bpc 4.8111
| epoch 10 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2867 | bpc 4.7717
| epoch 10 | 800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2673 | bpc 4.7379
| epoch 10 | 1000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2593 | bpc 4.7261
| epoch 10 | 1200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2706 | bpc 4.7404
| epoch 10 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2560 | bpc 4.7179
| epoch 10 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3254 | bpc 4.8112
| epoch 10 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2630 | bpc 4.7239
| epoch 10 | 2000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2670 | bpc 4.7417
| epoch 10 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2532 | bpc 4.7158
--------------------------------------------------------------------------------
| end of epoch 10 | time: 39.52s | train loss 3.2768 | train bpc 4.7506 | valid loss 7.9051 | valid bpc 11.4114 
--------------------------------------------------------------------------------
Start Training
| epoch 11 | 200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2897 | bpc 4.7662
| epoch 11 | 400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3143 | bpc 4.8148
| epoch 11 | 600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2873 | bpc 4.7725
| epoch 11 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2699 | bpc 4.7413
| epoch 11 | 1000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2601 | bpc 4.7272
| epoch 11 | 1200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2722 | bpc 4.7431
| epoch 11 | 1400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2561 | bpc 4.7174
| epoch 11 | 1600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3258 | bpc 4.8121
| epoch 11 | 1800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2618 | bpc 4.7221
| epoch 11 | 2000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2662 | bpc 4.7405
| epoch 11 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2535 | bpc 4.7164
--------------------------------------------------------------------------------
| end of epoch 11 | time: 39.43s | train loss 3.2773 | train bpc 4.7513 | valid loss 7.9063 | valid bpc 11.4134 
--------------------------------------------------------------------------------
Start Training
| epoch 12 | 200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2898 | bpc 4.7664
| epoch 12 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3124 | bpc 4.8122
| epoch 12 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2867 | bpc 4.7716
| epoch 12 | 800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2676 | bpc 4.7383
| epoch 12 | 1000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2588 | bpc 4.7252
| epoch 12 | 1200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2730 | bpc 4.7444
| epoch 12 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2549 | bpc 4.7158
| epoch 12 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3255 | bpc 4.8115
| epoch 12 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2623 | bpc 4.7227
| epoch 12 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2670 | bpc 4.7418
| epoch 12 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2547 | bpc 4.7180
--------------------------------------------------------------------------------
| end of epoch 12 | time: 39.37s | train loss 3.2770 | train bpc 4.7508 | valid loss 7.9039 | valid bpc 11.4100 
--------------------------------------------------------------------------------
Start Training
| epoch 13 | 200/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2900 | bpc 4.7664
| epoch 13 | 400/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3137 | bpc 4.8143
| epoch 13 | 600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2866 | bpc 4.7713
| epoch 13 | 800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2708 | bpc 4.7431
| epoch 13 | 1000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2615 | bpc 4.7291
| epoch 13 | 1200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2705 | bpc 4.7406
| epoch 13 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2564 | bpc 4.7180
| epoch 13 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3250 | bpc 4.8108
| epoch 13 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2623 | bpc 4.7230
| epoch 13 | 2000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2666 | bpc 4.7409
| epoch 13 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2534 | bpc 4.7161
--------------------------------------------------------------------------------
| end of epoch 13 | time: 39.31s | train loss 3.2773 | train bpc 4.7513 | valid loss 7.9046 | valid bpc 11.4105 
--------------------------------------------------------------------------------
Start Training
| epoch 14 | 200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2908 | bpc 4.7681
| epoch 14 | 400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3128 | bpc 4.8127
| epoch 14 | 600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2864 | bpc 4.7711
| epoch 14 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2693 | bpc 4.7408
| epoch 14 | 1000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2582 | bpc 4.7244
| epoch 14 | 1200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2714 | bpc 4.7417
| epoch 14 | 1400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2554 | bpc 4.7165
| epoch 14 | 1600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3270 | bpc 4.8134
| epoch 14 | 1800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2635 | bpc 4.7245
| epoch 14 | 2000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2669 | bpc 4.7416
| epoch 14 | 2200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2539 | bpc 4.7168
--------------------------------------------------------------------------------
| end of epoch 14 | time: 39.38s | train loss 3.2771 | train bpc 4.7511 | valid loss 7.9060 | valid bpc 11.4128 
--------------------------------------------------------------------------------
Start Training
| epoch 15 | 200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2908 | bpc 4.7680
| epoch 15 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3120 | bpc 4.8114
| epoch 15 | 600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2867 | bpc 4.7715
| epoch 15 | 800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2704 | bpc 4.7427
| epoch 15 | 1000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2586 | bpc 4.7249
| epoch 15 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2705 | bpc 4.7404
| epoch 15 | 1400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2565 | bpc 4.7182
| epoch 15 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3256 | bpc 4.8117
| epoch 15 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2628 | bpc 4.7235
| epoch 15 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2666 | bpc 4.7411
| epoch 15 | 2200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2535 | bpc 4.7161
--------------------------------------------------------------------------------
| end of epoch 15 | time: 39.35s | train loss 3.2770 | train bpc 4.7509 | valid loss 7.9020 | valid bpc 11.4072 
--------------------------------------------------------------------------------
Start Training
| epoch 16 | 200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2888 | bpc 4.7649
| epoch 16 | 400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3134 | bpc 4.8136
| epoch 16 | 600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2866 | bpc 4.7715
| epoch 16 | 800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2710 | bpc 4.7432
| epoch 16 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2600 | bpc 4.7270
| epoch 16 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2725 | bpc 4.7433
| epoch 16 | 1400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2559 | bpc 4.7176
| epoch 16 | 1600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.3260 | bpc 4.8120
| epoch 16 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2626 | bpc 4.7235
| epoch 16 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2665 | bpc 4.7410
| epoch 16 | 2200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2552 | bpc 4.7187
--------------------------------------------------------------------------------
| end of epoch 16 | time: 39.41s | train loss 3.2775 | train bpc 4.7516 | valid loss 7.9028 | valid bpc 11.4082 
--------------------------------------------------------------------------------
Start Training
| epoch 17 | 200/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2892 | bpc 4.7653
| epoch 17 | 400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3139 | bpc 4.8145
| epoch 17 | 600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2866 | bpc 4.7714
| epoch 17 | 800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2696 | bpc 4.7414
| epoch 17 | 1000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2603 | bpc 4.7273
| epoch 17 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2732 | bpc 4.7446
| epoch 17 | 1400/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2555 | bpc 4.7165
| epoch 17 | 1600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3270 | bpc 4.8139
| epoch 17 | 1800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2626 | bpc 4.7231
| epoch 17 | 2000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2665 | bpc 4.7408
| epoch 17 | 2200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2546 | bpc 4.7180
--------------------------------------------------------------------------------
| end of epoch 17 | time: 39.36s | train loss 3.2775 | train bpc 4.7516 | valid loss 7.9050 | valid bpc 11.4111 
--------------------------------------------------------------------------------
Start Training
| epoch 18 | 200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2908 | bpc 4.7679
| epoch 18 | 400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3134 | bpc 4.8139
| epoch 18 | 600/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2862 | bpc 4.7706
| epoch 18 | 800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2692 | bpc 4.7409
| epoch 18 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2608 | bpc 4.7281
| epoch 18 | 1200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2709 | bpc 4.7411
| epoch 18 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2568 | bpc 4.7189
| epoch 18 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3271 | bpc 4.8137
| epoch 18 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2635 | bpc 4.7244
| epoch 18 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2677 | bpc 4.7428
| epoch 18 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2544 | bpc 4.7175
--------------------------------------------------------------------------------
| end of epoch 18 | time: 39.27s | train loss 3.2776 | train bpc 4.7518 | valid loss 7.9069 | valid bpc 11.4140 
--------------------------------------------------------------------------------
Start Training
| epoch 19 | 200/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.2907 | bpc 4.7678
| epoch 19 | 400/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 3.3111 | bpc 4.8100
| epoch 19 | 600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2866 | bpc 4.7715
| epoch 19 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2696 | bpc 4.7414
| epoch 19 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2597 | bpc 4.7263
| epoch 19 | 1200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2720 | bpc 4.7429
| epoch 19 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2560 | bpc 4.7175
| epoch 19 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3249 | bpc 4.8102
| epoch 19 | 1800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2635 | bpc 4.7248
| epoch 19 | 2000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2669 | bpc 4.7415
| epoch 19 | 2200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2548 | bpc 4.7182
--------------------------------------------------------------------------------
| end of epoch 19 | time: 39.20s | train loss 3.2772 | train bpc 4.7512 | valid loss 7.9055 | valid bpc 11.4122 
--------------------------------------------------------------------------------
Start Training
| epoch 20 | 200/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2910 | bpc 4.7680
| epoch 20 | 400/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.3118 | bpc 4.8113
| epoch 20 | 600/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2882 | bpc 4.7736
| epoch 20 | 800/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2683 | bpc 4.7394
| epoch 20 | 1000/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2599 | bpc 4.7268
| epoch 20 | 1200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2711 | bpc 4.7415
| epoch 20 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2551 | bpc 4.7160
| epoch 20 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3262 | bpc 4.8127
| epoch 20 | 1800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2627 | bpc 4.7234
| epoch 20 | 2000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2665 | bpc 4.7407
| epoch 20 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2548 | bpc 4.7180
--------------------------------------------------------------------------------
| end of epoch 20 | time: 39.22s | train loss 3.2772 | train bpc 4.7512 | valid loss 7.9027 | valid bpc 11.4082 
--------------------------------------------------------------------------------
Start Training
| epoch 21 | 200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2913 | bpc 4.7687
| epoch 21 | 400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3126 | bpc 4.8123
| epoch 21 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2861 | bpc 4.7708
| epoch 21 | 800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2696 | bpc 4.7412
| epoch 21 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2597 | bpc 4.7265
| epoch 21 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2728 | bpc 4.7439
| epoch 21 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2553 | bpc 4.7163
| epoch 21 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3266 | bpc 4.8132
| epoch 21 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2633 | bpc 4.7243
| epoch 21 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2656 | bpc 4.7398
| epoch 21 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2558 | bpc 4.7196
--------------------------------------------------------------------------------
| end of epoch 21 | time: 39.29s | train loss 3.2774 | train bpc 4.7515 | valid loss 7.9054 | valid bpc 11.4119 
--------------------------------------------------------------------------------
Start Training
| epoch 22 | 200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2909 | bpc 4.7681
| epoch 22 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3116 | bpc 4.8109
| epoch 22 | 600/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2867 | bpc 4.7717
| epoch 22 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2696 | bpc 4.7410
| epoch 22 | 1000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2612 | bpc 4.7288
| epoch 22 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2714 | bpc 4.7420
| epoch 22 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2568 | bpc 4.7186
| epoch 22 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3261 | bpc 4.8122
| epoch 22 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2620 | bpc 4.7225
| epoch 22 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2679 | bpc 4.7430
| epoch 22 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2538 | bpc 4.7169
--------------------------------------------------------------------------------
| end of epoch 22 | time: 39.32s | train loss 3.2774 | train bpc 4.7514 | valid loss 7.9040 | valid bpc 11.4099 
--------------------------------------------------------------------------------
Start Training
| epoch 23 | 200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2901 | bpc 4.7669
| epoch 23 | 400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3124 | bpc 4.8123
| epoch 23 | 600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2872 | bpc 4.7722
| epoch 23 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2691 | bpc 4.7406
| epoch 23 | 1000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2608 | bpc 4.7281
| epoch 23 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2727 | bpc 4.7437
| epoch 23 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2548 | bpc 4.7159
| epoch 23 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3261 | bpc 4.8123
| epoch 23 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2622 | bpc 4.7226
| epoch 23 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2682 | bpc 4.7435
| epoch 23 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2535 | bpc 4.7161
--------------------------------------------------------------------------------
| end of epoch 23 | time: 39.30s | train loss 3.2773 | train bpc 4.7514 | valid loss 7.9057 | valid bpc 11.4127 
--------------------------------------------------------------------------------
Start Training
| epoch 24 | 200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2897 | bpc 4.7664
| epoch 24 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3115 | bpc 4.8108
| epoch 24 | 600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2858 | bpc 4.7704
| epoch 24 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2690 | bpc 4.7403
| epoch 24 | 1000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2586 | bpc 4.7250
| epoch 24 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2726 | bpc 4.7434
| epoch 24 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2563 | bpc 4.7180
| epoch 24 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3260 | bpc 4.8123
| epoch 24 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2620 | bpc 4.7223
| epoch 24 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2670 | bpc 4.7417
| epoch 24 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2550 | bpc 4.7184
--------------------------------------------------------------------------------
| end of epoch 24 | time: 39.31s | train loss 3.2769 | train bpc 4.7508 | valid loss 7.9044 | valid bpc 11.4105 
--------------------------------------------------------------------------------
Start Training
| epoch 25 | 200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2893 | bpc 4.7654
| epoch 25 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3129 | bpc 4.8132
| epoch 25 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2874 | bpc 4.7728
| epoch 25 | 800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2689 | bpc 4.7403
| epoch 25 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2606 | bpc 4.7279
| epoch 25 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2707 | bpc 4.7408
| epoch 25 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2574 | bpc 4.7195
| epoch 25 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3260 | bpc 4.8123
| epoch 25 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2633 | bpc 4.7243
| epoch 25 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2665 | bpc 4.7409
| epoch 25 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2548 | bpc 4.7182
--------------------------------------------------------------------------------
| end of epoch 25 | time: 39.27s | train loss 3.2774 | train bpc 4.7515 | valid loss 7.9032 | valid bpc 11.4088 
--------------------------------------------------------------------------------
Start Training
| epoch 26 | 200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2902 | bpc 4.7668
| epoch 26 | 400/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.3114 | bpc 4.8110
| epoch 26 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2868 | bpc 4.7715
| epoch 26 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2694 | bpc 4.7410
| epoch 26 | 1000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2602 | bpc 4.7274
| epoch 26 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2717 | bpc 4.7424
| epoch 26 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2557 | bpc 4.7171
| epoch 26 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3266 | bpc 4.8131
| epoch 26 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2619 | bpc 4.7222
| epoch 26 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2670 | bpc 4.7418
| epoch 26 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2536 | bpc 4.7163
--------------------------------------------------------------------------------
| end of epoch 26 | time: 39.30s | train loss 3.2771 | train bpc 4.7511 | valid loss 7.9034 | valid bpc 11.4090 
--------------------------------------------------------------------------------
Start Training
| epoch 27 | 200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2900 | bpc 4.7667
| epoch 27 | 400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3133 | bpc 4.8135
| epoch 27 | 600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2866 | bpc 4.7713
| epoch 27 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2704 | bpc 4.7426
| epoch 27 | 1000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2590 | bpc 4.7254
| epoch 27 | 1200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2715 | bpc 4.7420
| epoch 27 | 1400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2555 | bpc 4.7166
| epoch 27 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3264 | bpc 4.8130
| epoch 27 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2636 | bpc 4.7246
| epoch 27 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2661 | bpc 4.7406
| epoch 27 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2537 | bpc 4.7165
--------------------------------------------------------------------------------
| end of epoch 27 | time: 39.29s | train loss 3.2772 | train bpc 4.7512 | valid loss 7.9035 | valid bpc 11.4093 
--------------------------------------------------------------------------------
Start Training
| epoch 28 | 200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2882 | bpc 4.7638
| epoch 28 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3135 | bpc 4.8137
| epoch 28 | 600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2859 | bpc 4.7706
| epoch 28 | 800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2712 | bpc 4.7438
| epoch 28 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2592 | bpc 4.7256
| epoch 28 | 1200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2721 | bpc 4.7428
| epoch 28 | 1400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2554 | bpc 4.7168
| epoch 28 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3265 | bpc 4.8127
| epoch 28 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2620 | bpc 4.7224
| epoch 28 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2673 | bpc 4.7423
| epoch 28 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2527 | bpc 4.7151
--------------------------------------------------------------------------------
| end of epoch 28 | time: 39.31s | train loss 3.2770 | train bpc 4.7509 | valid loss 7.9038 | valid bpc 11.4096 
--------------------------------------------------------------------------------
Start Training
| epoch 29 | 200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2905 | bpc 4.7673
| epoch 29 | 400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3137 | bpc 4.8142
| epoch 29 | 600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2874 | bpc 4.7725
| epoch 29 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2690 | bpc 4.7405
| epoch 29 | 1000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2598 | bpc 4.7266
| epoch 29 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2700 | bpc 4.7399
| epoch 29 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2558 | bpc 4.7173
| epoch 29 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3257 | bpc 4.8118
| epoch 29 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2619 | bpc 4.7222
| epoch 29 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2674 | bpc 4.7423
| epoch 29 | 2200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2538 | bpc 4.7166
--------------------------------------------------------------------------------
| end of epoch 29 | time: 39.23s | train loss 3.2771 | train bpc 4.7510 | valid loss 7.9042 | valid bpc 11.4101 
--------------------------------------------------------------------------------
Start Training
| epoch 30 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 3.2898 | bpc 4.7665
| epoch 30 | 400/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.3125 | bpc 4.8122
| epoch 30 | 600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2874 | bpc 4.7726
| epoch 30 | 800/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2698 | bpc 4.7416
| epoch 30 | 1000/2222 batches | lr 0.0010 | ms/batch 16.95 | loss 3.2598 | bpc 4.7264
| epoch 30 | 1200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2721 | bpc 4.7432
| epoch 30 | 1400/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2561 | bpc 4.7175
| epoch 30 | 1600/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.3258 | bpc 4.8122
| epoch 30 | 1800/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2617 | bpc 4.7218
| epoch 30 | 2000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2674 | bpc 4.7423
| epoch 30 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2529 | bpc 4.7152
--------------------------------------------------------------------------------
| end of epoch 30 | time: 39.22s | train loss 3.2772 | train bpc 4.7511 | valid loss 7.9041 | valid bpc 11.4099 
--------------------------------------------------------------------------------
Start Training
| epoch 31 | 200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2897 | bpc 4.7665
| epoch 31 | 400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3125 | bpc 4.8121
| epoch 31 | 600/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2860 | bpc 4.7706
| epoch 31 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2697 | bpc 4.7416
| epoch 31 | 1000/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2597 | bpc 4.7265
| epoch 31 | 1200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2709 | bpc 4.7411
| epoch 31 | 1400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2564 | bpc 4.7180
| epoch 31 | 1600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.3265 | bpc 4.8128
| epoch 31 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2608 | bpc 4.7207
| epoch 31 | 2000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2687 | bpc 4.7441
| epoch 31 | 2200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2541 | bpc 4.7169
--------------------------------------------------------------------------------
| end of epoch 31 | time: 39.33s | train loss 3.2771 | train bpc 4.7511 | valid loss 7.9052 | valid bpc 11.4116 
--------------------------------------------------------------------------------
Start Training
| epoch 32 | 200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2890 | bpc 4.7651
| epoch 32 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3137 | bpc 4.8142
| epoch 32 | 600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2873 | bpc 4.7726
| epoch 32 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2698 | bpc 4.7415
| epoch 32 | 1000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2598 | bpc 4.7264
| epoch 32 | 1200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2720 | bpc 4.7430
| epoch 32 | 1400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2567 | bpc 4.7187
| epoch 32 | 1600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3262 | bpc 4.8126
| epoch 32 | 1800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2623 | bpc 4.7227
| epoch 32 | 2000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2675 | bpc 4.7424
| epoch 32 | 2200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2543 | bpc 4.7172
--------------------------------------------------------------------------------
| end of epoch 32 | time: 39.23s | train loss 3.2774 | train bpc 4.7515 | valid loss 7.9049 | valid bpc 11.4115 
--------------------------------------------------------------------------------
Start Training
| epoch 33 | 200/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 3.2895 | bpc 4.7658
| epoch 33 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3117 | bpc 4.8113
| epoch 33 | 600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2873 | bpc 4.7725
| epoch 33 | 800/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2695 | bpc 4.7410
| epoch 33 | 1000/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2592 | bpc 4.7260
| epoch 33 | 1200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2710 | bpc 4.7412
| epoch 33 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2561 | bpc 4.7178
| epoch 33 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3265 | bpc 4.8127
| epoch 33 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2622 | bpc 4.7229
| epoch 33 | 2000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2680 | bpc 4.7432
| epoch 33 | 2200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2533 | bpc 4.7158
--------------------------------------------------------------------------------
| end of epoch 33 | time: 39.23s | train loss 3.2770 | train bpc 4.7509 | valid loss 7.9025 | valid bpc 11.4078 
--------------------------------------------------------------------------------
Start Training
| epoch 34 | 200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2902 | bpc 4.7671
| epoch 34 | 400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3124 | bpc 4.8121
| epoch 34 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2874 | bpc 4.7727
| epoch 34 | 800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2692 | bpc 4.7407
| epoch 34 | 1000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2590 | bpc 4.7255
| epoch 34 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2712 | bpc 4.7415
| epoch 34 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2566 | bpc 4.7186
| epoch 34 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3262 | bpc 4.8125
| epoch 34 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2622 | bpc 4.7226
| epoch 34 | 2000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2668 | bpc 4.7414
| epoch 34 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2547 | bpc 4.7178
--------------------------------------------------------------------------------
| end of epoch 34 | time: 39.34s | train loss 3.2773 | train bpc 4.7513 | valid loss 7.9059 | valid bpc 11.4128 
--------------------------------------------------------------------------------
Start Training
| epoch 35 | 200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2905 | bpc 4.7673
| epoch 35 | 400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3122 | bpc 4.8120
| epoch 35 | 600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2868 | bpc 4.7719
| epoch 35 | 800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2699 | bpc 4.7415
| epoch 35 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2602 | bpc 4.7272
| epoch 35 | 1200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2728 | bpc 4.7442
| epoch 35 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2577 | bpc 4.7198
| epoch 35 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3274 | bpc 4.8142
| epoch 35 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2608 | bpc 4.7205
| epoch 35 | 2000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2672 | bpc 4.7422
| epoch 35 | 2200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2542 | bpc 4.7172
--------------------------------------------------------------------------------
| end of epoch 35 | time: 39.27s | train loss 3.2775 | train bpc 4.7517 | valid loss 7.9037 | valid bpc 11.4096 
--------------------------------------------------------------------------------
Start Training
| epoch 36 | 200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2901 | bpc 4.7667
| epoch 36 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3126 | bpc 4.8123
| epoch 36 | 600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2873 | bpc 4.7726
| epoch 36 | 800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2701 | bpc 4.7421
| epoch 36 | 1000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2610 | bpc 4.7283
| epoch 36 | 1200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2706 | bpc 4.7407
| epoch 36 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2566 | bpc 4.7183
| epoch 36 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3256 | bpc 4.8115
| epoch 36 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2630 | bpc 4.7239
| epoch 36 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2652 | bpc 4.7391
| epoch 36 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2555 | bpc 4.7193
--------------------------------------------------------------------------------
| end of epoch 36 | time: 39.29s | train loss 3.2774 | train bpc 4.7514 | valid loss 7.9042 | valid bpc 11.4103 
--------------------------------------------------------------------------------
Start Training
| epoch 37 | 200/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 3.2902 | bpc 4.7667
| epoch 37 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3126 | bpc 4.8127
| epoch 37 | 600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2863 | bpc 4.7707
| epoch 37 | 800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2717 | bpc 4.7446
| epoch 37 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2599 | bpc 4.7265
| epoch 37 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2716 | bpc 4.7423
| epoch 37 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2571 | bpc 4.7191
| epoch 37 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3265 | bpc 4.8129
| epoch 37 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2621 | bpc 4.7226
| epoch 37 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2691 | bpc 4.7447
| epoch 37 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2525 | bpc 4.7146
--------------------------------------------------------------------------------
| end of epoch 37 | time: 39.29s | train loss 3.2776 | train bpc 4.7517 | valid loss 7.9047 | valid bpc 11.4108 
--------------------------------------------------------------------------------
Start Training
| epoch 38 | 200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2899 | bpc 4.7665
| epoch 38 | 400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3137 | bpc 4.8142
| epoch 38 | 600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2878 | bpc 4.7731
| epoch 38 | 800/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2700 | bpc 4.7417
| epoch 38 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2592 | bpc 4.7258
| epoch 38 | 1200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2712 | bpc 4.7414
| epoch 38 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2561 | bpc 4.7178
| epoch 38 | 1600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3263 | bpc 4.8125
| epoch 38 | 1800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2627 | bpc 4.7235
| epoch 38 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2661 | bpc 4.7405
| epoch 38 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2541 | bpc 4.7171
--------------------------------------------------------------------------------
| end of epoch 38 | time: 39.29s | train loss 3.2773 | train bpc 4.7513 | valid loss 7.9054 | valid bpc 11.4123 
--------------------------------------------------------------------------------
Start Training
| epoch 39 | 200/2222 batches | lr 0.0010 | ms/batch 17.28 | loss 3.2904 | bpc 4.7673
| epoch 39 | 400/2222 batches | lr 0.0010 | ms/batch 17.14 | loss 3.3128 | bpc 4.8128
| epoch 39 | 600/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 3.2859 | bpc 4.7703
| epoch 39 | 800/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2691 | bpc 4.7407
| epoch 39 | 1000/2222 batches | lr 0.0010 | ms/batch 17.09 | loss 3.2594 | bpc 4.7261
| epoch 39 | 1200/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.2721 | bpc 4.7430
| epoch 39 | 1400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2558 | bpc 4.7170
| epoch 39 | 1600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3254 | bpc 4.8113
| epoch 39 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2617 | bpc 4.7222
| epoch 39 | 2000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2663 | bpc 4.7408
| epoch 39 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2535 | bpc 4.7159
--------------------------------------------------------------------------------
| end of epoch 39 | time: 39.34s | train loss 3.2769 | train bpc 4.7508 | valid loss 7.9033 | valid bpc 11.4089 
--------------------------------------------------------------------------------
Start Training
| epoch 40 | 200/2222 batches | lr 0.0010 | ms/batch 17.10 | loss 3.2899 | bpc 4.7663
| epoch 40 | 400/2222 batches | lr 0.0010 | ms/batch 17.08 | loss 3.3123 | bpc 4.8122
| epoch 40 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2878 | bpc 4.7732
| epoch 40 | 800/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2696 | bpc 4.7414
| epoch 40 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2594 | bpc 4.7261
| epoch 40 | 1200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2723 | bpc 4.7433
| epoch 40 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2550 | bpc 4.7158
| epoch 40 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3266 | bpc 4.8133
| epoch 40 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2622 | bpc 4.7226
| epoch 40 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2675 | bpc 4.7424
| epoch 40 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2527 | bpc 4.7150
--------------------------------------------------------------------------------
| end of epoch 40 | time: 39.31s | train loss 3.2771 | train bpc 4.7511 | valid loss 7.9068 | valid bpc 11.4140 
--------------------------------------------------------------------------------
Start Training
| epoch 41 | 200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2908 | bpc 4.7679
| epoch 41 | 400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.3116 | bpc 4.8110
| epoch 41 | 600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2872 | bpc 4.7721
| epoch 41 | 800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2695 | bpc 4.7414
| epoch 41 | 1000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2593 | bpc 4.7260
| epoch 41 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2732 | bpc 4.7445
| epoch 41 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2553 | bpc 4.7164
| epoch 41 | 1600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.3265 | bpc 4.8129
| epoch 41 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2616 | bpc 4.7218
| epoch 41 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2667 | bpc 4.7411
| epoch 41 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2524 | bpc 4.7148
--------------------------------------------------------------------------------
| end of epoch 41 | time: 39.33s | train loss 3.2770 | train bpc 4.7509 | valid loss 7.9040 | valid bpc 11.4096 
--------------------------------------------------------------------------------
Start Training
| epoch 42 | 200/2222 batches | lr 0.0010 | ms/batch 17.12 | loss 3.2896 | bpc 4.7661
| epoch 42 | 400/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.3123 | bpc 4.8122
| epoch 42 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2863 | bpc 4.7708
| epoch 42 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2707 | bpc 4.7430
| epoch 42 | 1000/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2588 | bpc 4.7250
| epoch 42 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2714 | bpc 4.7419
| epoch 42 | 1400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2574 | bpc 4.7194
| epoch 42 | 1600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.3261 | bpc 4.8124
| epoch 42 | 1800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2628 | bpc 4.7235
| epoch 42 | 2000/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2676 | bpc 4.7426
| epoch 42 | 2200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2538 | bpc 4.7168
--------------------------------------------------------------------------------
| end of epoch 42 | time: 39.42s | train loss 3.2773 | train bpc 4.7513 | valid loss 7.9048 | valid bpc 11.4112 
--------------------------------------------------------------------------------
Start Training
| epoch 43 | 200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2889 | bpc 4.7651
| epoch 43 | 400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3128 | bpc 4.8127
| epoch 43 | 600/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2874 | bpc 4.7725
| epoch 43 | 800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2693 | bpc 4.7407
| epoch 43 | 1000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2604 | bpc 4.7274
| epoch 43 | 1200/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2730 | bpc 4.7445
| epoch 43 | 1400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2566 | bpc 4.7182
| epoch 43 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3259 | bpc 4.8121
| epoch 43 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2634 | bpc 4.7245
| epoch 43 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2671 | bpc 4.7418
| epoch 43 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2532 | bpc 4.7158
--------------------------------------------------------------------------------
| end of epoch 43 | time: 39.50s | train loss 3.2774 | train bpc 4.7515 | valid loss 7.9030 | valid bpc 11.4084 
--------------------------------------------------------------------------------
Start Training
| epoch 44 | 200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2890 | bpc 4.7653
| epoch 44 | 400/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.3131 | bpc 4.8132
| epoch 44 | 600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 3.2866 | bpc 4.7713
| epoch 44 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2688 | bpc 4.7400
| epoch 44 | 1000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2594 | bpc 4.7260
| epoch 44 | 1200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2727 | bpc 4.7440
| epoch 44 | 1400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2567 | bpc 4.7183
| epoch 44 | 1600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3271 | bpc 4.8139
| epoch 44 | 1800/2222 batches | lr 0.0010 | ms/batch 16.98 | loss 3.2628 | bpc 4.7237
| epoch 44 | 2000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2658 | bpc 4.7401
| epoch 44 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.2537 | bpc 4.7164
--------------------------------------------------------------------------------
| end of epoch 44 | time: 39.42s | train loss 3.2772 | train bpc 4.7512 | valid loss 7.9058 | valid bpc 11.4126 
--------------------------------------------------------------------------------
Start Training
| epoch 45 | 200/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2915 | bpc 4.7688
| epoch 45 | 400/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 3.3124 | bpc 4.8119
| epoch 45 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2868 | bpc 4.7721
| epoch 45 | 800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2701 | bpc 4.7419
| epoch 45 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2589 | bpc 4.7253
| epoch 45 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2714 | bpc 4.7419
| epoch 45 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2557 | bpc 4.7170
| epoch 45 | 1600/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3253 | bpc 4.8112
| epoch 45 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2630 | bpc 4.7238
| epoch 45 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2672 | bpc 4.7423
| epoch 45 | 2200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2540 | bpc 4.7169
--------------------------------------------------------------------------------
| end of epoch 45 | time: 39.56s | train loss 3.2772 | train bpc 4.7512 | valid loss 7.9036 | valid bpc 11.4095 
--------------------------------------------------------------------------------
Start Training
| epoch 46 | 200/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2900 | bpc 4.7668
| epoch 46 | 400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3123 | bpc 4.8119
| epoch 46 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2868 | bpc 4.7721
| epoch 46 | 800/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2697 | bpc 4.7410
| epoch 46 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2605 | bpc 4.7279
| epoch 46 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2718 | bpc 4.7425
| epoch 46 | 1400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2557 | bpc 4.7168
| epoch 46 | 1600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.3253 | bpc 4.8114
| epoch 46 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2620 | bpc 4.7222
| epoch 46 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2682 | bpc 4.7435
| epoch 46 | 2200/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2533 | bpc 4.7161
--------------------------------------------------------------------------------
| end of epoch 46 | time: 39.36s | train loss 3.2772 | train bpc 4.7512 | valid loss 7.9058 | valid bpc 11.4126 
--------------------------------------------------------------------------------
Start Training
| epoch 47 | 200/2222 batches | lr 0.0010 | ms/batch 17.07 | loss 3.2899 | bpc 4.7666
| epoch 47 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.3116 | bpc 4.8110
| epoch 47 | 600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2869 | bpc 4.7720
| epoch 47 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2704 | bpc 4.7424
| epoch 47 | 1000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2604 | bpc 4.7276
| epoch 47 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2718 | bpc 4.7427
| epoch 47 | 1400/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.2553 | bpc 4.7161
| epoch 47 | 1600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 3.3265 | bpc 4.8128
| epoch 47 | 1800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2617 | bpc 4.7220
| epoch 47 | 2000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2672 | bpc 4.7419
| epoch 47 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2558 | bpc 4.7195
--------------------------------------------------------------------------------
| end of epoch 47 | time: 39.30s | train loss 3.2774 | train bpc 4.7514 | valid loss 7.9043 | valid bpc 11.4105 
--------------------------------------------------------------------------------
Start Training
| epoch 48 | 200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2901 | bpc 4.7668
| epoch 48 | 400/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.3129 | bpc 4.8131
| epoch 48 | 600/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 3.2871 | bpc 4.7721
| epoch 48 | 800/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 3.2712 | bpc 4.7437
| epoch 48 | 1000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2605 | bpc 4.7277
| epoch 48 | 1200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2724 | bpc 4.7433
| epoch 48 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2564 | bpc 4.7180
| epoch 48 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3266 | bpc 4.8131
| epoch 48 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2621 | bpc 4.7226
| epoch 48 | 2000/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2671 | bpc 4.7417
| epoch 48 | 2200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2535 | bpc 4.7162
--------------------------------------------------------------------------------
| end of epoch 48 | time: 39.32s | train loss 3.2776 | train bpc 4.7517 | valid loss 7.9049 | valid bpc 11.4111 
--------------------------------------------------------------------------------
Start Training
| epoch 49 | 200/2222 batches | lr 0.0010 | ms/batch 16.97 | loss 3.2900 | bpc 4.7667
| epoch 49 | 400/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.3122 | bpc 4.8120
| epoch 49 | 600/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2867 | bpc 4.7716
| epoch 49 | 800/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2697 | bpc 4.7414
| epoch 49 | 1000/2222 batches | lr 0.0010 | ms/batch 16.99 | loss 3.2588 | bpc 4.7254
| epoch 49 | 1200/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2716 | bpc 4.7422
| epoch 49 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2557 | bpc 4.7170
| epoch 49 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3256 | bpc 4.8114
| epoch 49 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2614 | bpc 4.7217
| epoch 49 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2653 | bpc 4.7393
| epoch 49 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2558 | bpc 4.7195
--------------------------------------------------------------------------------
| end of epoch 49 | time: 39.28s | train loss 3.2769 | train bpc 4.7508 | valid loss 7.9037 | valid bpc 11.4094 
--------------------------------------------------------------------------------
Start Training
| epoch 50 | 200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2900 | bpc 4.7669
| epoch 50 | 400/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.3122 | bpc 4.8115
| epoch 50 | 600/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2876 | bpc 4.7730
| epoch 50 | 800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2694 | bpc 4.7411
| epoch 50 | 1000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 3.2587 | bpc 4.7251
| epoch 50 | 1200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2721 | bpc 4.7427
| epoch 50 | 1400/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2573 | bpc 4.7194
| epoch 50 | 1600/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.3257 | bpc 4.8118
| epoch 50 | 1800/2222 batches | lr 0.0010 | ms/batch 17.02 | loss 3.2609 | bpc 4.7207
| epoch 50 | 2000/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2668 | bpc 4.7414
| epoch 50 | 2200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 3.2548 | bpc 4.7182
--------------------------------------------------------------------------------
| end of epoch 50 | time: 39.31s | train loss 3.2772 | train bpc 4.7511 | valid loss 7.9053 | valid bpc 11.4121 
--------------------------------------------------------------------------------
Run history:

epoch	
lr	
train_bpc	
train_loss	
val_bpc	
val_loss	

Run summary:

epoch	50
lr	0.001
train_bpc	4.75115
train_loss	3.27718
val_bpc	11.41207
val_loss	7.90533

View run efficient-firebrand-23 at: https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/xe0vn41d
View project at: https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
Find logs at: ./wandb/run-20240822_223105-xe0vn41d/logs
Start Test Validation
Evaluating batch 1/124
Evaluating batch 2/124
Evaluating batch 3/124
Evaluating batch 4/124
Evaluating batch 5/124
Evaluating batch 6/124
Evaluating batch 7/124
Evaluating batch 8/124
Evaluating batch 9/124
Evaluating batch 10/124
Evaluating batch 11/124
Evaluating batch 12/124
Evaluating batch 13/124
Evaluating batch 14/124
Evaluating batch 15/124
Evaluating batch 16/124
Evaluating batch 17/124
Evaluating batch 18/124
Evaluating batch 19/124
Evaluating batch 20/124
Evaluating batch 21/124
Evaluating batch 22/124
Evaluating batch 23/124
Evaluating batch 24/124
Evaluating batch 25/124
Evaluating batch 26/124
Evaluating batch 27/124
Evaluating batch 28/124
Evaluating batch 29/124
Evaluating batch 30/124
Evaluating batch 31/124
Evaluating batch 32/124
Evaluating batch 33/124
Evaluating batch 34/124
Evaluating batch 35/124
Evaluating batch 36/124
Evaluating batch 37/124
Evaluating batch 38/124
Evaluating batch 39/124
Evaluating batch 40/124
Evaluating batch 41/124
Evaluating batch 42/124
Evaluating batch 43/124
Evaluating batch 44/124
Evaluating batch 45/124
Evaluating batch 46/124
Evaluating batch 47/124
Evaluating batch 48/124
Evaluating batch 49/124
Evaluating batch 50/124
Evaluating batch 51/124
Evaluating batch 52/124
Evaluating batch 53/124
Evaluating batch 54/124
Evaluating batch 55/124
Evaluating batch 56/124
Evaluating batch 57/124
Evaluating batch 58/124
Evaluating batch 59/124
Evaluating batch 60/124
Evaluating batch 61/124
Evaluating batch 62/124
Evaluating batch 63/124
Evaluating batch 64/124
Evaluating batch 65/124
Evaluating batch 66/124
Evaluating batch 67/124
Evaluating batch 68/124
Evaluating batch 69/124
Evaluating batch 70/124
Evaluating batch 71/124
Evaluating batch 72/124
Evaluating batch 73/124
Evaluating batch 74/124
Evaluating batch 75/124
Evaluating batch 76/124
Evaluating batch 77/124
Evaluating batch 78/124
Evaluating batch 79/124
Evaluating batch 80/124
Evaluating batch 81/124
Evaluating batch 82/124
Evaluating batch 83/124
Evaluating batch 84/124
Evaluating batch 85/124
Evaluating batch 86/124
Evaluating batch 87/124
Evaluating batch 88/124
Evaluating batch 89/124
Evaluating batch 90/124
Evaluating batch 91/124
Evaluating batch 92/124
Evaluating batch 93/124
Evaluating batch 94/124
Evaluating batch 95/124
Evaluating batch 96/124
Evaluating batch 97/124
Evaluating batch 98/124
Evaluating batch 99/124
Evaluating batch 100/124
Evaluating batch 101/124
Evaluating batch 102/124
Evaluating batch 103/124
Evaluating batch 104/124
Evaluating batch 105/124
Evaluating batch 106/124
Evaluating batch 107/124
Evaluating batch 108/124
Evaluating batch 109/124
Evaluating batch 110/124
Evaluating batch 111/124
Evaluating batch 112/124
Evaluating batch 113/124
Evaluating batch 114/124
Evaluating batch 115/124
Evaluating batch 116/124
Evaluating batch 117/124
Evaluating batch 118/124
Evaluating batch 119/124
Evaluating batch 120/124
Evaluating batch 121/124
Evaluating batch 122/124
Evaluating batch 123/124
Evaluating batch 124/124
================================================================================
| End of training | test loss 6.785205364227295 | test bpc 9.807204246520996 
================================================================================
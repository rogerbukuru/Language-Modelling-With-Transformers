Tracking run with wandb version 0.17.7
Run data is saved locally in /content/wandb/run-20240825_132733-58tc1msa
Syncing run summer-eon-26 to Weights & Biases (docs)
View project at https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers
View run at https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/58tc1msa
Start Training
| epoch 1 | 200/2222 batches | lr 0.0010 | ms/batch 32.52 | loss 3.2157 | bpc 4.6472
| epoch 1 | 400/2222 batches | lr 0.0010 | ms/batch 24.40 | loss 2.8497 | bpc 4.1165
| epoch 1 | 600/2222 batches | lr 0.0010 | ms/batch 21.80 | loss 2.5108 | bpc 3.6268
| epoch 1 | 800/2222 batches | lr 0.0010 | ms/batch 20.44 | loss 2.3455 | bpc 3.3896
| epoch 1 | 1000/2222 batches | lr 0.0010 | ms/batch 19.64 | loss 2.2736 | bpc 3.2839
| epoch 1 | 1200/2222 batches | lr 0.0010 | ms/batch 19.13 | loss 2.2579 | bpc 3.2644
| epoch 1 | 1400/2222 batches | lr 0.0010 | ms/batch 18.79 | loss 2.1764 | bpc 3.1464
| epoch 1 | 1600/2222 batches | lr 0.0010 | ms/batch 18.53 | loss 2.2135 | bpc 3.1974
| epoch 1 | 1800/2222 batches | lr 0.0010 | ms/batch 18.33 | loss 2.0859 | bpc 3.0219
| epoch 1 | 2000/2222 batches | lr 0.0010 | ms/batch 18.18 | loss 2.0015 | bpc 2.8956
| epoch 1 | 2200/2222 batches | lr 0.0010 | ms/batch 18.06 | loss 1.9622 | bpc 2.8363
--------------------------------------------------------------------------------
| end of epoch 1 | time: 42.46s | train loss 2.3491 | train bpc 3.3954 | valid loss 9.6051 | valid bpc 13.8581 
--------------------------------------------------------------------------------
Start Training
| epoch 2 | 200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.9748 | bpc 2.8585
| epoch 2 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.9307 | bpc 2.7905
| epoch 2 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.8449 | bpc 2.6659
| epoch 2 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.8089 | bpc 2.6150
| epoch 2 | 1000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.7187 | bpc 2.4829
| epoch 2 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.7577 | bpc 2.5420
| epoch 2 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.7201 | bpc 2.4874
| epoch 2 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.9228 | bpc 2.7775
| epoch 2 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 1.6351 | bpc 2.3703
| epoch 2 | 2000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 1.5393 | bpc 2.2278
| epoch 2 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 1.5662 | bpc 2.2645
--------------------------------------------------------------------------------
| end of epoch 2 | time: 39.24s | train loss 1.7632 | train bpc 2.5497 | valid loss 9.3651 | valid bpc 13.5106 
--------------------------------------------------------------------------------
Start Training
| epoch 3 | 200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.5983 | bpc 2.3142
| epoch 3 | 400/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.5449 | bpc 2.2333
| epoch 3 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.4892 | bpc 2.1524
| epoch 3 | 800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.4792 | bpc 2.1387
| epoch 3 | 1000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.4257 | bpc 2.0598
| epoch 3 | 1200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.4937 | bpc 2.1609
| epoch 3 | 1400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.5066 | bpc 2.1791
| epoch 3 | 1600/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.7886 | bpc 2.5839
| epoch 3 | 1800/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.4374 | bpc 2.0849
| epoch 3 | 2000/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.3554 | bpc 1.9622
| epoch 3 | 2200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.4111 | bpc 2.0407
--------------------------------------------------------------------------------
| end of epoch 3 | time: 38.82s | train loss 1.5018 | train bpc 2.1723 | valid loss 9.4302 | valid bpc 13.6049 
--------------------------------------------------------------------------------
Start Training
| epoch 4 | 200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.4534 | bpc 2.1048
| epoch 4 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.3985 | bpc 2.0222
| epoch 4 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3612 | bpc 1.9677
| epoch 4 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3568 | bpc 1.9623
| epoch 4 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.3209 | bpc 1.9088
| epoch 4 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.3945 | bpc 2.0178
| epoch 4 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.4241 | bpc 2.0600
| epoch 4 | 1600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.7270 | bpc 2.4951
| epoch 4 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3581 | bpc 1.9705
| epoch 4 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2846 | bpc 1.8603
| epoch 4 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3421 | bpc 1.9411
--------------------------------------------------------------------------------
| end of epoch 4 | time: 39.00s | train loss 1.4016 | train bpc 2.0277 | valid loss 9.7405 | valid bpc 14.0521 
--------------------------------------------------------------------------------
Start Training
| epoch 5 | 200/2222 batches | lr 0.0010 | ms/batch 16.76 | loss 1.3867 | bpc 2.0085
| epoch 5 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.3348 | bpc 1.9304
| epoch 5 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.2993 | bpc 1.8783
| epoch 5 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2989 | bpc 1.8787
| epoch 5 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2663 | bpc 1.8299
| epoch 5 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3420 | bpc 1.9420
| epoch 5 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.3805 | bpc 1.9970
| epoch 5 | 1600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.6894 | bpc 2.4408
| epoch 5 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.3159 | bpc 1.9092
| epoch 5 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2421 | bpc 1.7987
| epoch 5 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3040 | bpc 1.8862
--------------------------------------------------------------------------------
| end of epoch 5 | time: 38.98s | train loss 1.3506 | train bpc 1.9541 | valid loss 9.5376 | valid bpc 13.7596 
--------------------------------------------------------------------------------
Start Training
| epoch 6 | 200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.3474 | bpc 1.9516
| epoch 6 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2920 | bpc 1.8685
| epoch 6 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2607 | bpc 1.8226
| epoch 6 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2602 | bpc 1.8228
| epoch 6 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2270 | bpc 1.7730
| epoch 6 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.3080 | bpc 1.8929
| epoch 6 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.3469 | bpc 1.9484
| epoch 6 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.6631 | bpc 2.4026
| epoch 6 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2858 | bpc 1.8660
| epoch 6 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2156 | bpc 1.7604
| epoch 6 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2771 | bpc 1.8474
--------------------------------------------------------------------------------
| end of epoch 6 | time: 38.95s | train loss 1.3165 | train bpc 1.9047 | valid loss 9.4744 | valid bpc 13.6688 
--------------------------------------------------------------------------------
Start Training
| epoch 7 | 200/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.3183 | bpc 1.9098
| epoch 7 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.2639 | bpc 1.8281
| epoch 7 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.2334 | bpc 1.7832
| epoch 7 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2327 | bpc 1.7832
| epoch 7 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1996 | bpc 1.7335
| epoch 7 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2822 | bpc 1.8557
| epoch 7 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.3257 | bpc 1.9178
| epoch 7 | 1600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.6423 | bpc 2.3728
| epoch 7 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2644 | bpc 1.8344
| epoch 7 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1928 | bpc 1.7280
| epoch 7 | 2200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2577 | bpc 1.8193
--------------------------------------------------------------------------------
| end of epoch 7 | time: 38.92s | train loss 1.2919 | train bpc 1.8693 | valid loss 8.9273 | valid bpc 12.8794 
--------------------------------------------------------------------------------
Start Training
| epoch 8 | 200/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2976 | bpc 1.8798
| epoch 8 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2411 | bpc 1.7951
| epoch 8 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2127 | bpc 1.7532
| epoch 8 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2098 | bpc 1.7501
| epoch 8 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1772 | bpc 1.7014
| epoch 8 | 1200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2613 | bpc 1.8255
| epoch 8 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.3085 | bpc 1.8929
| epoch 8 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.6261 | bpc 2.3495
| epoch 8 | 1800/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2451 | bpc 1.8068
| epoch 8 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1781 | bpc 1.7067
| epoch 8 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2424 | bpc 1.7972
--------------------------------------------------------------------------------
| end of epoch 8 | time: 38.97s | train loss 1.2726 | train bpc 1.8414 | valid loss 8.6993 | valid bpc 12.5512 
--------------------------------------------------------------------------------
Start Training
| epoch 9 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2806 | bpc 1.8555
| epoch 9 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.2236 | bpc 1.7701
| epoch 9 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1955 | bpc 1.7286
| epoch 9 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1919 | bpc 1.7243
| epoch 9 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1627 | bpc 1.6803
| epoch 9 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2456 | bpc 1.8030
| epoch 9 | 1400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2946 | bpc 1.8731
| epoch 9 | 1600/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.6135 | bpc 2.3313
| epoch 9 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.2322 | bpc 1.7885
| epoch 9 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1647 | bpc 1.6872
| epoch 9 | 2200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2304 | bpc 1.7800
--------------------------------------------------------------------------------
| end of epoch 9 | time: 38.89s | train loss 1.2577 | train bpc 1.8200 | valid loss 9.1141 | valid bpc 13.1492 
--------------------------------------------------------------------------------
Start Training
| epoch 10 | 200/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2678 | bpc 1.8372
| epoch 10 | 400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.2083 | bpc 1.7478
| epoch 10 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1831 | bpc 1.7106
| epoch 10 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1779 | bpc 1.7043
| epoch 10 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1468 | bpc 1.6575
| epoch 10 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2346 | bpc 1.7871
| epoch 10 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2820 | bpc 1.8548
| epoch 10 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.6039 | bpc 2.3175
| epoch 10 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2224 | bpc 1.7745
| epoch 10 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1536 | bpc 1.6711
| epoch 10 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2202 | bpc 1.7655
--------------------------------------------------------------------------------
| end of epoch 10 | time: 38.95s | train loss 1.2455 | train bpc 1.8025 | valid loss 8.9930 | valid bpc 12.9742 
--------------------------------------------------------------------------------
Start Training
| epoch 11 | 200/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2551 | bpc 1.8186
| epoch 11 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1981 | bpc 1.7335
| epoch 11 | 600/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1700 | bpc 1.6918
| epoch 11 | 800/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1667 | bpc 1.6881
| epoch 11 | 1000/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1368 | bpc 1.6431
| epoch 11 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2219 | bpc 1.7689
| epoch 11 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2743 | bpc 1.8435
| epoch 11 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5949 | bpc 2.3044
| epoch 11 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2129 | bpc 1.7608
| epoch 11 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1470 | bpc 1.6617
| epoch 11 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2112 | bpc 1.7521
--------------------------------------------------------------------------------
| end of epoch 11 | time: 38.96s | train loss 1.2354 | train bpc 1.7878 | valid loss 8.7056 | valid bpc 12.5595 
--------------------------------------------------------------------------------
Start Training
| epoch 12 | 200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.2480 | bpc 1.8085
| epoch 12 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1882 | bpc 1.7188
| epoch 12 | 600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1609 | bpc 1.6785
| epoch 12 | 800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1559 | bpc 1.6726
| epoch 12 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1273 | bpc 1.6293
| epoch 12 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2131 | bpc 1.7562
| epoch 12 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2670 | bpc 1.8332
| epoch 12 | 1600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.5870 | bpc 2.2930
| epoch 12 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2047 | bpc 1.7491
| epoch 12 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1395 | bpc 1.6509
| epoch 12 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2028 | bpc 1.7402
--------------------------------------------------------------------------------
| end of epoch 12 | time: 38.97s | train loss 1.2268 | train bpc 1.7755 | valid loss 9.2089 | valid bpc 13.2858 
--------------------------------------------------------------------------------
Start Training
| epoch 13 | 200/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.2394 | bpc 1.7962
| epoch 13 | 400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1815 | bpc 1.7093
| epoch 13 | 600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.1523 | bpc 1.6663
| epoch 13 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1480 | bpc 1.6610
| epoch 13 | 1000/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 1.1168 | bpc 1.6143
| epoch 13 | 1200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.2064 | bpc 1.7464
| epoch 13 | 1400/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 1.2601 | bpc 1.8234
| epoch 13 | 1600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.5809 | bpc 2.2841
| epoch 13 | 1800/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1979 | bpc 1.7392
| epoch 13 | 2000/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.1328 | bpc 1.6410
| epoch 13 | 2200/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1976 | bpc 1.7327
--------------------------------------------------------------------------------
| end of epoch 13 | time: 39.04s | train loss 1.2195 | train bpc 1.7650 | valid loss 9.3098 | valid bpc 13.4315 
--------------------------------------------------------------------------------
Start Training
| epoch 14 | 200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2330 | bpc 1.7869
| epoch 14 | 400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1726 | bpc 1.6965
| epoch 14 | 600/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1464 | bpc 1.6577
| epoch 14 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1410 | bpc 1.6509
| epoch 14 | 1000/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1121 | bpc 1.6075
| epoch 14 | 1200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1971 | bpc 1.7330
| epoch 14 | 1400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.2558 | bpc 1.8169
| epoch 14 | 1600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.5759 | bpc 2.2772
| epoch 14 | 1800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1905 | bpc 1.7283
| epoch 14 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1274 | bpc 1.6332
| epoch 14 | 2200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1922 | bpc 1.7248
--------------------------------------------------------------------------------
| end of epoch 14 | time: 39.01s | train loss 1.2131 | train bpc 1.7556 | valid loss 9.3954 | valid bpc 13.5549 
--------------------------------------------------------------------------------
Start Training
| epoch 15 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2285 | bpc 1.7805
| epoch 15 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1659 | bpc 1.6867
| epoch 15 | 600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.1400 | bpc 1.6483
| epoch 15 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1339 | bpc 1.6407
| epoch 15 | 1000/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1040 | bpc 1.5957
| epoch 15 | 1200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1926 | bpc 1.7266
| epoch 15 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2484 | bpc 1.8065
| epoch 15 | 1600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.5734 | bpc 2.2733
| epoch 15 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1846 | bpc 1.7200
| epoch 15 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1238 | bpc 1.6281
| epoch 15 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1876 | bpc 1.7184
--------------------------------------------------------------------------------
| end of epoch 15 | time: 38.99s | train loss 1.2076 | train bpc 1.7478 | valid loss 9.1539 | valid bpc 13.2065 
--------------------------------------------------------------------------------
Start Training
| epoch 16 | 200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2240 | bpc 1.7740
| epoch 16 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1610 | bpc 1.6797
| epoch 16 | 600/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1338 | bpc 1.6395
| epoch 16 | 800/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1280 | bpc 1.6321
| epoch 16 | 1000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0976 | bpc 1.5865
| epoch 16 | 1200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1882 | bpc 1.7201
| epoch 16 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2444 | bpc 1.8006
| epoch 16 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5645 | bpc 2.2606
| epoch 16 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1825 | bpc 1.7167
| epoch 16 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1179 | bpc 1.6197
| epoch 16 | 2200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1849 | bpc 1.7142
--------------------------------------------------------------------------------
| end of epoch 16 | time: 39.01s | train loss 1.2024 | train bpc 1.7403 | valid loss 9.4056 | valid bpc 13.5694 
--------------------------------------------------------------------------------
Start Training
| epoch 17 | 200/2222 batches | lr 0.0010 | ms/batch 16.75 | loss 1.2183 | bpc 1.7656
| epoch 17 | 400/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1555 | bpc 1.6717
| epoch 17 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1296 | bpc 1.6335
| epoch 17 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1223 | bpc 1.6238
| epoch 17 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0931 | bpc 1.5801
| epoch 17 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1818 | bpc 1.7108
| epoch 17 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2411 | bpc 1.7957
| epoch 17 | 1600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.5612 | bpc 2.2558
| epoch 17 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1788 | bpc 1.7115
| epoch 17 | 2000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1153 | bpc 1.6158
| epoch 17 | 2200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1800 | bpc 1.7073
--------------------------------------------------------------------------------
| end of epoch 17 | time: 38.89s | train loss 1.1979 | train bpc 1.7338 | valid loss 8.8057 | valid bpc 12.7041 
--------------------------------------------------------------------------------
Start Training
| epoch 18 | 200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.2143 | bpc 1.7599
| epoch 18 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1508 | bpc 1.6649
| epoch 18 | 600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1251 | bpc 1.6269
| epoch 18 | 800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1183 | bpc 1.6184
| epoch 18 | 1000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0872 | bpc 1.5715
| epoch 18 | 1200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1785 | bpc 1.7062
| epoch 18 | 1400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2377 | bpc 1.7908
| epoch 18 | 1600/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.5593 | bpc 2.2531
| epoch 18 | 1800/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1718 | bpc 1.7013
| epoch 18 | 2000/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1138 | bpc 1.6138
| epoch 18 | 2200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1761 | bpc 1.7018
--------------------------------------------------------------------------------
| end of epoch 18 | time: 38.84s | train loss 1.1940 | train bpc 1.7282 | valid loss 8.3167 | valid bpc 11.9986 
--------------------------------------------------------------------------------
Start Training
| epoch 19 | 200/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2089 | bpc 1.7521
| epoch 19 | 400/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1461 | bpc 1.6583
| epoch 19 | 600/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1213 | bpc 1.6214
| epoch 19 | 800/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1135 | bpc 1.6114
| epoch 19 | 1000/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.0834 | bpc 1.5660
| epoch 19 | 1200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1745 | bpc 1.7005
| epoch 19 | 1400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.2353 | bpc 1.7875
| epoch 19 | 1600/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.5567 | bpc 2.2492
| epoch 19 | 1800/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1706 | bpc 1.6997
| epoch 19 | 2000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1083 | bpc 1.6059
| epoch 19 | 2200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1728 | bpc 1.6968
--------------------------------------------------------------------------------
| end of epoch 19 | time: 38.89s | train loss 1.1902 | train bpc 1.7227 | valid loss 8.6737 | valid bpc 12.5133 
--------------------------------------------------------------------------------
Start Training
| epoch 20 | 200/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.2054 | bpc 1.7473
| epoch 20 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1420 | bpc 1.6524
| epoch 20 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1178 | bpc 1.6167
| epoch 20 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1099 | bpc 1.6059
| epoch 20 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0787 | bpc 1.5593
| epoch 20 | 1200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1696 | bpc 1.6933
| epoch 20 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2318 | bpc 1.7824
| epoch 20 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5530 | bpc 2.2440
| epoch 20 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1662 | bpc 1.6937
| epoch 20 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1078 | bpc 1.6052
| epoch 20 | 2200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1696 | bpc 1.6922
--------------------------------------------------------------------------------
| end of epoch 20 | time: 39.02s | train loss 1.1866 | train bpc 1.7176 | valid loss 8.1043 | valid bpc 11.6921 
--------------------------------------------------------------------------------
Start Training
| epoch 21 | 200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2029 | bpc 1.7436
| epoch 21 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1396 | bpc 1.6488
| epoch 21 | 600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.1132 | bpc 1.6099
| epoch 21 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1047 | bpc 1.5985
| epoch 21 | 1000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0757 | bpc 1.5549
| epoch 21 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1667 | bpc 1.6891
| epoch 21 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2275 | bpc 1.7762
| epoch 21 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.5502 | bpc 2.2398
| epoch 21 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1639 | bpc 1.6903
| epoch 21 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1032 | bpc 1.5985
| epoch 21 | 2200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1677 | bpc 1.6898
--------------------------------------------------------------------------------
| end of epoch 21 | time: 38.91s | train loss 1.1833 | train bpc 1.7128 | valid loss 8.1197 | valid bpc 11.7148 
--------------------------------------------------------------------------------
Start Training
| epoch 22 | 200/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.2001 | bpc 1.7397
| epoch 22 | 400/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1347 | bpc 1.6418
| epoch 22 | 600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1114 | bpc 1.6074
| epoch 22 | 800/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1010 | bpc 1.5933
| epoch 22 | 1000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0720 | bpc 1.5496
| epoch 22 | 1200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1648 | bpc 1.6865
| epoch 22 | 1400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.2249 | bpc 1.7724
| epoch 22 | 1600/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.5477 | bpc 2.2362
| epoch 22 | 1800/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1603 | bpc 1.6849
| epoch 22 | 2000/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1014 | bpc 1.5959
| epoch 22 | 2200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1650 | bpc 1.6857
--------------------------------------------------------------------------------
| end of epoch 22 | time: 38.85s | train loss 1.1803 | train bpc 1.7084 | valid loss 8.3783 | valid bpc 12.0874 
--------------------------------------------------------------------------------
Start Training
| epoch 23 | 200/2222 batches | lr 0.0010 | ms/batch 16.78 | loss 1.1960 | bpc 1.7338
| epoch 23 | 400/2222 batches | lr 0.0010 | ms/batch 16.78 | loss 1.1314 | bpc 1.6371
| epoch 23 | 600/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1078 | bpc 1.6021
| epoch 23 | 800/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0990 | bpc 1.5903
| epoch 23 | 1000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0684 | bpc 1.5443
| epoch 23 | 1200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1610 | bpc 1.6811
| epoch 23 | 1400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.2211 | bpc 1.7670
| epoch 23 | 1600/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.5470 | bpc 2.2353
| epoch 23 | 1800/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1580 | bpc 1.6818
| epoch 23 | 2000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0983 | bpc 1.5913
| epoch 23 | 2200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1621 | bpc 1.6814
--------------------------------------------------------------------------------
| end of epoch 23 | time: 38.86s | train loss 1.1774 | train bpc 1.7042 | valid loss 9.0235 | valid bpc 13.0181 
--------------------------------------------------------------------------------
Start Training
| epoch 24 | 200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1950 | bpc 1.7322
| epoch 24 | 400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1285 | bpc 1.6330
| epoch 24 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1038 | bpc 1.5962
| epoch 24 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0957 | bpc 1.5855
| epoch 24 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0670 | bpc 1.5425
| epoch 24 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1576 | bpc 1.6760
| epoch 24 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2204 | bpc 1.7662
| epoch 24 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.5428 | bpc 2.2292
| epoch 24 | 1800/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1574 | bpc 1.6809
| epoch 24 | 2000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0972 | bpc 1.5897
| epoch 24 | 2200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1590 | bpc 1.6773
--------------------------------------------------------------------------------
| end of epoch 24 | time: 38.93s | train loss 1.1751 | train bpc 1.7009 | valid loss 8.3103 | valid bpc 11.9894 
--------------------------------------------------------------------------------
Start Training
| epoch 25 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1923 | bpc 1.7286
| epoch 25 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1269 | bpc 1.6306
| epoch 25 | 600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.1021 | bpc 1.5938
| epoch 25 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.0925 | bpc 1.5812
| epoch 25 | 1000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0627 | bpc 1.5362
| epoch 25 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1565 | bpc 1.6745
| epoch 25 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2184 | bpc 1.7632
| epoch 25 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.5404 | bpc 2.2258
| epoch 25 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1543 | bpc 1.6764
| epoch 25 | 2000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0945 | bpc 1.5860
| epoch 25 | 2200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1580 | bpc 1.6757
--------------------------------------------------------------------------------
| end of epoch 25 | time: 38.95s | train loss 1.1727 | train bpc 1.6975 | valid loss 8.2807 | valid bpc 11.9469 
--------------------------------------------------------------------------------
Start Training
| epoch 26 | 200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1893 | bpc 1.7241
| epoch 26 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1246 | bpc 1.6272
| epoch 26 | 600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0995 | bpc 1.5901
| epoch 26 | 800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0912 | bpc 1.5791
| epoch 26 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0601 | bpc 1.5324
| epoch 26 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1537 | bpc 1.6706
| epoch 26 | 1400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2156 | bpc 1.7590
| epoch 26 | 1600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.5396 | bpc 2.2246
| epoch 26 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1516 | bpc 1.6725
| epoch 26 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0916 | bpc 1.5817
| epoch 26 | 2200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1573 | bpc 1.6749
--------------------------------------------------------------------------------
| end of epoch 26 | time: 38.90s | train loss 1.1705 | train bpc 1.6943 | valid loss 8.4384 | valid bpc 12.1743 
--------------------------------------------------------------------------------
Start Training
| epoch 27 | 200/2222 batches | lr 0.0010 | ms/batch 16.78 | loss 1.1879 | bpc 1.7221
| epoch 27 | 400/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1209 | bpc 1.6218
| epoch 27 | 600/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0973 | bpc 1.5869
| epoch 27 | 800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0876 | bpc 1.5740
| epoch 27 | 1000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0576 | bpc 1.5290
| epoch 27 | 1200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1520 | bpc 1.6682
| epoch 27 | 1400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2139 | bpc 1.7568
| epoch 27 | 1600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.5365 | bpc 2.2201
| epoch 27 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1502 | bpc 1.6704
| epoch 27 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0914 | bpc 1.5817
| epoch 27 | 2200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1550 | bpc 1.6715
--------------------------------------------------------------------------------
| end of epoch 27 | time: 38.90s | train loss 1.1684 | train bpc 1.6914 | valid loss 8.5133 | valid bpc 12.2824 
--------------------------------------------------------------------------------
Start Training
| epoch 28 | 200/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.1853 | bpc 1.7184
| epoch 28 | 400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1188 | bpc 1.6189
| epoch 28 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0948 | bpc 1.5834
| epoch 28 | 800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0850 | bpc 1.5704
| epoch 28 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0561 | bpc 1.5266
| epoch 28 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1492 | bpc 1.6641
| epoch 28 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.2131 | bpc 1.7556
| epoch 28 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5338 | bpc 2.2164
| epoch 28 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1483 | bpc 1.6678
| epoch 28 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0906 | bpc 1.5805
| epoch 28 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1523 | bpc 1.6674
--------------------------------------------------------------------------------
| end of epoch 28 | time: 38.95s | train loss 1.1663 | train bpc 1.6883 | valid loss 8.3818 | valid bpc 12.0928 
--------------------------------------------------------------------------------
Start Training
| epoch 29 | 200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1839 | bpc 1.7165
| epoch 29 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1184 | bpc 1.6183
| epoch 29 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0941 | bpc 1.5825
| epoch 29 | 800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0819 | bpc 1.5659
| epoch 29 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0549 | bpc 1.5249
| epoch 29 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1476 | bpc 1.6617
| epoch 29 | 1400/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.2106 | bpc 1.7522
| epoch 29 | 1600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.5330 | bpc 2.2152
| epoch 29 | 1800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1464 | bpc 1.6650
| epoch 29 | 2000/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.0880 | bpc 1.5767
| epoch 29 | 2200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1527 | bpc 1.6683
--------------------------------------------------------------------------------
| end of epoch 29 | time: 39.02s | train loss 1.1649 | train bpc 1.6864 | valid loss 8.7576 | valid bpc 12.6352 
--------------------------------------------------------------------------------
Start Training
| epoch 30 | 200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1820 | bpc 1.7140
| epoch 30 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1145 | bpc 1.6127
| epoch 30 | 600/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 1.0913 | bpc 1.5785
| epoch 30 | 800/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0798 | bpc 1.5629
| epoch 30 | 1000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0537 | bpc 1.5234
| epoch 30 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1460 | bpc 1.6597
| epoch 30 | 1400/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.2095 | bpc 1.7505
| epoch 30 | 1600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.5337 | bpc 2.2160
| epoch 30 | 1800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1448 | bpc 1.6631
| epoch 30 | 2000/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0860 | bpc 1.5735
| epoch 30 | 2200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1493 | bpc 1.6633
--------------------------------------------------------------------------------
| end of epoch 30 | time: 38.99s | train loss 1.1629 | train bpc 1.6836 | valid loss 8.6265 | valid bpc 12.4458 
--------------------------------------------------------------------------------
Start Training
| epoch 31 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1807 | bpc 1.7119
| epoch 31 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1130 | bpc 1.6105
| epoch 31 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0897 | bpc 1.5760
| epoch 31 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0785 | bpc 1.5611
| epoch 31 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0506 | bpc 1.5188
| epoch 31 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1428 | bpc 1.6548
| epoch 31 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.2082 | bpc 1.7488
| epoch 31 | 1600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.5314 | bpc 2.2130
| epoch 31 | 1800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1422 | bpc 1.6590
| epoch 31 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0842 | bpc 1.5713
| epoch 31 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1499 | bpc 1.6645
--------------------------------------------------------------------------------
| end of epoch 31 | time: 38.98s | train loss 1.1611 | train bpc 1.6810 | valid loss 8.3475 | valid bpc 12.0432 
--------------------------------------------------------------------------------
Start Training
| epoch 32 | 200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1789 | bpc 1.7095
| epoch 32 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1119 | bpc 1.6091
| epoch 32 | 600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0877 | bpc 1.5733
| epoch 32 | 800/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0785 | bpc 1.5610
| epoch 32 | 1000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0478 | bpc 1.5149
| epoch 32 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1404 | bpc 1.6513
| epoch 32 | 1400/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.2076 | bpc 1.7479
| epoch 32 | 1600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.5301 | bpc 2.2109
| epoch 32 | 1800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1410 | bpc 1.6574
| epoch 32 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0850 | bpc 1.5726
| epoch 32 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1468 | bpc 1.6595
--------------------------------------------------------------------------------
| end of epoch 32 | time: 38.98s | train loss 1.1598 | train bpc 1.6790 | valid loss 8.4895 | valid bpc 12.2484 
--------------------------------------------------------------------------------
Start Training
| epoch 33 | 200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1777 | bpc 1.7077
| epoch 33 | 400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1085 | bpc 1.6043
| epoch 33 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0852 | bpc 1.5694
| epoch 33 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0764 | bpc 1.5583
| epoch 33 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0466 | bpc 1.5131
| epoch 33 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1389 | bpc 1.6494
| epoch 33 | 1400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.2050 | bpc 1.7441
| epoch 33 | 1600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.5286 | bpc 2.2087
| epoch 33 | 1800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1405 | bpc 1.6567
| epoch 33 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0832 | bpc 1.5697
| epoch 33 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1464 | bpc 1.6592
--------------------------------------------------------------------------------
| end of epoch 33 | time: 38.96s | train loss 1.1580 | train bpc 1.6764 | valid loss 8.7462 | valid bpc 12.6186 
--------------------------------------------------------------------------------
Start Training
| epoch 34 | 200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1762 | bpc 1.7055
| epoch 34 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1085 | bpc 1.6042
| epoch 34 | 600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0835 | bpc 1.5670
| epoch 34 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0748 | bpc 1.5557
| epoch 34 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0436 | bpc 1.5087
| epoch 34 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1369 | bpc 1.6465
| epoch 34 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2050 | bpc 1.7438
| epoch 34 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5258 | bpc 2.2047
| epoch 34 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1394 | bpc 1.6553
| epoch 34 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0814 | bpc 1.5671
| epoch 34 | 2200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1455 | bpc 1.6577
--------------------------------------------------------------------------------
| end of epoch 34 | time: 38.91s | train loss 1.1565 | train bpc 1.6743 | valid loss 8.2821 | valid bpc 11.9492 
--------------------------------------------------------------------------------
Start Training
| epoch 35 | 200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1746 | bpc 1.7033
| epoch 35 | 400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1057 | bpc 1.6001
| epoch 35 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0833 | bpc 1.5668
| epoch 35 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0715 | bpc 1.5511
| epoch 35 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0441 | bpc 1.5095
| epoch 35 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1378 | bpc 1.6477
| epoch 35 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2002 | bpc 1.7371
| epoch 35 | 1600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.5244 | bpc 2.2028
| epoch 35 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1390 | bpc 1.6545
| epoch 35 | 2000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0799 | bpc 1.5650
| epoch 35 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1442 | bpc 1.6559
--------------------------------------------------------------------------------
| end of epoch 35 | time: 38.95s | train loss 1.1552 | train bpc 1.6724 | valid loss 8.2220 | valid bpc 11.8624 
--------------------------------------------------------------------------------
Start Training
| epoch 36 | 200/2222 batches | lr 0.0010 | ms/batch 16.78 | loss 1.1718 | bpc 1.6992
| epoch 36 | 400/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.1047 | bpc 1.5988
| epoch 36 | 600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0805 | bpc 1.5627
| epoch 36 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0712 | bpc 1.5504
| epoch 36 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0412 | bpc 1.5052
| epoch 36 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1351 | bpc 1.6440
| epoch 36 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.2015 | bpc 1.7390
| epoch 36 | 1600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.5234 | bpc 2.2013
| epoch 36 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1366 | bpc 1.6513
| epoch 36 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0783 | bpc 1.5626
| epoch 36 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1429 | bpc 1.6540
--------------------------------------------------------------------------------
| end of epoch 36 | time: 38.98s | train loss 1.1535 | train bpc 1.6700 | valid loss 7.8099 | valid bpc 11.2681 
--------------------------------------------------------------------------------
Start Training
| epoch 37 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1724 | bpc 1.7001
| epoch 37 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1025 | bpc 1.5956
| epoch 37 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0798 | bpc 1.5619
| epoch 37 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0685 | bpc 1.5466
| epoch 37 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0411 | bpc 1.5051
| epoch 37 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1337 | bpc 1.6420
| epoch 37 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2015 | bpc 1.7392
| epoch 37 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5223 | bpc 2.1997
| epoch 37 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1353 | bpc 1.6494
| epoch 37 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0776 | bpc 1.5617
| epoch 37 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1422 | bpc 1.6529
--------------------------------------------------------------------------------
| end of epoch 37 | time: 38.96s | train loss 1.1525 | train bpc 1.6686 | valid loss 7.9600 | valid bpc 11.4843 
--------------------------------------------------------------------------------
Start Training
| epoch 38 | 200/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1716 | bpc 1.6989
| epoch 38 | 400/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.1022 | bpc 1.5953
| epoch 38 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0772 | bpc 1.5582
| epoch 38 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0685 | bpc 1.5466
| epoch 38 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0395 | bpc 1.5028
| epoch 38 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1324 | bpc 1.6402
| epoch 38 | 1400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1995 | bpc 1.7361
| epoch 38 | 1600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.5215 | bpc 2.1987
| epoch 38 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1344 | bpc 1.6482
| epoch 38 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0766 | bpc 1.5606
| epoch 38 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1415 | bpc 1.6520
--------------------------------------------------------------------------------
| end of epoch 38 | time: 39.10s | train loss 1.1514 | train bpc 1.6671 | valid loss 8.4481 | valid bpc 12.1887 
--------------------------------------------------------------------------------
Start Training
| epoch 39 | 200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1708 | bpc 1.6978
| epoch 39 | 400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0986 | bpc 1.5901
| epoch 39 | 600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0781 | bpc 1.5594
| epoch 39 | 800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0662 | bpc 1.5433
| epoch 39 | 1000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0370 | bpc 1.4993
| epoch 39 | 1200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1311 | bpc 1.6384
| epoch 39 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1982 | bpc 1.7343
| epoch 39 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.5189 | bpc 2.1950
| epoch 39 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1326 | bpc 1.6458
| epoch 39 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0765 | bpc 1.5601
| epoch 39 | 2200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1397 | bpc 1.6495
--------------------------------------------------------------------------------
| end of epoch 39 | time: 38.90s | train loss 1.1500 | train bpc 1.6650 | valid loss 8.1316 | valid bpc 11.7320 
--------------------------------------------------------------------------------
Start Training
| epoch 40 | 200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.1684 | bpc 1.6947
| epoch 40 | 400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0987 | bpc 1.5903
| epoch 40 | 600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.0755 | bpc 1.5557
| epoch 40 | 800/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.0663 | bpc 1.5437
| epoch 40 | 1000/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.0353 | bpc 1.4971
| epoch 40 | 1200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1310 | bpc 1.6382
| epoch 40 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1960 | bpc 1.7312
| epoch 40 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5189 | bpc 2.1949
| epoch 40 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1317 | bpc 1.6440
| epoch 40 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0747 | bpc 1.5577
| epoch 40 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1389 | bpc 1.6483
--------------------------------------------------------------------------------
| end of epoch 40 | time: 38.94s | train loss 1.1488 | train bpc 1.6633 | valid loss 8.5638 | valid bpc 12.3552 
--------------------------------------------------------------------------------
Start Training
| epoch 41 | 200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1687 | bpc 1.6950
| epoch 41 | 400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0968 | bpc 1.5876
| epoch 41 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0743 | bpc 1.5539
| epoch 41 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0644 | bpc 1.5409
| epoch 41 | 1000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0356 | bpc 1.4973
| epoch 41 | 1200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1285 | bpc 1.6349
| epoch 41 | 1400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1955 | bpc 1.7305
| epoch 41 | 1600/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.5183 | bpc 2.1939
| epoch 41 | 1800/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1302 | bpc 1.6423
| epoch 41 | 2000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0747 | bpc 1.5578
| epoch 41 | 2200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1372 | bpc 1.6458
--------------------------------------------------------------------------------
| end of epoch 41 | time: 38.88s | train loss 1.1478 | train bpc 1.6620 | valid loss 8.0901 | valid bpc 11.6720 
--------------------------------------------------------------------------------
Start Training
| epoch 42 | 200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1663 | bpc 1.6915
| epoch 42 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0970 | bpc 1.5878
| epoch 42 | 600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0731 | bpc 1.5522
| epoch 42 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0644 | bpc 1.5408
| epoch 42 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0333 | bpc 1.4940
| epoch 42 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1287 | bpc 1.6348
| epoch 42 | 1400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1955 | bpc 1.7307
| epoch 42 | 1600/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.5158 | bpc 2.1904
| epoch 42 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1306 | bpc 1.6430
| epoch 42 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0715 | bpc 1.5530
| epoch 42 | 2200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1379 | bpc 1.6469
--------------------------------------------------------------------------------
| end of epoch 42 | time: 38.95s | train loss 1.1469 | train bpc 1.6606 | valid loss 8.3320 | valid bpc 12.0213 
--------------------------------------------------------------------------------
Start Training
| epoch 43 | 200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1657 | bpc 1.6906
| epoch 43 | 400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0951 | bpc 1.5852
| epoch 43 | 600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.0717 | bpc 1.5502
| epoch 43 | 800/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 1.0614 | bpc 1.5366
| epoch 43 | 1000/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.0333 | bpc 1.4939
| epoch 43 | 1200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1259 | bpc 1.6308
| epoch 43 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1947 | bpc 1.7295
| epoch 43 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5158 | bpc 2.1904
| epoch 43 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1291 | bpc 1.6406
| epoch 43 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0713 | bpc 1.5530
| epoch 43 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1356 | bpc 1.6434
--------------------------------------------------------------------------------
| end of epoch 43 | time: 38.96s | train loss 1.1456 | train bpc 1.6588 | valid loss 8.0300 | valid bpc 11.5855 
--------------------------------------------------------------------------------
Start Training
| epoch 44 | 200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1638 | bpc 1.6878
| epoch 44 | 400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0940 | bpc 1.5837
| epoch 44 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0718 | bpc 1.5502
| epoch 44 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0592 | bpc 1.5335
| epoch 44 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0323 | bpc 1.4925
| epoch 44 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1257 | bpc 1.6304
| epoch 44 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1944 | bpc 1.7292
| epoch 44 | 1600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.5141 | bpc 2.1879
| epoch 44 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1282 | bpc 1.6394
| epoch 44 | 2000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0717 | bpc 1.5534
| epoch 44 | 2200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1343 | bpc 1.6417
--------------------------------------------------------------------------------
| end of epoch 44 | time: 38.83s | train loss 1.1446 | train bpc 1.6574 | valid loss 7.9769 | valid bpc 11.5087 
--------------------------------------------------------------------------------
Start Training
| epoch 45 | 200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1632 | bpc 1.6871
| epoch 45 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0932 | bpc 1.5822
| epoch 45 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0698 | bpc 1.5475
| epoch 45 | 800/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0588 | bpc 1.5327
| epoch 45 | 1000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0309 | bpc 1.4905
| epoch 45 | 1200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1243 | bpc 1.6285
| epoch 45 | 1400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1919 | bpc 1.7253
| epoch 45 | 1600/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.5143 | bpc 2.1883
| epoch 45 | 1800/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1267 | bpc 1.6369
| epoch 45 | 2000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0702 | bpc 1.5511
| epoch 45 | 2200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1354 | bpc 1.6433
--------------------------------------------------------------------------------
| end of epoch 45 | time: 38.86s | train loss 1.1436 | train bpc 1.6558 | valid loss 8.4064 | valid bpc 12.1283 
--------------------------------------------------------------------------------
Start Training
| epoch 46 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1640 | bpc 1.6881
| epoch 46 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0909 | bpc 1.5788
| epoch 46 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0693 | bpc 1.5469
| epoch 46 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0579 | bpc 1.5315
| epoch 46 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0297 | bpc 1.4889
| epoch 46 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1239 | bpc 1.6279
| epoch 46 | 1400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1896 | bpc 1.7220
| epoch 46 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.5143 | bpc 2.1882
| epoch 46 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1274 | bpc 1.6380
| epoch 46 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0694 | bpc 1.5502
| epoch 46 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1351 | bpc 1.6427
--------------------------------------------------------------------------------
| end of epoch 46 | time: 38.94s | train loss 1.1430 | train bpc 1.6550 | valid loss 8.4989 | valid bpc 12.2621 
--------------------------------------------------------------------------------
Start Training
| epoch 47 | 200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1614 | bpc 1.6845
| epoch 47 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0918 | bpc 1.5803
| epoch 47 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0696 | bpc 1.5469
| epoch 47 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0557 | bpc 1.5286
| epoch 47 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0284 | bpc 1.4868
| epoch 47 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1249 | bpc 1.6294
| epoch 47 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1909 | bpc 1.7240
| epoch 47 | 1600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.5116 | bpc 2.1844
| epoch 47 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1266 | bpc 1.6370
| epoch 47 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0683 | bpc 1.5487
| epoch 47 | 2200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1331 | bpc 1.6400
--------------------------------------------------------------------------------
| end of epoch 47 | time: 38.98s | train loss 1.1421 | train bpc 1.6538 | valid loss 8.4787 | valid bpc 12.2327 
--------------------------------------------------------------------------------
Start Training
| epoch 48 | 200/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1604 | bpc 1.6830
| epoch 48 | 400/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.0892 | bpc 1.5765
| epoch 48 | 600/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0675 | bpc 1.5443
| epoch 48 | 800/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0556 | bpc 1.5281
| epoch 48 | 1000/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.0279 | bpc 1.4862
| epoch 48 | 1200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1227 | bpc 1.6262
| epoch 48 | 1400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1898 | bpc 1.7225
| epoch 48 | 1600/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.5120 | bpc 2.1851
| epoch 48 | 1800/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1255 | bpc 1.6356
| epoch 48 | 2000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0689 | bpc 1.5490
| epoch 48 | 2200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1326 | bpc 1.6395
--------------------------------------------------------------------------------
| end of epoch 48 | time: 38.85s | train loss 1.1413 | train bpc 1.6526 | valid loss 8.3533 | valid bpc 12.0522 
--------------------------------------------------------------------------------
Start Training
| epoch 49 | 200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1605 | bpc 1.6835
| epoch 49 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.0888 | bpc 1.5761
| epoch 49 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0673 | bpc 1.5439
| epoch 49 | 800/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0543 | bpc 1.5265
| epoch 49 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0276 | bpc 1.4858
| epoch 49 | 1200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1223 | bpc 1.6259
| epoch 49 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1893 | bpc 1.7217
| epoch 49 | 1600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.5118 | bpc 2.1848
| epoch 49 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1232 | bpc 1.6323
| epoch 49 | 2000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0678 | bpc 1.5480
| epoch 49 | 2200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1327 | bpc 1.6393
--------------------------------------------------------------------------------
| end of epoch 49 | time: 38.87s | train loss 1.1407 | train bpc 1.6518 | valid loss 8.5604 | valid bpc 12.3507 
--------------------------------------------------------------------------------
Start Training
| epoch 50 | 200/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.1588 | bpc 1.6809
| epoch 50 | 400/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.0880 | bpc 1.5750
| epoch 50 | 600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0638 | bpc 1.5388
| epoch 50 | 800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0550 | bpc 1.5273
| epoch 50 | 1000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0258 | bpc 1.4834
| epoch 50 | 1200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1198 | bpc 1.6221
| epoch 50 | 1400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1887 | bpc 1.7209
| epoch 50 | 1600/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.5107 | bpc 2.1830
| epoch 50 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1238 | bpc 1.6330
| epoch 50 | 2000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0659 | bpc 1.5452
| epoch 50 | 2200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1315 | bpc 1.6376
--------------------------------------------------------------------------------
| end of epoch 50 | time: 38.92s | train loss 1.1394 | train bpc 1.6499 | valid loss 8.7622 | valid bpc 12.6419 
--------------------------------------------------------------------------------
Run history:

epoch	
lr	
train_bpc	
train_loss	
val_bpc	
val_loss	

Run summary:

epoch	50
lr	0.001
train_bpc	1.64993
train_loss	1.13942
val_bpc	12.64189
val_loss	8.7622

View run summer-eon-26 at: https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/58tc1msa
View project at: https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
Find logs at: ./wandb/run-20240825_132733-58tc1msa/logs
Start Test Validation
Evaluating batch 1/124
Evaluating batch 2/124
Evaluating batch 3/124
Evaluating batch 4/124
Evaluating batch 5/124
Evaluating batch 6/124
Evaluating batch 7/124
Evaluating batch 8/124
Evaluating batch 9/124
Evaluating batch 10/124
Evaluating batch 11/124
Evaluating batch 12/124
Evaluating batch 13/124
Evaluating batch 14/124
Evaluating batch 15/124
Evaluating batch 16/124
Evaluating batch 17/124
Evaluating batch 18/124
Evaluating batch 19/124
Evaluating batch 20/124
Evaluating batch 21/124
Evaluating batch 22/124
Evaluating batch 23/124
Evaluating batch 24/124
Evaluating batch 25/124
Evaluating batch 26/124
Evaluating batch 27/124
Evaluating batch 28/124
Evaluating batch 29/124
Evaluating batch 30/124
Evaluating batch 31/124
Evaluating batch 32/124
Evaluating batch 33/124
Evaluating batch 34/124
Evaluating batch 35/124
Evaluating batch 36/124
Evaluating batch 37/124
Evaluating batch 38/124
Evaluating batch 39/124
Evaluating batch 40/124
Evaluating batch 41/124
Evaluating batch 42/124
Evaluating batch 43/124
Evaluating batch 44/124
Evaluating batch 45/124
Evaluating batch 46/124
Evaluating batch 47/124
Evaluating batch 48/124
Evaluating batch 49/124
Evaluating batch 50/124
Evaluating batch 51/124
Evaluating batch 52/124
Evaluating batch 53/124
Evaluating batch 54/124
Evaluating batch 55/124
Evaluating batch 56/124
Evaluating batch 57/124
Evaluating batch 58/124
Evaluating batch 59/124
Evaluating batch 60/124
Evaluating batch 61/124
Evaluating batch 62/124
Evaluating batch 63/124
Evaluating batch 64/124
Evaluating batch 65/124
Evaluating batch 66/124
Evaluating batch 67/124
Evaluating batch 68/124
Evaluating batch 69/124
Evaluating batch 70/124
Evaluating batch 71/124
Evaluating batch 72/124
Evaluating batch 73/124
Evaluating batch 74/124
Evaluating batch 75/124
Evaluating batch 76/124
Evaluating batch 77/124
Evaluating batch 78/124
Evaluating batch 79/124
Evaluating batch 80/124
Evaluating batch 81/124
Evaluating batch 82/124
Evaluating batch 83/124
Evaluating batch 84/124
Evaluating batch 85/124
Evaluating batch 86/124
Evaluating batch 87/124
Evaluating batch 88/124
Evaluating batch 89/124
Evaluating batch 90/124
Evaluating batch 91/124
Evaluating batch 92/124
Evaluating batch 93/124
Evaluating batch 94/124
Evaluating batch 95/124
Evaluating batch 96/124
Evaluating batch 97/124
Evaluating batch 98/124
Evaluating batch 99/124
Evaluating batch 100/124
Evaluating batch 101/124
Evaluating batch 102/124
Evaluating batch 103/124
Evaluating batch 104/124
Evaluating batch 105/124
Evaluating batch 106/124
Evaluating batch 107/124
Evaluating batch 108/124
Evaluating batch 109/124
Evaluating batch 110/124
Evaluating batch 111/124
Evaluating batch 112/124
Evaluating batch 113/124
Evaluating batch 114/124
Evaluating batch 115/124
Evaluating batch 116/124
Evaluating batch 117/124
Evaluating batch 118/124
Evaluating batch 119/124
Evaluating batch 120/124
Evaluating batch 121/124
Evaluating batch 122/124
Evaluating batch 123/124
Evaluating batch 124/124
================================================================================
| End of training | test loss 8.19334888458252 | test bpc 11.820731163024902 
================================================================================
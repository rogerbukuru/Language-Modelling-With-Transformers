{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M95I8Uv2P4iX",
    "outputId": "f9370db4-1572-42e3-fe50-5f3b4db5fbbd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: livelossplot in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.7.1)\n",
      "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.4.3)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (3.1.4)\n",
      "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.26.4)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (24.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2.1.4)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (9.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.0.2)\n",
      "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.3.3)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2024.6.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.13.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "a GPU is connected.\n",
      "gpu\n"
     ]
    }
   ],
   "source": [
    "!pip install livelossplot\n",
    "!pip install wandb\n",
    "\n",
    "import os\n",
    "import math\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import functools\n",
    "import optax\n",
    "import itertools\n",
    "import random\n",
    "import time\n",
    "import wandb\n",
    "import optax\n",
    "\n",
    "\n",
    "from jax import grad, jit, vmap\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "\n",
    "if os.environ[\"COLAB_GPU\"] and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
    "    print(\"a GPU is connected.\")\n",
    "else:\n",
    "    print(\"Only CPU accelerator is connected.\")\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
    "\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LlSNh_Q8YzHC"
   },
   "outputs": [],
   "source": [
    "class SequenceToQKV(nn.Module):\n",
    "  output_size: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, X):\n",
    "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
    "\n",
    "    q_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
    "    k_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
    "    v_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
    "\n",
    "    Q = q_layer(X)\n",
    "    K = k_layer(X)\n",
    "    V = v_layer(X)\n",
    "\n",
    "    return Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DGnfegRIV35G"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "  d_m: int\n",
    "  num_heads: int\n",
    "\n",
    "  def setup(self):\n",
    "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
    "    self.d_k  = self.d_m // self.num_heads\n",
    "\n",
    "    self.W_q = nn.Dense(self.d_m, kernel_init=initializer)\n",
    "    self.W_k = nn.Dense(self.d_m, kernel_init=initializer)\n",
    "    self.W_v = nn.Dense(self.d_m, kernel_init=initializer)\n",
    "    self.Wo  = nn.Dense(self.d_m, kernel_init=initializer)\n",
    "    self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
    "\n",
    "  def scaled_dot_product_attention(self,query, key, value, mask=None):\n",
    "    d_k = key.shape[-1]\n",
    "    T_k = key.shape[-2]\n",
    "    T_q = query.shape[-2]\n",
    "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
    "    scaled_logits = logits/jnp.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "      scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
    "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
    "    attention = jnp.matmul(attention_weights, value)\n",
    "    return attention, attention_weights\n",
    "\n",
    "  def __call__(self, X, mask=None):\n",
    "\n",
    "    # get the batch size, sequence length and embedding size\n",
    "    Q, K, V   = self.sequence_to_qkv(X)\n",
    "    B, T, d_m = K.shape\n",
    "    assert d_m == self.d_m\n",
    "\n",
    "\n",
    "    q_heads = Q.reshape(B, -1, self.num_heads, self.d_k).swapaxes(1,2)\n",
    "    k_heads = K.reshape(B, -1, self.num_heads, self.d_k).swapaxes(1,2)\n",
    "    v_heads = V.reshape(B, -1, self.num_heads, self.d_k).swapaxes(1,2)\n",
    "\n",
    "    # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, d_m) - re-assemble all head outputs\n",
    "    attention, attention_weights = self.scaled_dot_product_attention(\n",
    "        q_heads, k_heads, v_heads, mask\n",
    "        )\n",
    "\n",
    "    attention = attention.swapaxes(1,2).reshape(B, -1, d_m)\n",
    "    X_new = self.Wo(attention)\n",
    "    return X_new, attention_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EhunsHNvWEKC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UJYeRDkihwTe"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    d_m: int\n",
    "    max_seq_length: int\n",
    "\n",
    "    def setup(self):\n",
    "        pe = jnp.zeros((self.max_seq_length, d_m))\n",
    "        positions = jnp.arange(0, self.max_seq_length)[: jnp.newaxis]\n",
    "\n",
    "        i = jnp.arange(0, d_m, 2)\n",
    "        div_term = jnp.exp(i * (-math.log(10000.0) / d_m))\n",
    "        frequencies = positions * div_term\n",
    "\n",
    "        pe = pe.at[:, 0::2].set(jnp.sin(frequencies))\n",
    "        pe = pe.at[:, 1::2].set(jnp.cos(frequencies))\n",
    "        self.pe  = pe\n",
    "\n",
    "    def __call__(self, x):\n",
    "      return x + self.pe[:x.shape[0], :]\n",
    "\n",
    "def positionalEncoding(d_m,sequence_length):\n",
    "      assert d_m % 2 == 0, \"token_embedding should be divisible by two\"\n",
    "\n",
    "      pe = jnp.zeros((sequence_length, d_m))\n",
    "      positions = jnp.arange(0, sequence_length)[:, jnp.newaxis]\n",
    "\n",
    "      i = jnp.arange(0, d_m, 2)\n",
    "      div_term = jnp.exp(i * (-math.log(10000.0) / d_m))\n",
    "      frequencies = positions * div_term\n",
    "\n",
    "      pe = pe.at[:, 0::2].set(jnp.sin(frequencies))\n",
    "      pe = pe.at[:, 1::2].set(jnp.cos(frequencies))\n",
    "      return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pHHDQuCTlsI9"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  \"\"\"\n",
    "  A 2-layer MLP which widens then narrows the input.\n",
    "\n",
    "  Args:\n",
    "    widening_factor [optional, default=4]: The size of the hidden layer will be d_model * widening_factor.\n",
    "  \"\"\"\n",
    "\n",
    "  widening_factor: int = 4\n",
    "  init_scale: float = 0.25\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    '''\n",
    "    Args:\n",
    "      x: [B, T, d_m]\n",
    "\n",
    "    Return:\n",
    "      x: [B, T, d_m]\n",
    "    '''\n",
    "    d_m = x.shape[-1]\n",
    "    layer1_size = self.widening_factor * d_m\n",
    "\n",
    "    initializer = nn.initializers.variance_scaling(\n",
    "        scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
    "    )\n",
    "    layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
    "    layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
    "\n",
    "    x = jax.nn.gelu(layer1(x))\n",
    "    x = layer2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8lw0xw1nneNX"
   },
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, processed_x):\n",
    "    '''\n",
    "    Args:\n",
    "      x: Sequence of tokens before feeding into MHA or FF blocks, with shape [B, T, d_m]\n",
    "      x: Sequence of after being processed by MHA or FF blocks, with shape [B, T, d_m]\n",
    "\n",
    "    Return:\n",
    "      add_norm_x: Transformed tokens with shape [B, T, d_m]\n",
    "    '''\n",
    "\n",
    "    added = x + processed_x\n",
    "    normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
    "    return normalised(added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BiYFxJUFnm4Y"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "  d_m: int\n",
    "  num_heads: int\n",
    "  ff_widening_factor: int\n",
    "  dropout_rate: float\n",
    "  training: bool\n",
    "  pre_norm: bool\n",
    "\n",
    "  def setup(self):\n",
    "    self.mha = MultiHeadAttention(d_m = self.d_m, num_heads = self.num_heads)\n",
    "    self.add_norm1 = AddNorm()\n",
    "    self.ff = FeedForward(widening_factor=self.ff_widening_factor)\n",
    "    self.add_norm2 = AddNorm()\n",
    "    self.dropout = nn.Dropout(rate=self.dropout_rate)\n",
    "\n",
    "  def __call__(self, X, mask=None, key=None):\n",
    "\n",
    "      x_output = None\n",
    "      attention_weights = None\n",
    "\n",
    "      if not self.pre_norm:\n",
    "        attention_output, attention_weights = self.mha(X=X,mask=mask)\n",
    "        add_norm1_output = self.add_norm1(X, self.dropout(attention_output, deterministic=not self.training, rng=key))\n",
    "        ff_output = self.ff(add_norm1_output)\n",
    "        add_norm2_output = self.add_norm2(add_norm1_output, self.dropout(ff_output, deterministic=not self.training, rng=key))\n",
    "        x_output = add_norm2_output\n",
    "      else:\n",
    "        X = self.add_norm1(X,X)\n",
    "        attention_output, attention_weights = self.mha(X=X,mask=mask)\n",
    "        add_norm2_output = self.add_norm2(X, self.dropout(attention_output, deterministic=not self.training, rng=key))\n",
    "        ff_output = self.ff(add_norm2_output)\n",
    "        x_output = self.dropout(ff_output, deterministic=not self.training, rng=key)\n",
    "\n",
    "      return x_output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dtlY9erPq5v9"
   },
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "\n",
    "\n",
    "    num_heads:int\n",
    "    num_layers:int\n",
    "    d_m:int\n",
    "    ff_widening_factor:int\n",
    "    vocab_size: int\n",
    "    dropout_rate: float\n",
    "    training: bool\n",
    "    pre_norm: bool\n",
    "    tie_weights: bool\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m) # convert tokens to embeddings\n",
    "        self.decoder_blocks = [DecoderBlock(self.d_m, self.num_heads, self.ff_widening_factor, self.dropout_rate, self.training, self.pre_norm) for _ in range(self.num_layers)]\n",
    "        self.pred_layer = nn.Dense(self.vocab_size)\n",
    "        self.dropout = nn.Dropout(rate=self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, mask=None, key=None):\n",
    "        x = self.embedding(x)\n",
    "        sequence_len = x.shape[-2]\n",
    "        positions = positionalEncoding(self.d_m,sequence_len)\n",
    "        x = x + positions\n",
    "        attention_weights = []\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            key, subkey = jax.random.split(key)\n",
    "            out, attention_weight = decoder_block(x, mask, key=subkey)\n",
    "            out = self.dropout(out, deterministic=not self.training, rng=key)\n",
    "            x = out\n",
    "            attention_weights.append(attention_weight)\n",
    "\n",
    "        logits = None\n",
    "        if self.tie_weights:\n",
    "            logits = nn.log_softmax(x @ self.embedding.embedding.T)\n",
    "        else:\n",
    "          logits = nn.log_softmax(self.pred_layer(x))\n",
    "\n",
    "        return logits, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "0ArrXAw4u4mu"
   },
   "outputs": [],
   "source": [
    "B, T, d_m, N, vocab_size = 18, 32, 16, 8, 25670\n",
    "\n",
    "lm = LM(num_heads=1, num_layers=1, d_m=d_m, vocab_size=vocab_size, ff_widening_factor=4,\n",
    "        dropout_rate=0.1, training=True,\n",
    "        pre_norm=False,\n",
    "        tie_weights=False\n",
    "        )\n",
    "key = jax.random.PRNGKey(90)\n",
    "mask = jnp.tril(np.ones((T, T)))\n",
    "\n",
    "# initialise module and get dummy output\n",
    "key = jax.random.PRNGKey(90)\n",
    "X = jax.random.randint(key, [B, T], 0, vocab_size)\n",
    "params = lm.init(key, X, mask=mask, key=key)\n",
    "\n",
    "# extract output from decoder\n",
    "logits, decoder_att_weights = lm.apply(\n",
    "    params,\n",
    "    X,\n",
    "    mask=mask,\n",
    "    key = key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGgxk4Ikpre4"
   },
   "source": [
    "# **Training Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DpdzHRncw_8m"
   },
   "outputs": [],
   "source": [
    "class CharacterBasedAsciiDatasetForLLM:\n",
    "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
    "\n",
    "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
    "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            corpus = f.read()\n",
    "\n",
    "        # Tokenize by splitting the text into characters\n",
    "        characters = list(corpus)\n",
    "        self.vocab_size = len(set(characters))  # Number of unique words\n",
    "\n",
    "        # Create a mapping from characters to unique IDs\n",
    "        self.character_to_id = {character: i for i, character in enumerate(set(characters))}\n",
    "\n",
    "        # Store the inverse mapping from IDs to characters\n",
    "        self.id_to_character = {i: character for character, i in self.character_to_id.items()}\n",
    "\n",
    "        # Convert the words in the corpus to their corresponding IDs\n",
    "        corpus = np.array([self.character_to_id[character] for character in characters]).astype(np.int32)\n",
    "\n",
    "        crop_len = sequence_length + 1\n",
    "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
    "        if ragged:\n",
    "            corpus = corpus[:-ragged]\n",
    "        corpus = corpus.reshape([-1, crop_len])\n",
    "\n",
    "        if num_batches < 10:\n",
    "            raise ValueError(\n",
    "                f\"Only {num_batches} batches; consider a shorter \"\n",
    "                \"sequence or a smaller batch.\"\n",
    "            )\n",
    "\n",
    "        self._ds = CharacterBasedAsciiDatasetForLLM._infinite_shuffle(\n",
    "            corpus, batch_size * 10\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Yield next mini-batch.\"\"\"\n",
    "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
    "        batch = np.stack(batch)\n",
    "        # Create the language modeling observation/target pairs.\n",
    "        return dict(\n",
    "            input=batch[:, :-1], target=batch[:, 1:]\n",
    "        )\n",
    "\n",
    "    def ids_to_characters(self, ids):\n",
    "        \"\"\"Convert a sequence of character IDs to characters.\"\"\"\n",
    "        return [self.id_to_character[id] for id in ids]\n",
    "\n",
    "    @staticmethod\n",
    "    def _infinite_shuffle(iterable, buffer_size):\n",
    "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
    "        ds = itertools.cycle(iterable)\n",
    "        buf = [next(ds) for _ in range(buffer_size)]\n",
    "        random.shuffle(buf)\n",
    "        while True:\n",
    "            item = next(ds)\n",
    "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
    "            result, buf[idx] = buf[idx], item\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MNYD1DoSE_hx"
   },
   "outputs": [],
   "source": [
    "class CharacterBasedAsciiDatasetForLLM:\n",
    "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
    "\n",
    "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
    "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            corpus = f.read()\n",
    "\n",
    "        # Tokenize by splitting the text into characters\n",
    "        characters = list(corpus)\n",
    "        self.vocab_size = len(set(characters))  # Number of unique words\n",
    "\n",
    "        # Create a mapping from characters to unique IDs\n",
    "        self.character_to_id = {character: i for i, character in enumerate(set(characters))}\n",
    "\n",
    "        # Store the inverse mapping from IDs to characters\n",
    "        self.id_to_character = {i: character for character, i in self.character_to_id.items()}\n",
    "\n",
    "        # Convert the words in the corpus to their corresponding IDs\n",
    "        corpus = np.array([self.character_to_id[character] for character in characters]).astype(np.int32)\n",
    "\n",
    "        crop_len = sequence_length + 1\n",
    "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
    "        if ragged:\n",
    "            corpus = corpus[:-ragged]\n",
    "        corpus = corpus.reshape([-1, crop_len])\n",
    "\n",
    "        if num_batches < 10:\n",
    "            raise ValueError(\n",
    "                f\"Only {num_batches} batches; consider a shorter \"\n",
    "                \"sequence or a smaller batch.\"\n",
    "            )\n",
    "\n",
    "        self._ds = CharacterBasedAsciiDatasetForLLM._infinite_shuffle(\n",
    "            corpus, batch_size * 10\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Yield next mini-batch.\"\"\"\n",
    "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
    "        batch = np.stack(batch)\n",
    "        # Create the language modeling observation/target pairs.\n",
    "        return dict(\n",
    "            input=batch[:, :-1], target=batch[:, 1:]\n",
    "        )\n",
    "\n",
    "    def ids_to_characters(self, ids):\n",
    "        \"\"\"Convert a sequence of character IDs to characters.\"\"\"\n",
    "        return [self.id_to_character[id] for id in ids]\n",
    "\n",
    "    @staticmethod\n",
    "    def _infinite_shuffle(iterable, buffer_size):\n",
    "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
    "        ds = itertools.cycle(iterable)\n",
    "        buf = [next(ds) for _ in range(buffer_size)]\n",
    "        random.shuffle(buf)\n",
    "        while True:\n",
    "            item = next(ds)\n",
    "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
    "            result, buf[idx] = buf[idx], item\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Iy6Ea5iQFAFg"
   },
   "outputs": [],
   "source": [
    "class CharacterBasedAsciiDatasetForLLM:\n",
    "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
    "\n",
    "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
    "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            corpus = f.read()\n",
    "\n",
    "        # Tokenize by splitting the text into characters\n",
    "        characters = list(corpus)\n",
    "        self.vocab_size = len(set(characters))  # Number of unique words\n",
    "\n",
    "        # Create a mapping from characters to unique IDs\n",
    "        self.character_to_id = {character: i for i, character in enumerate(set(characters))}\n",
    "\n",
    "        # Store the inverse mapping from IDs to characters\n",
    "        self.id_to_character = {i: character for character, i in self.character_to_id.items()}\n",
    "\n",
    "        # Convert the words in the corpus to their corresponding IDs\n",
    "        corpus = np.array([self.character_to_id[character] for character in characters]).astype(np.int32)\n",
    "\n",
    "        crop_len = sequence_length + 1\n",
    "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
    "        if ragged:\n",
    "            corpus = corpus[:-ragged]\n",
    "        corpus = corpus.reshape([-1, crop_len])\n",
    "\n",
    "        if num_batches < 10:\n",
    "            raise ValueError(\n",
    "                f\"Only {num_batches} batches; consider a shorter \"\n",
    "                \"sequence or a smaller batch.\"\n",
    "            )\n",
    "\n",
    "        self.num_batches = num_batches  # Store the number of batches\n",
    "        self._ds = CharacterBasedAsciiDatasetForLLM._infinite_shuffle(\n",
    "            corpus, batch_size * 10\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Yield next mini-batch.\"\"\"\n",
    "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
    "        batch = np.stack(batch)\n",
    "        # Create the language modeling observation/target pairs.\n",
    "        return dict(\n",
    "            input=batch[:, :-1], target=batch[:, 1:]\n",
    "        )\n",
    "\n",
    "    def ids_to_characters(self, ids):\n",
    "        \"\"\"Convert a sequence of character IDs to characters.\"\"\"\n",
    "        return [self.id_to_character[id] for id in ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    @staticmethod\n",
    "    def _infinite_shuffle(iterable, buffer_size):\n",
    "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
    "        ds = itertools.cycle(iterable)\n",
    "        buf = [next(ds) for _ in range(buffer_size)]\n",
    "        random.shuffle(buf)\n",
    "        while True:\n",
    "            item = next(ds)\n",
    "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
    "            result, buf[idx] = buf[idx], item\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Cv9yYXRDpy9k"
   },
   "outputs": [],
   "source": [
    "def sequence_loss_fn(logits, targets):\n",
    "  \"\"\"Compute the loss on data wrt params.\"\"\"\n",
    "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
    "  assert logits.shape == target_labels.shape\n",
    "  mask = jnp.greater(targets, 0)\n",
    "  loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
    "  sequence_loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
    "\n",
    "  return sequence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pEZEqMqgQny0"
   },
   "outputs": [],
   "source": [
    "def compute_bpc(logits, batch):\n",
    "  # Compute BPC (bits-per-character)\n",
    "  log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  target_log_probs = jnp.take_along_axis(log_probs, batch['target'][..., None], axis=-1).squeeze(-1)\n",
    "  bpc = -jnp.mean(target_log_probs) / jnp.log(2)\n",
    "  return bpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0s9jpE06qNg1"
   },
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnums=(4, 5))\n",
    "def train_step(params, optimizer_state, batch, key, apply_fn, update_fn):\n",
    "  def loss_fn(params):\n",
    "    T = batch['input'].shape[1]\n",
    "    logits,_ = apply_fn(params, batch['input'], jnp.tril(np.ones((T, T))), key)\n",
    "    loss   = sequence_loss_fn(logits, batch['target'])\n",
    "    return loss, logits\n",
    "\n",
    "  (loss,logits), gradients = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "  bpc = compute_bpc(logits, batch)\n",
    "  updates, optimizer_state = update_fn(gradients, optimizer_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, optimizer_state, loss, bpc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "go7U4M_FfWSv"
   },
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnums=(2, ))\n",
    "def generate_prediction(params, input, apply_fn):\n",
    "  logits = apply_fn(params, input)\n",
    "  argmax_out = jnp.argmax(logits, axis=-1)\n",
    "  return argmax_out[0][-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CF_gzkBcQrO3"
   },
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnums=(4))\n",
    "def validation_step(params, batch, mask, key, apply_fn):\n",
    "    \"\"\"\n",
    "    Perform a validation step, computing both the loss and bits-per-character (BPC).\n",
    "\n",
    "    Args:\n",
    "        params: Model parameters.\n",
    "        batch: Validation batch containing 'input' and 'target'.\n",
    "        mask: Mask for the sequence.\n",
    "        key: JAX PRNG key.\n",
    "        model_apply: The model's apply function.\n",
    "\n",
    "    Returns:\n",
    "        loss: The computed validation loss.\n",
    "        bpc: The computed bits-per-character for the given batch.\n",
    "    \"\"\"\n",
    "    # Compute logits from the model\n",
    "    logits,_ = apply_fn(params, batch['input'], mask, key)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = sequence_loss_fn(logits, batch['target'])\n",
    "\n",
    "    # Compute bpc\n",
    "    bpc = bpc = compute_bpc(logits, batch)\n",
    "\n",
    "    return loss, bpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "M2xILceuo6w2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo6qHwL1ydUF"
   },
   "source": [
    "# **Hyperparameter-Tuning Training and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDdU7xDE5FSw"
   },
   "outputs": [],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Language Modelling with Transformers\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"batch_size\": 0.02,\n",
    "    \"seq_length\": 128,\n",
    "    \"d_m\": 64,\n",
    "    \"num_heads\": 4,\n",
    "    \"num_layers_list\": 1,\n",
    "    \"ff_widening_factor\": 4,\n",
    "    \"LR\": 2e-3,\n",
    "    \"dropout_rate\": 0.1\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Define search ranges for hyperparameters\n",
    "batch_sizes = [32, 64]  # Adjust around 32\n",
    "seq_lengths = [64, 128]  # Adjust around 64\n",
    "d_ms = [128, 256]        # Adjust around 128\n",
    "num_heads_list = [4, 8]   # Adjust around 4\n",
    "num_layers_list = [2, 4]  # Adjust around 1\n",
    "ff_widening_factors = [4, 8]  # Adjust around 2\n",
    "LRs = [1e-3, 2e-3]     # Adjust around 2e-3\n",
    "dropout_rates = [0.1, 0.2]  # Adjust around 0.1\n",
    "\n",
    "best_val_bpc = np.inf\n",
    "best_hyperparameters = {}\n",
    "best_params = None\n",
    "MAX_STEPS = 10 # Do 10 epochs\n",
    "\n",
    "# Loop over all combinations of hyperparameters\n",
    "for bs in batch_sizes:\n",
    "    for seq_len in seq_lengths:\n",
    "        for d_m in d_ms:\n",
    "            for nh in num_heads_list:\n",
    "                for nl in num_layers_list:\n",
    "                    for ff_wf in ff_widening_factors:\n",
    "                        for lr in LRs:\n",
    "                            for dr in dropout_rates:\n",
    "                                print(f\"Training with BS={bs}, Seq_Len={seq_len}, DM={d_m}, NH={nh}, NL={nl}, FF_WF={ff_wf}, LR={lr}, DR={dr}\")\n",
    "\n",
    "                                # Update LM parameters\n",
    "                                lm = LM(\n",
    "                                    num_heads=nh,\n",
    "                                    num_layers=nl,\n",
    "                                    d_m=d_m,\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    ff_widening_factor=ff_wf,\n",
    "                                    dropout_rate=dr,\n",
    "                                    training=True,\n",
    "                                    pre_norm=False,\n",
    "                                    tie_weights=False\n",
    "                                )\n",
    "\n",
    "                                # Update dataset with new sequence length\n",
    "                                train_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.train\", bs, seq_len)\n",
    "\n",
    "                                # Update validation dataset with new sequence length\n",
    "                                val_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.valid\", bs, seq_len)\n",
    "\n",
    "                                # Reinitialize optimizer with new learning rate\n",
    "                                optimizer = optax.adam(lr, b1=0.9, b2=0.99)\n",
    "\n",
    "                                # Initialize parameters\n",
    "                                key = jax.random.PRNGKey(90)\n",
    "                                key, init_key = jax.random.split(key)\n",
    "                                batch = next(train_dataset)  # Get a batch to initialize parameters\n",
    "                                mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
    "                                params = lm.init(key, batch['input'], mask, init_key)\n",
    "                                optimizer_state = optimizer.init(params)\n",
    "\n",
    "                                # Training Loop\n",
    "                                for epoch in range(1, MAX_STEPS + 1):\n",
    "                                    print(\"Start Training\")\n",
    "                                    start_time = time.time()\n",
    "                                    train_losses = []\n",
    "                                    train_bpcs = []\n",
    "\n",
    "                                    for batch_idx in range(len(train_dataset)):\n",
    "                                        key, train_key = jax.random.split(key)\n",
    "                                        batch = next(train_dataset)\n",
    "                                        params, optimizer_state, train_loss, train_bpc = train_step(\n",
    "                                            params, optimizer_state, batch, train_key, lm.apply, optimizer.update)\n",
    "                                        train_losses.append(train_loss)\n",
    "                                        train_bpcs.append(train_bpc)\n",
    "\n",
    "                                        # Log intermediate training progress every 200 batches\n",
    "                                        if (batch_idx + 1) % 200 == 0:\n",
    "                                            avg_train_loss = jnp.mean(jnp.array(train_losses[-200:]))\n",
    "                                            avg_train_bpc = jnp.mean(jnp.array(train_bpcs[-200:]))\n",
    "                                            elapsed_time = (time.time() - start_time) / (batch_idx + 1) * 1000\n",
    "                                            print(f\"| epoch {epoch} | {batch_idx + 1}/{len(train_dataset)} batches | \"\n",
    "                                                  f\"lr {lr:.4f} | ms/batch {elapsed_time:.2f} | loss {avg_train_loss:.4f} | \"\n",
    "                                                  f\"bpc {avg_train_bpc:.4f}\")\n",
    "\n",
    "                                    # Average training loss and BPC for the epoch\n",
    "                                    avg_train_loss = jnp.mean(jnp.array(train_losses))\n",
    "                                    avg_train_bpc = jnp.mean(jnp.array(train_bpcs))\n",
    "\n",
    "                                    # Validation Phase\n",
    "                                    val_losses = []\n",
    "                                    val_bpcs = []\n",
    "                                    for batch_idx in range(len(val_batch)):\n",
    "                                      val_batch = next(val_dataset)\n",
    "                                      mask = jnp.tril(jnp.ones((val_batch['input'].shape[1], val_batch['input'].shape[1])))\n",
    "                                      key, val_key = jax.random.split(key)\n",
    "                                      val_loss, val_bpc = validation_step(params, val_batch, mask, val_key, lm.apply)\n",
    "                                      val_losses.append(val_loss)\n",
    "                                      val_bpcs.append(val_bpc)\n",
    "\n",
    "                                    # Average validation loss and BPC for the epoch\n",
    "                                    avg_val_loss = jnp.mean(jnp.array(val_losses))\n",
    "                                    avg_val_bpc = jnp.mean(jnp.array(val_bpcs))\n",
    "\n",
    "                                    # Calculate epoch time\n",
    "                                    epoch_time = time.time() - start_time\n",
    "\n",
    "                                    # Log end of epoch results\n",
    "                                    print(\"--------------------------------------------------------------------------------\")\n",
    "                                    print(f\"| end of epoch {epoch} | time: {epoch_time:.2f}s | train loss {avg_train_loss:.4f} | train bpc {avg_train_bpc:.4f} | \"\n",
    "                                          f\"valid loss {avg_val_loss:.4f} | valid bpc {avg_val_bpc:.4f} \")\n",
    "                                    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "                                    # Store best hyperparameters\n",
    "                                    if val_bpc < best_val_bpc:\n",
    "                                        best_val_bpc = val_bpc\n",
    "                                        best_hyperparameters = {\n",
    "                                            \"batch_size\": bs,\n",
    "                                            \"seq_length\": seq_len,\n",
    "                                            \"d_m\": d_m,\n",
    "                                            \"num_heads\": nh,\n",
    "                                            \"num_layers\": nl,\n",
    "                                            \"ff_widening_factor\": ff_wf,\n",
    "                                            \"LR\": lr,\n",
    "                                            \"dropout_rate\": dr\n",
    "                                        }\n",
    "                                        best_params = params  # Store the best model parameters\n",
    "\n",
    "# After training, evaluate on the entire test set using the best hyperparameters and parameters\n",
    "# Reinitialize LM with the best hyperparameters\n",
    "lm = LM(\n",
    "    num_heads=best_hyperparameters[\"num_heads\"],\n",
    "    num_layers=best_hyperparameters[\"num_layers\"],\n",
    "    d_m=best_hyperparameters[\"d_m\"],\n",
    "    vocab_size=vocab_size,\n",
    "    ff_widening_factor=best_hyperparameters[\"ff_widening_factor\"],\n",
    "    dropout_rate=best_hyperparameters[\"dropout_rate\"],\n",
    "    training=False,  # Ensure the model is in evaluation mode\n",
    "    pre_norm=False,\n",
    "    tie_weights=False\n",
    ")\n",
    "\n",
    "# Reinitialize test dataset with the best batch size and sequence length\n",
    "test_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.test\", best_hyperparameters[\"batch_size\"], best_hyperparameters[\"seq_length\"])\n",
    "\n",
    "# Evaluate on all batches in the test dataset\n",
    "test_losses = []\n",
    "test_bpcs = []\n",
    "for test_batch in test_dataset:\n",
    "    mask = jnp.tril(jnp.ones((test_batch['input'].shape[1], test_batch['input'].shape[1])))\n",
    "    key, test_key = jax.random.split(key)\n",
    "    test_loss, test_bpc = validation_step(best_params, test_batch, mask, test_key, lm.apply)\n",
    "    test_losses.append(test_loss)\n",
    "    test_bpcs.append(test_bpc)\n",
    "\n",
    "# Compute average test loss and BPC across all batches\n",
    "avg_test_loss = jnp.mean(jnp.array(test_losses))\n",
    "avg_test_bpc = jnp.mean(jnp.array(test_bpcs))\n",
    "\n",
    "# Final Test BPC and Loss log\n",
    "print(\"================================================================================\")\n",
    "print(f\"| End of training | test loss {avg_test_loss:.2f} | test bpc {avg_test_bpc:.2f} \")\n",
    "print(\"================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Vo_fAoEy5c0"
   },
   "source": [
    "# **Non-Hyperparameter-Tuning Training and Testing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_zOlfhoGxd-"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_length = 128\n",
    "d_m        = 256\n",
    "num_heads  = 4\n",
    "num_layers = 2\n",
    "ff_widening_factor = 4\n",
    "LR = 1e-3\n",
    "dropout_rate=0.2\n",
    "training=True\n",
    "pre_norm = False\n",
    "tie_weights = True\n",
    "useAdamWOptimization = False\n",
    "useLearningRateSchedule = False\n",
    "MAX_STEPS = 50\n",
    "WARMUP_STEPS = 10\n",
    "optimizer = None\n",
    "\n",
    "# set up the data\n",
    "train_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.train\", batch_size, seq_length)\n",
    " # Update validation dataset with new sequence length\n",
    "val_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.valid\", batch_size, seq_length)\n",
    "vocab_size = train_dataset.vocab_size\n",
    "batch = next(train_dataset)\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# initialise model\n",
    "\n",
    "lm = LM(num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            d_m=d_m,\n",
    "            vocab_size=vocab_size,\n",
    "            ff_widening_factor=ff_widening_factor,\n",
    "            dropout_rate = dropout_rate,\n",
    "            training=training,\n",
    "            pre_norm = pre_norm,\n",
    "            tie_weights=tie_weights\n",
    "      )\n",
    "mask   = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
    "key = jax.random.PRNGKey(90)\n",
    "params = lm.init(key, batch['input'], mask,key)\n",
    "\n",
    "# set up the learning rate schedule\n",
    "learning_rate_fn = optax.warmup_cosine_decay_schedule(\n",
    "    init_value = 0.0,\n",
    "    peak_value = LR,\n",
    "    warmup_steps = WARMUP_STEPS,\n",
    "    decay_steps = MAX_STEPS - WARMUP_STEPS,\n",
    "    end_value = 0.0\n",
    ")\n",
    "# set up the optimiser\n",
    "if useAdamWOptimization:\n",
    "    if useLearningRateSchedule:\n",
    "      optimizer = optax.adamw(\n",
    "          learning_rate=learning_rate_fn,\n",
    "          b1=0.9,\n",
    "          b2=0.99,\n",
    "          weight_decay=1e-4) # Weight decay for regularization\n",
    "    else:\n",
    "      optimizer = optax.adamw(\n",
    "          LR,\n",
    "          b1=0.9,\n",
    "          b2=0.99,\n",
    "          weight_decay=1e-4, # Weight decay for regularization\n",
    "          )\n",
    "else:\n",
    "  if useLearningRateSchedule:\n",
    "    optimizer = optax.adam(\n",
    "        learning_rate=learning_rate_fn,\n",
    "        b1=0.9,\n",
    "        b2=0.99)\n",
    "  else:\n",
    "    optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
    "optimizer_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f0bced8aa2944587a6d5351c44267ab7",
      "0fdba02052c04155a7b12fb6cb508100",
      "3130f876ded9491ca19ac8473689699a",
      "5eb13287d30e4028995c478a98f40c5e",
      "cff8736081da46409ad6bb7bd44613d5",
      "8692dbbd46154f7284d65336e548786d",
      "557a034fc53b4acba623ebfffbdd6ef6",
      "ab134c0566fd46d7b0eee5d7b2d44ca0"
     ]
    },
    "id": "AwFMdzY9O1nx",
    "outputId": "6e397b98-3c5c-45b8-b38a-c939e4e86789"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20240825_132733-58tc1msa</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/58tc1msa' target=\"_blank\">summer-eon-26</a></strong> to <a href='https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers' target=\"_blank\">https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/58tc1msa' target=\"_blank\">https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/58tc1msa</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start Training\n",
      "| epoch 1 | 200/2222 batches | lr 0.0010 | ms/batch 32.52 | loss 3.2157 | bpc 4.6472\n",
      "| epoch 1 | 400/2222 batches | lr 0.0010 | ms/batch 24.40 | loss 2.8497 | bpc 4.1165\n",
      "| epoch 1 | 600/2222 batches | lr 0.0010 | ms/batch 21.80 | loss 2.5108 | bpc 3.6268\n",
      "| epoch 1 | 800/2222 batches | lr 0.0010 | ms/batch 20.44 | loss 2.3455 | bpc 3.3896\n",
      "| epoch 1 | 1000/2222 batches | lr 0.0010 | ms/batch 19.64 | loss 2.2736 | bpc 3.2839\n",
      "| epoch 1 | 1200/2222 batches | lr 0.0010 | ms/batch 19.13 | loss 2.2579 | bpc 3.2644\n",
      "| epoch 1 | 1400/2222 batches | lr 0.0010 | ms/batch 18.79 | loss 2.1764 | bpc 3.1464\n",
      "| epoch 1 | 1600/2222 batches | lr 0.0010 | ms/batch 18.53 | loss 2.2135 | bpc 3.1974\n",
      "| epoch 1 | 1800/2222 batches | lr 0.0010 | ms/batch 18.33 | loss 2.0859 | bpc 3.0219\n",
      "| epoch 1 | 2000/2222 batches | lr 0.0010 | ms/batch 18.18 | loss 2.0015 | bpc 2.8956\n",
      "| epoch 1 | 2200/2222 batches | lr 0.0010 | ms/batch 18.06 | loss 1.9622 | bpc 2.8363\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 1 | time: 42.46s | train loss 2.3491 | train bpc 3.3954 | valid loss 9.6051 | valid bpc 13.8581 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 2 | 200/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.9748 | bpc 2.8585\n",
      "| epoch 2 | 400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.9307 | bpc 2.7905\n",
      "| epoch 2 | 600/2222 batches | lr 0.0010 | ms/batch 17.05 | loss 1.8449 | bpc 2.6659\n",
      "| epoch 2 | 800/2222 batches | lr 0.0010 | ms/batch 17.06 | loss 1.8089 | bpc 2.6150\n",
      "| epoch 2 | 1000/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.7187 | bpc 2.4829\n",
      "| epoch 2 | 1200/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.7577 | bpc 2.5420\n",
      "| epoch 2 | 1400/2222 batches | lr 0.0010 | ms/batch 17.04 | loss 1.7201 | bpc 2.4874\n",
      "| epoch 2 | 1600/2222 batches | lr 0.0010 | ms/batch 17.03 | loss 1.9228 | bpc 2.7775\n",
      "| epoch 2 | 1800/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 1.6351 | bpc 2.3703\n",
      "| epoch 2 | 2000/2222 batches | lr 0.0010 | ms/batch 17.01 | loss 1.5393 | bpc 2.2278\n",
      "| epoch 2 | 2200/2222 batches | lr 0.0010 | ms/batch 17.00 | loss 1.5662 | bpc 2.2645\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 2 | time: 39.24s | train loss 1.7632 | train bpc 2.5497 | valid loss 9.3651 | valid bpc 13.5106 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 3 | 200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.5983 | bpc 2.3142\n",
      "| epoch 3 | 400/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.5449 | bpc 2.2333\n",
      "| epoch 3 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.4892 | bpc 2.1524\n",
      "| epoch 3 | 800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.4792 | bpc 2.1387\n",
      "| epoch 3 | 1000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.4257 | bpc 2.0598\n",
      "| epoch 3 | 1200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.4937 | bpc 2.1609\n",
      "| epoch 3 | 1400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.5066 | bpc 2.1791\n",
      "| epoch 3 | 1600/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.7886 | bpc 2.5839\n",
      "| epoch 3 | 1800/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.4374 | bpc 2.0849\n",
      "| epoch 3 | 2000/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.3554 | bpc 1.9622\n",
      "| epoch 3 | 2200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.4111 | bpc 2.0407\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 3 | time: 38.82s | train loss 1.5018 | train bpc 2.1723 | valid loss 9.4302 | valid bpc 13.6049 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 4 | 200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.4534 | bpc 2.1048\n",
      "| epoch 4 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.3985 | bpc 2.0222\n",
      "| epoch 4 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3612 | bpc 1.9677\n",
      "| epoch 4 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3568 | bpc 1.9623\n",
      "| epoch 4 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.3209 | bpc 1.9088\n",
      "| epoch 4 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.3945 | bpc 2.0178\n",
      "| epoch 4 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.4241 | bpc 2.0600\n",
      "| epoch 4 | 1600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.7270 | bpc 2.4951\n",
      "| epoch 4 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3581 | bpc 1.9705\n",
      "| epoch 4 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2846 | bpc 1.8603\n",
      "| epoch 4 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3421 | bpc 1.9411\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 4 | time: 39.00s | train loss 1.4016 | train bpc 2.0277 | valid loss 9.7405 | valid bpc 14.0521 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 5 | 200/2222 batches | lr 0.0010 | ms/batch 16.76 | loss 1.3867 | bpc 2.0085\n",
      "| epoch 5 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.3348 | bpc 1.9304\n",
      "| epoch 5 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.2993 | bpc 1.8783\n",
      "| epoch 5 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2989 | bpc 1.8787\n",
      "| epoch 5 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2663 | bpc 1.8299\n",
      "| epoch 5 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3420 | bpc 1.9420\n",
      "| epoch 5 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.3805 | bpc 1.9970\n",
      "| epoch 5 | 1600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.6894 | bpc 2.4408\n",
      "| epoch 5 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.3159 | bpc 1.9092\n",
      "| epoch 5 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2421 | bpc 1.7987\n",
      "| epoch 5 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.3040 | bpc 1.8862\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 5 | time: 38.98s | train loss 1.3506 | train bpc 1.9541 | valid loss 9.5376 | valid bpc 13.7596 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 6 | 200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.3474 | bpc 1.9516\n",
      "| epoch 6 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2920 | bpc 1.8685\n",
      "| epoch 6 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2607 | bpc 1.8226\n",
      "| epoch 6 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2602 | bpc 1.8228\n",
      "| epoch 6 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2270 | bpc 1.7730\n",
      "| epoch 6 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.3080 | bpc 1.8929\n",
      "| epoch 6 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.3469 | bpc 1.9484\n",
      "| epoch 6 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.6631 | bpc 2.4026\n",
      "| epoch 6 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2858 | bpc 1.8660\n",
      "| epoch 6 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2156 | bpc 1.7604\n",
      "| epoch 6 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2771 | bpc 1.8474\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 6 | time: 38.95s | train loss 1.3165 | train bpc 1.9047 | valid loss 9.4744 | valid bpc 13.6688 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 7 | 200/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.3183 | bpc 1.9098\n",
      "| epoch 7 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.2639 | bpc 1.8281\n",
      "| epoch 7 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.2334 | bpc 1.7832\n",
      "| epoch 7 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2327 | bpc 1.7832\n",
      "| epoch 7 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1996 | bpc 1.7335\n",
      "| epoch 7 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2822 | bpc 1.8557\n",
      "| epoch 7 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.3257 | bpc 1.9178\n",
      "| epoch 7 | 1600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.6423 | bpc 2.3728\n",
      "| epoch 7 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2644 | bpc 1.8344\n",
      "| epoch 7 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1928 | bpc 1.7280\n",
      "| epoch 7 | 2200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2577 | bpc 1.8193\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 7 | time: 38.92s | train loss 1.2919 | train bpc 1.8693 | valid loss 8.9273 | valid bpc 12.8794 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 8 | 200/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2976 | bpc 1.8798\n",
      "| epoch 8 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2411 | bpc 1.7951\n",
      "| epoch 8 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2127 | bpc 1.7532\n",
      "| epoch 8 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2098 | bpc 1.7501\n",
      "| epoch 8 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1772 | bpc 1.7014\n",
      "| epoch 8 | 1200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2613 | bpc 1.8255\n",
      "| epoch 8 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.3085 | bpc 1.8929\n",
      "| epoch 8 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.6261 | bpc 2.3495\n",
      "| epoch 8 | 1800/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2451 | bpc 1.8068\n",
      "| epoch 8 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1781 | bpc 1.7067\n",
      "| epoch 8 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2424 | bpc 1.7972\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 8 | time: 38.97s | train loss 1.2726 | train bpc 1.8414 | valid loss 8.6993 | valid bpc 12.5512 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 9 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2806 | bpc 1.8555\n",
      "| epoch 9 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.2236 | bpc 1.7701\n",
      "| epoch 9 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1955 | bpc 1.7286\n",
      "| epoch 9 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1919 | bpc 1.7243\n",
      "| epoch 9 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1627 | bpc 1.6803\n",
      "| epoch 9 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2456 | bpc 1.8030\n",
      "| epoch 9 | 1400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2946 | bpc 1.8731\n",
      "| epoch 9 | 1600/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.6135 | bpc 2.3313\n",
      "| epoch 9 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.2322 | bpc 1.7885\n",
      "| epoch 9 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1647 | bpc 1.6872\n",
      "| epoch 9 | 2200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2304 | bpc 1.7800\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 9 | time: 38.89s | train loss 1.2577 | train bpc 1.8200 | valid loss 9.1141 | valid bpc 13.1492 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 10 | 200/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2678 | bpc 1.8372\n",
      "| epoch 10 | 400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.2083 | bpc 1.7478\n",
      "| epoch 10 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1831 | bpc 1.7106\n",
      "| epoch 10 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1779 | bpc 1.7043\n",
      "| epoch 10 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1468 | bpc 1.6575\n",
      "| epoch 10 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2346 | bpc 1.7871\n",
      "| epoch 10 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2820 | bpc 1.8548\n",
      "| epoch 10 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.6039 | bpc 2.3175\n",
      "| epoch 10 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2224 | bpc 1.7745\n",
      "| epoch 10 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1536 | bpc 1.6711\n",
      "| epoch 10 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2202 | bpc 1.7655\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 10 | time: 38.95s | train loss 1.2455 | train bpc 1.8025 | valid loss 8.9930 | valid bpc 12.9742 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 11 | 200/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2551 | bpc 1.8186\n",
      "| epoch 11 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1981 | bpc 1.7335\n",
      "| epoch 11 | 600/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1700 | bpc 1.6918\n",
      "| epoch 11 | 800/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1667 | bpc 1.6881\n",
      "| epoch 11 | 1000/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1368 | bpc 1.6431\n",
      "| epoch 11 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2219 | bpc 1.7689\n",
      "| epoch 11 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2743 | bpc 1.8435\n",
      "| epoch 11 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5949 | bpc 2.3044\n",
      "| epoch 11 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2129 | bpc 1.7608\n",
      "| epoch 11 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1470 | bpc 1.6617\n",
      "| epoch 11 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2112 | bpc 1.7521\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 11 | time: 38.96s | train loss 1.2354 | train bpc 1.7878 | valid loss 8.7056 | valid bpc 12.5595 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 12 | 200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.2480 | bpc 1.8085\n",
      "| epoch 12 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1882 | bpc 1.7188\n",
      "| epoch 12 | 600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1609 | bpc 1.6785\n",
      "| epoch 12 | 800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1559 | bpc 1.6726\n",
      "| epoch 12 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1273 | bpc 1.6293\n",
      "| epoch 12 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2131 | bpc 1.7562\n",
      "| epoch 12 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2670 | bpc 1.8332\n",
      "| epoch 12 | 1600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.5870 | bpc 2.2930\n",
      "| epoch 12 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2047 | bpc 1.7491\n",
      "| epoch 12 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1395 | bpc 1.6509\n",
      "| epoch 12 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2028 | bpc 1.7402\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 12 | time: 38.97s | train loss 1.2268 | train bpc 1.7755 | valid loss 9.2089 | valid bpc 13.2858 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 13 | 200/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.2394 | bpc 1.7962\n",
      "| epoch 13 | 400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1815 | bpc 1.7093\n",
      "| epoch 13 | 600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.1523 | bpc 1.6663\n",
      "| epoch 13 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1480 | bpc 1.6610\n",
      "| epoch 13 | 1000/2222 batches | lr 0.0010 | ms/batch 16.96 | loss 1.1168 | bpc 1.6143\n",
      "| epoch 13 | 1200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.2064 | bpc 1.7464\n",
      "| epoch 13 | 1400/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 1.2601 | bpc 1.8234\n",
      "| epoch 13 | 1600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.5809 | bpc 2.2841\n",
      "| epoch 13 | 1800/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1979 | bpc 1.7392\n",
      "| epoch 13 | 2000/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.1328 | bpc 1.6410\n",
      "| epoch 13 | 2200/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1976 | bpc 1.7327\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 13 | time: 39.04s | train loss 1.2195 | train bpc 1.7650 | valid loss 9.3098 | valid bpc 13.4315 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 14 | 200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2330 | bpc 1.7869\n",
      "| epoch 14 | 400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1726 | bpc 1.6965\n",
      "| epoch 14 | 600/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1464 | bpc 1.6577\n",
      "| epoch 14 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1410 | bpc 1.6509\n",
      "| epoch 14 | 1000/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1121 | bpc 1.6075\n",
      "| epoch 14 | 1200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1971 | bpc 1.7330\n",
      "| epoch 14 | 1400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.2558 | bpc 1.8169\n",
      "| epoch 14 | 1600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.5759 | bpc 2.2772\n",
      "| epoch 14 | 1800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1905 | bpc 1.7283\n",
      "| epoch 14 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1274 | bpc 1.6332\n",
      "| epoch 14 | 2200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1922 | bpc 1.7248\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 14 | time: 39.01s | train loss 1.2131 | train bpc 1.7556 | valid loss 9.3954 | valid bpc 13.5549 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 15 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2285 | bpc 1.7805\n",
      "| epoch 15 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1659 | bpc 1.6867\n",
      "| epoch 15 | 600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.1400 | bpc 1.6483\n",
      "| epoch 15 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1339 | bpc 1.6407\n",
      "| epoch 15 | 1000/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1040 | bpc 1.5957\n",
      "| epoch 15 | 1200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1926 | bpc 1.7266\n",
      "| epoch 15 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2484 | bpc 1.8065\n",
      "| epoch 15 | 1600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.5734 | bpc 2.2733\n",
      "| epoch 15 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1846 | bpc 1.7200\n",
      "| epoch 15 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1238 | bpc 1.6281\n",
      "| epoch 15 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1876 | bpc 1.7184\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 15 | time: 38.99s | train loss 1.2076 | train bpc 1.7478 | valid loss 9.1539 | valid bpc 13.2065 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 16 | 200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2240 | bpc 1.7740\n",
      "| epoch 16 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1610 | bpc 1.6797\n",
      "| epoch 16 | 600/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1338 | bpc 1.6395\n",
      "| epoch 16 | 800/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1280 | bpc 1.6321\n",
      "| epoch 16 | 1000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0976 | bpc 1.5865\n",
      "| epoch 16 | 1200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1882 | bpc 1.7201\n",
      "| epoch 16 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2444 | bpc 1.8006\n",
      "| epoch 16 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5645 | bpc 2.2606\n",
      "| epoch 16 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1825 | bpc 1.7167\n",
      "| epoch 16 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1179 | bpc 1.6197\n",
      "| epoch 16 | 2200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1849 | bpc 1.7142\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 16 | time: 39.01s | train loss 1.2024 | train bpc 1.7403 | valid loss 9.4056 | valid bpc 13.5694 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 17 | 200/2222 batches | lr 0.0010 | ms/batch 16.75 | loss 1.2183 | bpc 1.7656\n",
      "| epoch 17 | 400/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1555 | bpc 1.6717\n",
      "| epoch 17 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1296 | bpc 1.6335\n",
      "| epoch 17 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1223 | bpc 1.6238\n",
      "| epoch 17 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0931 | bpc 1.5801\n",
      "| epoch 17 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1818 | bpc 1.7108\n",
      "| epoch 17 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2411 | bpc 1.7957\n",
      "| epoch 17 | 1600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.5612 | bpc 2.2558\n",
      "| epoch 17 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1788 | bpc 1.7115\n",
      "| epoch 17 | 2000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1153 | bpc 1.6158\n",
      "| epoch 17 | 2200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1800 | bpc 1.7073\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 17 | time: 38.89s | train loss 1.1979 | train bpc 1.7338 | valid loss 8.8057 | valid bpc 12.7041 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 18 | 200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.2143 | bpc 1.7599\n",
      "| epoch 18 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1508 | bpc 1.6649\n",
      "| epoch 18 | 600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1251 | bpc 1.6269\n",
      "| epoch 18 | 800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1183 | bpc 1.6184\n",
      "| epoch 18 | 1000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0872 | bpc 1.5715\n",
      "| epoch 18 | 1200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1785 | bpc 1.7062\n",
      "| epoch 18 | 1400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2377 | bpc 1.7908\n",
      "| epoch 18 | 1600/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.5593 | bpc 2.2531\n",
      "| epoch 18 | 1800/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1718 | bpc 1.7013\n",
      "| epoch 18 | 2000/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1138 | bpc 1.6138\n",
      "| epoch 18 | 2200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1761 | bpc 1.7018\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 18 | time: 38.84s | train loss 1.1940 | train bpc 1.7282 | valid loss 8.3167 | valid bpc 11.9986 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 19 | 200/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.2089 | bpc 1.7521\n",
      "| epoch 19 | 400/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1461 | bpc 1.6583\n",
      "| epoch 19 | 600/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1213 | bpc 1.6214\n",
      "| epoch 19 | 800/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1135 | bpc 1.6114\n",
      "| epoch 19 | 1000/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.0834 | bpc 1.5660\n",
      "| epoch 19 | 1200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1745 | bpc 1.7005\n",
      "| epoch 19 | 1400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.2353 | bpc 1.7875\n",
      "| epoch 19 | 1600/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.5567 | bpc 2.2492\n",
      "| epoch 19 | 1800/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1706 | bpc 1.6997\n",
      "| epoch 19 | 2000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1083 | bpc 1.6059\n",
      "| epoch 19 | 2200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1728 | bpc 1.6968\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 19 | time: 38.89s | train loss 1.1902 | train bpc 1.7227 | valid loss 8.6737 | valid bpc 12.5133 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 20 | 200/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.2054 | bpc 1.7473\n",
      "| epoch 20 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1420 | bpc 1.6524\n",
      "| epoch 20 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1178 | bpc 1.6167\n",
      "| epoch 20 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1099 | bpc 1.6059\n",
      "| epoch 20 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0787 | bpc 1.5593\n",
      "| epoch 20 | 1200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1696 | bpc 1.6933\n",
      "| epoch 20 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2318 | bpc 1.7824\n",
      "| epoch 20 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5530 | bpc 2.2440\n",
      "| epoch 20 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1662 | bpc 1.6937\n",
      "| epoch 20 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1078 | bpc 1.6052\n",
      "| epoch 20 | 2200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1696 | bpc 1.6922\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 20 | time: 39.02s | train loss 1.1866 | train bpc 1.7176 | valid loss 8.1043 | valid bpc 11.6921 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 21 | 200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2029 | bpc 1.7436\n",
      "| epoch 21 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1396 | bpc 1.6488\n",
      "| epoch 21 | 600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.1132 | bpc 1.6099\n",
      "| epoch 21 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1047 | bpc 1.5985\n",
      "| epoch 21 | 1000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0757 | bpc 1.5549\n",
      "| epoch 21 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1667 | bpc 1.6891\n",
      "| epoch 21 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2275 | bpc 1.7762\n",
      "| epoch 21 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.5502 | bpc 2.2398\n",
      "| epoch 21 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1639 | bpc 1.6903\n",
      "| epoch 21 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1032 | bpc 1.5985\n",
      "| epoch 21 | 2200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1677 | bpc 1.6898\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 21 | time: 38.91s | train loss 1.1833 | train bpc 1.7128 | valid loss 8.1197 | valid bpc 11.7148 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 22 | 200/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.2001 | bpc 1.7397\n",
      "| epoch 22 | 400/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1347 | bpc 1.6418\n",
      "| epoch 22 | 600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1114 | bpc 1.6074\n",
      "| epoch 22 | 800/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1010 | bpc 1.5933\n",
      "| epoch 22 | 1000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0720 | bpc 1.5496\n",
      "| epoch 22 | 1200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1648 | bpc 1.6865\n",
      "| epoch 22 | 1400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.2249 | bpc 1.7724\n",
      "| epoch 22 | 1600/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.5477 | bpc 2.2362\n",
      "| epoch 22 | 1800/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1603 | bpc 1.6849\n",
      "| epoch 22 | 2000/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1014 | bpc 1.5959\n",
      "| epoch 22 | 2200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1650 | bpc 1.6857\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 22 | time: 38.85s | train loss 1.1803 | train bpc 1.7084 | valid loss 8.3783 | valid bpc 12.0874 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 23 | 200/2222 batches | lr 0.0010 | ms/batch 16.78 | loss 1.1960 | bpc 1.7338\n",
      "| epoch 23 | 400/2222 batches | lr 0.0010 | ms/batch 16.78 | loss 1.1314 | bpc 1.6371\n",
      "| epoch 23 | 600/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1078 | bpc 1.6021\n",
      "| epoch 23 | 800/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0990 | bpc 1.5903\n",
      "| epoch 23 | 1000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0684 | bpc 1.5443\n",
      "| epoch 23 | 1200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1610 | bpc 1.6811\n",
      "| epoch 23 | 1400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.2211 | bpc 1.7670\n",
      "| epoch 23 | 1600/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.5470 | bpc 2.2353\n",
      "| epoch 23 | 1800/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1580 | bpc 1.6818\n",
      "| epoch 23 | 2000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0983 | bpc 1.5913\n",
      "| epoch 23 | 2200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1621 | bpc 1.6814\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 23 | time: 38.86s | train loss 1.1774 | train bpc 1.7042 | valid loss 9.0235 | valid bpc 13.0181 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 24 | 200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1950 | bpc 1.7322\n",
      "| epoch 24 | 400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1285 | bpc 1.6330\n",
      "| epoch 24 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1038 | bpc 1.5962\n",
      "| epoch 24 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0957 | bpc 1.5855\n",
      "| epoch 24 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0670 | bpc 1.5425\n",
      "| epoch 24 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1576 | bpc 1.6760\n",
      "| epoch 24 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.2204 | bpc 1.7662\n",
      "| epoch 24 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.5428 | bpc 2.2292\n",
      "| epoch 24 | 1800/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1574 | bpc 1.6809\n",
      "| epoch 24 | 2000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0972 | bpc 1.5897\n",
      "| epoch 24 | 2200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1590 | bpc 1.6773\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 24 | time: 38.93s | train loss 1.1751 | train bpc 1.7009 | valid loss 8.3103 | valid bpc 11.9894 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 25 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1923 | bpc 1.7286\n",
      "| epoch 25 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1269 | bpc 1.6306\n",
      "| epoch 25 | 600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.1021 | bpc 1.5938\n",
      "| epoch 25 | 800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.0925 | bpc 1.5812\n",
      "| epoch 25 | 1000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0627 | bpc 1.5362\n",
      "| epoch 25 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1565 | bpc 1.6745\n",
      "| epoch 25 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2184 | bpc 1.7632\n",
      "| epoch 25 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.5404 | bpc 2.2258\n",
      "| epoch 25 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1543 | bpc 1.6764\n",
      "| epoch 25 | 2000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0945 | bpc 1.5860\n",
      "| epoch 25 | 2200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1580 | bpc 1.6757\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 25 | time: 38.95s | train loss 1.1727 | train bpc 1.6975 | valid loss 8.2807 | valid bpc 11.9469 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 26 | 200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1893 | bpc 1.7241\n",
      "| epoch 26 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1246 | bpc 1.6272\n",
      "| epoch 26 | 600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0995 | bpc 1.5901\n",
      "| epoch 26 | 800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0912 | bpc 1.5791\n",
      "| epoch 26 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0601 | bpc 1.5324\n",
      "| epoch 26 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1537 | bpc 1.6706\n",
      "| epoch 26 | 1400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2156 | bpc 1.7590\n",
      "| epoch 26 | 1600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.5396 | bpc 2.2246\n",
      "| epoch 26 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1516 | bpc 1.6725\n",
      "| epoch 26 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0916 | bpc 1.5817\n",
      "| epoch 26 | 2200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1573 | bpc 1.6749\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 26 | time: 38.90s | train loss 1.1705 | train bpc 1.6943 | valid loss 8.4384 | valid bpc 12.1743 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 27 | 200/2222 batches | lr 0.0010 | ms/batch 16.78 | loss 1.1879 | bpc 1.7221\n",
      "| epoch 27 | 400/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1209 | bpc 1.6218\n",
      "| epoch 27 | 600/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0973 | bpc 1.5869\n",
      "| epoch 27 | 800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0876 | bpc 1.5740\n",
      "| epoch 27 | 1000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0576 | bpc 1.5290\n",
      "| epoch 27 | 1200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1520 | bpc 1.6682\n",
      "| epoch 27 | 1400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.2139 | bpc 1.7568\n",
      "| epoch 27 | 1600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.5365 | bpc 2.2201\n",
      "| epoch 27 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1502 | bpc 1.6704\n",
      "| epoch 27 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0914 | bpc 1.5817\n",
      "| epoch 27 | 2200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1550 | bpc 1.6715\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 27 | time: 38.90s | train loss 1.1684 | train bpc 1.6914 | valid loss 8.5133 | valid bpc 12.2824 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 28 | 200/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.1853 | bpc 1.7184\n",
      "| epoch 28 | 400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1188 | bpc 1.6189\n",
      "| epoch 28 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0948 | bpc 1.5834\n",
      "| epoch 28 | 800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0850 | bpc 1.5704\n",
      "| epoch 28 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0561 | bpc 1.5266\n",
      "| epoch 28 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1492 | bpc 1.6641\n",
      "| epoch 28 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.2131 | bpc 1.7556\n",
      "| epoch 28 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5338 | bpc 2.2164\n",
      "| epoch 28 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1483 | bpc 1.6678\n",
      "| epoch 28 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0906 | bpc 1.5805\n",
      "| epoch 28 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1523 | bpc 1.6674\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 28 | time: 38.95s | train loss 1.1663 | train bpc 1.6883 | valid loss 8.3818 | valid bpc 12.0928 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 29 | 200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1839 | bpc 1.7165\n",
      "| epoch 29 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1184 | bpc 1.6183\n",
      "| epoch 29 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0941 | bpc 1.5825\n",
      "| epoch 29 | 800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0819 | bpc 1.5659\n",
      "| epoch 29 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0549 | bpc 1.5249\n",
      "| epoch 29 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1476 | bpc 1.6617\n",
      "| epoch 29 | 1400/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.2106 | bpc 1.7522\n",
      "| epoch 29 | 1600/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.5330 | bpc 2.2152\n",
      "| epoch 29 | 1800/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.1464 | bpc 1.6650\n",
      "| epoch 29 | 2000/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.0880 | bpc 1.5767\n",
      "| epoch 29 | 2200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1527 | bpc 1.6683\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 29 | time: 39.02s | train loss 1.1649 | train bpc 1.6864 | valid loss 8.7576 | valid bpc 12.6352 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 30 | 200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1820 | bpc 1.7140\n",
      "| epoch 30 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1145 | bpc 1.6127\n",
      "| epoch 30 | 600/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 1.0913 | bpc 1.5785\n",
      "| epoch 30 | 800/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0798 | bpc 1.5629\n",
      "| epoch 30 | 1000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0537 | bpc 1.5234\n",
      "| epoch 30 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1460 | bpc 1.6597\n",
      "| epoch 30 | 1400/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.2095 | bpc 1.7505\n",
      "| epoch 30 | 1600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.5337 | bpc 2.2160\n",
      "| epoch 30 | 1800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1448 | bpc 1.6631\n",
      "| epoch 30 | 2000/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0860 | bpc 1.5735\n",
      "| epoch 30 | 2200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1493 | bpc 1.6633\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 30 | time: 38.99s | train loss 1.1629 | train bpc 1.6836 | valid loss 8.6265 | valid bpc 12.4458 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 31 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1807 | bpc 1.7119\n",
      "| epoch 31 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1130 | bpc 1.6105\n",
      "| epoch 31 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0897 | bpc 1.5760\n",
      "| epoch 31 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0785 | bpc 1.5611\n",
      "| epoch 31 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0506 | bpc 1.5188\n",
      "| epoch 31 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1428 | bpc 1.6548\n",
      "| epoch 31 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.2082 | bpc 1.7488\n",
      "| epoch 31 | 1600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.5314 | bpc 2.2130\n",
      "| epoch 31 | 1800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1422 | bpc 1.6590\n",
      "| epoch 31 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0842 | bpc 1.5713\n",
      "| epoch 31 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1499 | bpc 1.6645\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 31 | time: 38.98s | train loss 1.1611 | train bpc 1.6810 | valid loss 8.3475 | valid bpc 12.0432 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 32 | 200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1789 | bpc 1.7095\n",
      "| epoch 32 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1119 | bpc 1.6091\n",
      "| epoch 32 | 600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0877 | bpc 1.5733\n",
      "| epoch 32 | 800/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0785 | bpc 1.5610\n",
      "| epoch 32 | 1000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0478 | bpc 1.5149\n",
      "| epoch 32 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1404 | bpc 1.6513\n",
      "| epoch 32 | 1400/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.2076 | bpc 1.7479\n",
      "| epoch 32 | 1600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.5301 | bpc 2.2109\n",
      "| epoch 32 | 1800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1410 | bpc 1.6574\n",
      "| epoch 32 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0850 | bpc 1.5726\n",
      "| epoch 32 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1468 | bpc 1.6595\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 32 | time: 38.98s | train loss 1.1598 | train bpc 1.6790 | valid loss 8.4895 | valid bpc 12.2484 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 33 | 200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1777 | bpc 1.7077\n",
      "| epoch 33 | 400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1085 | bpc 1.6043\n",
      "| epoch 33 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0852 | bpc 1.5694\n",
      "| epoch 33 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0764 | bpc 1.5583\n",
      "| epoch 33 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0466 | bpc 1.5131\n",
      "| epoch 33 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1389 | bpc 1.6494\n",
      "| epoch 33 | 1400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.2050 | bpc 1.7441\n",
      "| epoch 33 | 1600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.5286 | bpc 2.2087\n",
      "| epoch 33 | 1800/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1405 | bpc 1.6567\n",
      "| epoch 33 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0832 | bpc 1.5697\n",
      "| epoch 33 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1464 | bpc 1.6592\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 33 | time: 38.96s | train loss 1.1580 | train bpc 1.6764 | valid loss 8.7462 | valid bpc 12.6186 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 34 | 200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1762 | bpc 1.7055\n",
      "| epoch 34 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1085 | bpc 1.6042\n",
      "| epoch 34 | 600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0835 | bpc 1.5670\n",
      "| epoch 34 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0748 | bpc 1.5557\n",
      "| epoch 34 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0436 | bpc 1.5087\n",
      "| epoch 34 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1369 | bpc 1.6465\n",
      "| epoch 34 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2050 | bpc 1.7438\n",
      "| epoch 34 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5258 | bpc 2.2047\n",
      "| epoch 34 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1394 | bpc 1.6553\n",
      "| epoch 34 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0814 | bpc 1.5671\n",
      "| epoch 34 | 2200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1455 | bpc 1.6577\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 34 | time: 38.91s | train loss 1.1565 | train bpc 1.6743 | valid loss 8.2821 | valid bpc 11.9492 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 35 | 200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1746 | bpc 1.7033\n",
      "| epoch 35 | 400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1057 | bpc 1.6001\n",
      "| epoch 35 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0833 | bpc 1.5668\n",
      "| epoch 35 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0715 | bpc 1.5511\n",
      "| epoch 35 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0441 | bpc 1.5095\n",
      "| epoch 35 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1378 | bpc 1.6477\n",
      "| epoch 35 | 1400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.2002 | bpc 1.7371\n",
      "| epoch 35 | 1600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.5244 | bpc 2.2028\n",
      "| epoch 35 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1390 | bpc 1.6545\n",
      "| epoch 35 | 2000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0799 | bpc 1.5650\n",
      "| epoch 35 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1442 | bpc 1.6559\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 35 | time: 38.95s | train loss 1.1552 | train bpc 1.6724 | valid loss 8.2220 | valid bpc 11.8624 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 36 | 200/2222 batches | lr 0.0010 | ms/batch 16.78 | loss 1.1718 | bpc 1.6992\n",
      "| epoch 36 | 400/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.1047 | bpc 1.5988\n",
      "| epoch 36 | 600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0805 | bpc 1.5627\n",
      "| epoch 36 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0712 | bpc 1.5504\n",
      "| epoch 36 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0412 | bpc 1.5052\n",
      "| epoch 36 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1351 | bpc 1.6440\n",
      "| epoch 36 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.2015 | bpc 1.7390\n",
      "| epoch 36 | 1600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.5234 | bpc 2.2013\n",
      "| epoch 36 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1366 | bpc 1.6513\n",
      "| epoch 36 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0783 | bpc 1.5626\n",
      "| epoch 36 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1429 | bpc 1.6540\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 36 | time: 38.98s | train loss 1.1535 | train bpc 1.6700 | valid loss 7.8099 | valid bpc 11.2681 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 37 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1724 | bpc 1.7001\n",
      "| epoch 37 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1025 | bpc 1.5956\n",
      "| epoch 37 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0798 | bpc 1.5619\n",
      "| epoch 37 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0685 | bpc 1.5466\n",
      "| epoch 37 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0411 | bpc 1.5051\n",
      "| epoch 37 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1337 | bpc 1.6420\n",
      "| epoch 37 | 1400/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.2015 | bpc 1.7392\n",
      "| epoch 37 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5223 | bpc 2.1997\n",
      "| epoch 37 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1353 | bpc 1.6494\n",
      "| epoch 37 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0776 | bpc 1.5617\n",
      "| epoch 37 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1422 | bpc 1.6529\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 37 | time: 38.96s | train loss 1.1525 | train bpc 1.6686 | valid loss 7.9600 | valid bpc 11.4843 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 38 | 200/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1716 | bpc 1.6989\n",
      "| epoch 38 | 400/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.1022 | bpc 1.5953\n",
      "| epoch 38 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0772 | bpc 1.5582\n",
      "| epoch 38 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0685 | bpc 1.5466\n",
      "| epoch 38 | 1000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0395 | bpc 1.5028\n",
      "| epoch 38 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1324 | bpc 1.6402\n",
      "| epoch 38 | 1400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1995 | bpc 1.7361\n",
      "| epoch 38 | 1600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.5215 | bpc 2.1987\n",
      "| epoch 38 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1344 | bpc 1.6482\n",
      "| epoch 38 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0766 | bpc 1.5606\n",
      "| epoch 38 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1415 | bpc 1.6520\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 38 | time: 39.10s | train loss 1.1514 | train bpc 1.6671 | valid loss 8.4481 | valid bpc 12.1887 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 39 | 200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1708 | bpc 1.6978\n",
      "| epoch 39 | 400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0986 | bpc 1.5901\n",
      "| epoch 39 | 600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0781 | bpc 1.5594\n",
      "| epoch 39 | 800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0662 | bpc 1.5433\n",
      "| epoch 39 | 1000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0370 | bpc 1.4993\n",
      "| epoch 39 | 1200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1311 | bpc 1.6384\n",
      "| epoch 39 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1982 | bpc 1.7343\n",
      "| epoch 39 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.5189 | bpc 2.1950\n",
      "| epoch 39 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1326 | bpc 1.6458\n",
      "| epoch 39 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0765 | bpc 1.5601\n",
      "| epoch 39 | 2200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1397 | bpc 1.6495\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 39 | time: 38.90s | train loss 1.1500 | train bpc 1.6650 | valid loss 8.1316 | valid bpc 11.7320 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 40 | 200/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.1684 | bpc 1.6947\n",
      "| epoch 40 | 400/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0987 | bpc 1.5903\n",
      "| epoch 40 | 600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.0755 | bpc 1.5557\n",
      "| epoch 40 | 800/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.0663 | bpc 1.5437\n",
      "| epoch 40 | 1000/2222 batches | lr 0.0010 | ms/batch 16.91 | loss 1.0353 | bpc 1.4971\n",
      "| epoch 40 | 1200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1310 | bpc 1.6382\n",
      "| epoch 40 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1960 | bpc 1.7312\n",
      "| epoch 40 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5189 | bpc 2.1949\n",
      "| epoch 40 | 1800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1317 | bpc 1.6440\n",
      "| epoch 40 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0747 | bpc 1.5577\n",
      "| epoch 40 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1389 | bpc 1.6483\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 40 | time: 38.94s | train loss 1.1488 | train bpc 1.6633 | valid loss 8.5638 | valid bpc 12.3552 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 41 | 200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1687 | bpc 1.6950\n",
      "| epoch 41 | 400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0968 | bpc 1.5876\n",
      "| epoch 41 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0743 | bpc 1.5539\n",
      "| epoch 41 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0644 | bpc 1.5409\n",
      "| epoch 41 | 1000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0356 | bpc 1.4973\n",
      "| epoch 41 | 1200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1285 | bpc 1.6349\n",
      "| epoch 41 | 1400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1955 | bpc 1.7305\n",
      "| epoch 41 | 1600/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.5183 | bpc 2.1939\n",
      "| epoch 41 | 1800/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1302 | bpc 1.6423\n",
      "| epoch 41 | 2000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0747 | bpc 1.5578\n",
      "| epoch 41 | 2200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1372 | bpc 1.6458\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 41 | time: 38.88s | train loss 1.1478 | train bpc 1.6620 | valid loss 8.0901 | valid bpc 11.6720 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 42 | 200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1663 | bpc 1.6915\n",
      "| epoch 42 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0970 | bpc 1.5878\n",
      "| epoch 42 | 600/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.0731 | bpc 1.5522\n",
      "| epoch 42 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0644 | bpc 1.5408\n",
      "| epoch 42 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0333 | bpc 1.4940\n",
      "| epoch 42 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1287 | bpc 1.6348\n",
      "| epoch 42 | 1400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1955 | bpc 1.7307\n",
      "| epoch 42 | 1600/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.5158 | bpc 2.1904\n",
      "| epoch 42 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1306 | bpc 1.6430\n",
      "| epoch 42 | 2000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0715 | bpc 1.5530\n",
      "| epoch 42 | 2200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1379 | bpc 1.6469\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 42 | time: 38.95s | train loss 1.1469 | train bpc 1.6606 | valid loss 8.3320 | valid bpc 12.0213 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 43 | 200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1657 | bpc 1.6906\n",
      "| epoch 43 | 400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0951 | bpc 1.5852\n",
      "| epoch 43 | 600/2222 batches | lr 0.0010 | ms/batch 16.94 | loss 1.0717 | bpc 1.5502\n",
      "| epoch 43 | 800/2222 batches | lr 0.0010 | ms/batch 16.93 | loss 1.0614 | bpc 1.5366\n",
      "| epoch 43 | 1000/2222 batches | lr 0.0010 | ms/batch 16.92 | loss 1.0333 | bpc 1.4939\n",
      "| epoch 43 | 1200/2222 batches | lr 0.0010 | ms/batch 16.90 | loss 1.1259 | bpc 1.6308\n",
      "| epoch 43 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1947 | bpc 1.7295\n",
      "| epoch 43 | 1600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.5158 | bpc 2.1904\n",
      "| epoch 43 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1291 | bpc 1.6406\n",
      "| epoch 43 | 2000/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0713 | bpc 1.5530\n",
      "| epoch 43 | 2200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1356 | bpc 1.6434\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 43 | time: 38.96s | train loss 1.1456 | train bpc 1.6588 | valid loss 8.0300 | valid bpc 11.5855 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 44 | 200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1638 | bpc 1.6878\n",
      "| epoch 44 | 400/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0940 | bpc 1.5837\n",
      "| epoch 44 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0718 | bpc 1.5502\n",
      "| epoch 44 | 800/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0592 | bpc 1.5335\n",
      "| epoch 44 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0323 | bpc 1.4925\n",
      "| epoch 44 | 1200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1257 | bpc 1.6304\n",
      "| epoch 44 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1944 | bpc 1.7292\n",
      "| epoch 44 | 1600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.5141 | bpc 2.1879\n",
      "| epoch 44 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1282 | bpc 1.6394\n",
      "| epoch 44 | 2000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0717 | bpc 1.5534\n",
      "| epoch 44 | 2200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1343 | bpc 1.6417\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 44 | time: 38.83s | train loss 1.1446 | train bpc 1.6574 | valid loss 7.9769 | valid bpc 11.5087 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 45 | 200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1632 | bpc 1.6871\n",
      "| epoch 45 | 400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0932 | bpc 1.5822\n",
      "| epoch 45 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0698 | bpc 1.5475\n",
      "| epoch 45 | 800/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0588 | bpc 1.5327\n",
      "| epoch 45 | 1000/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0309 | bpc 1.4905\n",
      "| epoch 45 | 1200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1243 | bpc 1.6285\n",
      "| epoch 45 | 1400/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1919 | bpc 1.7253\n",
      "| epoch 45 | 1600/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.5143 | bpc 2.1883\n",
      "| epoch 45 | 1800/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1267 | bpc 1.6369\n",
      "| epoch 45 | 2000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0702 | bpc 1.5511\n",
      "| epoch 45 | 2200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1354 | bpc 1.6433\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 45 | time: 38.86s | train loss 1.1436 | train bpc 1.6558 | valid loss 8.4064 | valid bpc 12.1283 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 46 | 200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1640 | bpc 1.6881\n",
      "| epoch 46 | 400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0909 | bpc 1.5788\n",
      "| epoch 46 | 600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0693 | bpc 1.5469\n",
      "| epoch 46 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0579 | bpc 1.5315\n",
      "| epoch 46 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0297 | bpc 1.4889\n",
      "| epoch 46 | 1200/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1239 | bpc 1.6279\n",
      "| epoch 46 | 1400/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1896 | bpc 1.7220\n",
      "| epoch 46 | 1600/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.5143 | bpc 2.1882\n",
      "| epoch 46 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1274 | bpc 1.6380\n",
      "| epoch 46 | 2000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0694 | bpc 1.5502\n",
      "| epoch 46 | 2200/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.1351 | bpc 1.6427\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 46 | time: 38.94s | train loss 1.1430 | train bpc 1.6550 | valid loss 8.4989 | valid bpc 12.2621 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 47 | 200/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1614 | bpc 1.6845\n",
      "| epoch 47 | 400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0918 | bpc 1.5803\n",
      "| epoch 47 | 600/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0696 | bpc 1.5469\n",
      "| epoch 47 | 800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.0557 | bpc 1.5286\n",
      "| epoch 47 | 1000/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0284 | bpc 1.4868\n",
      "| epoch 47 | 1200/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1249 | bpc 1.6294\n",
      "| epoch 47 | 1400/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1909 | bpc 1.7240\n",
      "| epoch 47 | 1600/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.5116 | bpc 2.1844\n",
      "| epoch 47 | 1800/2222 batches | lr 0.0010 | ms/batch 16.88 | loss 1.1266 | bpc 1.6370\n",
      "| epoch 47 | 2000/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.0683 | bpc 1.5487\n",
      "| epoch 47 | 2200/2222 batches | lr 0.0010 | ms/batch 16.89 | loss 1.1331 | bpc 1.6400\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 47 | time: 38.98s | train loss 1.1421 | train bpc 1.6538 | valid loss 8.4787 | valid bpc 12.2327 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 48 | 200/2222 batches | lr 0.0010 | ms/batch 16.79 | loss 1.1604 | bpc 1.6830\n",
      "| epoch 48 | 400/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.0892 | bpc 1.5765\n",
      "| epoch 48 | 600/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0675 | bpc 1.5443\n",
      "| epoch 48 | 800/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0556 | bpc 1.5281\n",
      "| epoch 48 | 1000/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.0279 | bpc 1.4862\n",
      "| epoch 48 | 1200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1227 | bpc 1.6262\n",
      "| epoch 48 | 1400/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1898 | bpc 1.7225\n",
      "| epoch 48 | 1600/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.5120 | bpc 2.1851\n",
      "| epoch 48 | 1800/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.1255 | bpc 1.6356\n",
      "| epoch 48 | 2000/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.0689 | bpc 1.5490\n",
      "| epoch 48 | 2200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1326 | bpc 1.6395\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 48 | time: 38.85s | train loss 1.1413 | train bpc 1.6526 | valid loss 8.3533 | valid bpc 12.0522 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 49 | 200/2222 batches | lr 0.0010 | ms/batch 16.82 | loss 1.1605 | bpc 1.6835\n",
      "| epoch 49 | 400/2222 batches | lr 0.0010 | ms/batch 16.81 | loss 1.0888 | bpc 1.5761\n",
      "| epoch 49 | 600/2222 batches | lr 0.0010 | ms/batch 16.87 | loss 1.0673 | bpc 1.5439\n",
      "| epoch 49 | 800/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0543 | bpc 1.5265\n",
      "| epoch 49 | 1000/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.0276 | bpc 1.4858\n",
      "| epoch 49 | 1200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1223 | bpc 1.6259\n",
      "| epoch 49 | 1400/2222 batches | lr 0.0010 | ms/batch 16.86 | loss 1.1893 | bpc 1.7217\n",
      "| epoch 49 | 1600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.5118 | bpc 2.1848\n",
      "| epoch 49 | 1800/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.1232 | bpc 1.6323\n",
      "| epoch 49 | 2000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0678 | bpc 1.5480\n",
      "| epoch 49 | 2200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1327 | bpc 1.6393\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 49 | time: 38.87s | train loss 1.1407 | train bpc 1.6518 | valid loss 8.5604 | valid bpc 12.3507 \n",
      "--------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch 50 | 200/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.1588 | bpc 1.6809\n",
      "| epoch 50 | 400/2222 batches | lr 0.0010 | ms/batch 16.80 | loss 1.0880 | bpc 1.5750\n",
      "| epoch 50 | 600/2222 batches | lr 0.0010 | ms/batch 16.85 | loss 1.0638 | bpc 1.5388\n",
      "| epoch 50 | 800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0550 | bpc 1.5273\n",
      "| epoch 50 | 1000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0258 | bpc 1.4834\n",
      "| epoch 50 | 1200/2222 batches | lr 0.0010 | ms/batch 16.83 | loss 1.1198 | bpc 1.6221\n",
      "| epoch 50 | 1400/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1887 | bpc 1.7209\n",
      "| epoch 50 | 1600/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.5107 | bpc 2.1830\n",
      "| epoch 50 | 1800/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1238 | bpc 1.6330\n",
      "| epoch 50 | 2000/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.0659 | bpc 1.5452\n",
      "| epoch 50 | 2200/2222 batches | lr 0.0010 | ms/batch 16.84 | loss 1.1315 | bpc 1.6376\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch 50 | time: 38.92s | train loss 1.1394 | train bpc 1.6499 | valid loss 8.7622 | valid bpc 12.6419 \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.070 MB of 0.070 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0bced8aa2944587a6d5351c44267ab7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>train_bpc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_bpc</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>train_bpc</td><td>1.64993</td></tr><tr><td>train_loss</td><td>1.13942</td></tr><tr><td>val_bpc</td><td>12.64189</td></tr><tr><td>val_loss</td><td>8.7622</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-eon-26</strong> at: <a href='https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/58tc1msa' target=\"_blank\">https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/58tc1msa</a><br/> View project at: <a href='https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers' target=\"_blank\">https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240825_132733-58tc1msa/logs</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start Test Validation\n",
      "Evaluating batch 1/124\n",
      "Evaluating batch 2/124\n",
      "Evaluating batch 3/124\n",
      "Evaluating batch 4/124\n",
      "Evaluating batch 5/124\n",
      "Evaluating batch 6/124\n",
      "Evaluating batch 7/124\n",
      "Evaluating batch 8/124\n",
      "Evaluating batch 9/124\n",
      "Evaluating batch 10/124\n",
      "Evaluating batch 11/124\n",
      "Evaluating batch 12/124\n",
      "Evaluating batch 13/124\n",
      "Evaluating batch 14/124\n",
      "Evaluating batch 15/124\n",
      "Evaluating batch 16/124\n",
      "Evaluating batch 17/124\n",
      "Evaluating batch 18/124\n",
      "Evaluating batch 19/124\n",
      "Evaluating batch 20/124\n",
      "Evaluating batch 21/124\n",
      "Evaluating batch 22/124\n",
      "Evaluating batch 23/124\n",
      "Evaluating batch 24/124\n",
      "Evaluating batch 25/124\n",
      "Evaluating batch 26/124\n",
      "Evaluating batch 27/124\n",
      "Evaluating batch 28/124\n",
      "Evaluating batch 29/124\n",
      "Evaluating batch 30/124\n",
      "Evaluating batch 31/124\n",
      "Evaluating batch 32/124\n",
      "Evaluating batch 33/124\n",
      "Evaluating batch 34/124\n",
      "Evaluating batch 35/124\n",
      "Evaluating batch 36/124\n",
      "Evaluating batch 37/124\n",
      "Evaluating batch 38/124\n",
      "Evaluating batch 39/124\n",
      "Evaluating batch 40/124\n",
      "Evaluating batch 41/124\n",
      "Evaluating batch 42/124\n",
      "Evaluating batch 43/124\n",
      "Evaluating batch 44/124\n",
      "Evaluating batch 45/124\n",
      "Evaluating batch 46/124\n",
      "Evaluating batch 47/124\n",
      "Evaluating batch 48/124\n",
      "Evaluating batch 49/124\n",
      "Evaluating batch 50/124\n",
      "Evaluating batch 51/124\n",
      "Evaluating batch 52/124\n",
      "Evaluating batch 53/124\n",
      "Evaluating batch 54/124\n",
      "Evaluating batch 55/124\n",
      "Evaluating batch 56/124\n",
      "Evaluating batch 57/124\n",
      "Evaluating batch 58/124\n",
      "Evaluating batch 59/124\n",
      "Evaluating batch 60/124\n",
      "Evaluating batch 61/124\n",
      "Evaluating batch 62/124\n",
      "Evaluating batch 63/124\n",
      "Evaluating batch 64/124\n",
      "Evaluating batch 65/124\n",
      "Evaluating batch 66/124\n",
      "Evaluating batch 67/124\n",
      "Evaluating batch 68/124\n",
      "Evaluating batch 69/124\n",
      "Evaluating batch 70/124\n",
      "Evaluating batch 71/124\n",
      "Evaluating batch 72/124\n",
      "Evaluating batch 73/124\n",
      "Evaluating batch 74/124\n",
      "Evaluating batch 75/124\n",
      "Evaluating batch 76/124\n",
      "Evaluating batch 77/124\n",
      "Evaluating batch 78/124\n",
      "Evaluating batch 79/124\n",
      "Evaluating batch 80/124\n",
      "Evaluating batch 81/124\n",
      "Evaluating batch 82/124\n",
      "Evaluating batch 83/124\n",
      "Evaluating batch 84/124\n",
      "Evaluating batch 85/124\n",
      "Evaluating batch 86/124\n",
      "Evaluating batch 87/124\n",
      "Evaluating batch 88/124\n",
      "Evaluating batch 89/124\n",
      "Evaluating batch 90/124\n",
      "Evaluating batch 91/124\n",
      "Evaluating batch 92/124\n",
      "Evaluating batch 93/124\n",
      "Evaluating batch 94/124\n",
      "Evaluating batch 95/124\n",
      "Evaluating batch 96/124\n",
      "Evaluating batch 97/124\n",
      "Evaluating batch 98/124\n",
      "Evaluating batch 99/124\n",
      "Evaluating batch 100/124\n",
      "Evaluating batch 101/124\n",
      "Evaluating batch 102/124\n",
      "Evaluating batch 103/124\n",
      "Evaluating batch 104/124\n",
      "Evaluating batch 105/124\n",
      "Evaluating batch 106/124\n",
      "Evaluating batch 107/124\n",
      "Evaluating batch 108/124\n",
      "Evaluating batch 109/124\n",
      "Evaluating batch 110/124\n",
      "Evaluating batch 111/124\n",
      "Evaluating batch 112/124\n",
      "Evaluating batch 113/124\n",
      "Evaluating batch 114/124\n",
      "Evaluating batch 115/124\n",
      "Evaluating batch 116/124\n",
      "Evaluating batch 117/124\n",
      "Evaluating batch 118/124\n",
      "Evaluating batch 119/124\n",
      "Evaluating batch 120/124\n",
      "Evaluating batch 121/124\n",
      "Evaluating batch 122/124\n",
      "Evaluating batch 123/124\n",
      "Evaluating batch 124/124\n",
      "================================================================================\n",
      "| End of training | test loss 8.19334888458252 | test bpc 11.820731163024902 \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Language Modelling with Transformers\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"batch_size\": 0.02,\n",
    "    \"seq_length\": 128,\n",
    "    \"d_m\": 64,\n",
    "    \"num_heads\": 4,\n",
    "    \"num_layers_list\": 1,\n",
    "    \"ff_widening_factor\": 4,\n",
    "    \"LR\": 2e-3,\n",
    "    \"dropout_rate\": 0.1\n",
    "    }\n",
    ")\n",
    "\n",
    "best_val_bpc = np.inf\n",
    "best_params = None\n",
    "VOCAB_SIZE = vocab_size\n",
    "\n",
    "#checkpoint_dir = \"checkpoints\"\n",
    "#if not os.path.exists(checkpoint_dir):\n",
    "#    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(1, MAX_STEPS + 1):\n",
    "    print(\"Start Training\")\n",
    "    start_time = time.time()\n",
    "    train_losses = []\n",
    "    train_bpcs = []\n",
    "\n",
    "    for batch_idx in range(len(train_dataset)):\n",
    "        key, train_key = jax.random.split(key)\n",
    "        batch = next(train_dataset)\n",
    "        params, optimizer_state, train_loss, train_bpc = train_step(\n",
    "            params, optimizer_state, batch, train_key, lm.apply, optimizer.update)\n",
    "        train_losses.append(train_loss)\n",
    "        train_bpcs.append(train_bpc)\n",
    "\n",
    "        # Log intermediate training progress every 200 batches\n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            avg_train_loss = jnp.mean(jnp.array(train_losses[-200:]))\n",
    "            avg_train_bpc = jnp.mean(jnp.array(train_bpcs[-200:]))\n",
    "            elapsed_time = (time.time() - start_time) / (batch_idx + 1) * 1000\n",
    "            print(f\"| epoch {epoch} | {batch_idx + 1}/{len(train_dataset)} batches | \"\n",
    "                  f\"lr {LR:.4f} | ms/batch {elapsed_time:.2f} | loss {avg_train_loss:.4f} | \"\n",
    "                  f\"bpc {avg_train_bpc:.4f}\")\n",
    "\n",
    "    # Average training loss and BPC for the epoch\n",
    "    avg_train_loss = jnp.mean(jnp.array(train_losses))\n",
    "    avg_train_bpc = jnp.mean(jnp.array(train_bpcs))\n",
    "\n",
    "    # Validation Phase\n",
    "    val_losses = []\n",
    "    val_bpcs = []\n",
    "    for batch_idx in range(len(val_dataset)):\n",
    "      val_batch = next(val_dataset)\n",
    "      mask = jnp.tril(jnp.ones((val_batch['input'].shape[1], val_batch['input'].shape[1])))\n",
    "      key, val_key = jax.random.split(key)\n",
    "      val_loss, val_bpc = validation_step(params, val_batch, mask, val_key, lm.apply)\n",
    "      val_losses.append(val_loss)\n",
    "      val_bpcs.append(val_bpc)\n",
    "\n",
    "    # Average validation loss and BPC for the epoch\n",
    "    avg_val_loss = jnp.mean(jnp.array(val_losses))\n",
    "    avg_val_bpc = jnp.mean(jnp.array(val_bpcs))\n",
    "\n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    # Log end of epoch results\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    print(f\"| end of epoch {epoch} | time: {epoch_time:.2f}s | train loss {avg_train_loss:.4f} | train bpc {avg_train_bpc:.4f} | \"\n",
    "          f\"valid loss {avg_val_loss:.4f} | valid bpc {avg_val_bpc:.4f} \")\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    wandb.log({\n",
    "               \"train_loss\": avg_train_loss,\n",
    "               \"train_bpc\": avg_train_bpc,\n",
    "               \"val_loss\": avg_val_loss,\n",
    "               \"val_bpc\": avg_val_bpc,\n",
    "               \"epoch\": epoch,\n",
    "               \"lr\": LR,\n",
    "               })\n",
    "\n",
    "    # Store best hyperparameters\n",
    "    if val_bpc < best_val_bpc:\n",
    "        best_val_bpc = val_bpc\n",
    "        best_params = params  # Store the best model parameters\n",
    "        # Save the best parameters and validation BPC\n",
    "        #checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.npz\")\n",
    "        #with open(checkpoint_path, 'wb') as f:\n",
    "        #    np.savez(f, params=best_params, val_bpc=best_val_bpc)\n",
    "        #print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "wandb.finish()\n",
    "\n",
    "lm = LM(num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            d_m=d_m,\n",
    "            vocab_size=vocab_size,\n",
    "            ff_widening_factor=ff_widening_factor,\n",
    "            dropout_rate = dropout_rate,\n",
    "            training=False,\n",
    "            pre_norm = pre_norm,\n",
    "            tie_weights=tie_weights\n",
    "      )\n",
    "print(f\"Start Test Validation\")\n",
    "# Reinitialize test dataset with the best batch size and sequence length\n",
    "test_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.test\", batch_size, seq_length)\n",
    "\n",
    "# Evaluate on all batches in the test dataset\n",
    "test_losses = []\n",
    "test_bpcs = []\n",
    "for batch_idx in range(len(test_dataset)):\n",
    "    print(f\"Evaluating batch {batch_idx + 1}/{len(test_dataset)}\")\n",
    "    test_batch = next(test_dataset)\n",
    "    mask = jnp.tril(jnp.ones((test_batch['input'].shape[1], test_batch['input'].shape[1])))\n",
    "    key, test_key = jax.random.split(key)\n",
    "    test_loss, test_bpc = validation_step(best_params, test_batch, mask, test_key, lm.apply)\n",
    "    test_losses.append(test_loss)\n",
    "    test_bpcs.append(test_bpc)\n",
    "\n",
    "# Compute average test loss and BPC across all batches\n",
    "avg_test_loss = jnp.mean(jnp.array(test_losses))\n",
    "avg_test_bpc = jnp.mean(jnp.array(test_bpcs))\n",
    "\n",
    "# Final Test BPC and Loss log\n",
    "print(\"================================================================================\")\n",
    "print(f\"| End of training | test loss {avg_test_loss} | test bpc {avg_test_bpc} \")\n",
    "print(\"================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961,
     "referenced_widgets": [
      "60e558c3b13b4fc99f527e14ce554089",
      "aba42ab7002142f4882be1c57a64dd1c",
      "d7f2fd2804df4f729c7b2f65d265896d",
      "018d646821754a829ecf4a6624004470",
      "cd344acc4d824165990cfe0fb71129c3",
      "43ed9c9ba4df4fbeadc1dbc2cca11e2b",
      "dddce3f2fb2848ab8412575ae48510e7",
      "b00b705168ff43ee9d5203a25bb7ecbb"
     ]
    },
    "id": "yx8lPbLOQjya",
    "outputId": "76ab4be2-a1d2-4c46-a02c-f6f14e4d0504"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpc\n",
      "\tbpc              \t (min:    2.779, max:    8.004, cur:    2.779)\n",
      "Loss\n",
      "\tloss             \t (min:    1.444, max:    5.549, cur:    1.554)\n",
      "epoch: 4800, loss: 1.553591251373291, bpc: 2.77935791015625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e558c3b13b4fc99f527e14ce554089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.016 MB of 0.016 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>train_bpc</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4800</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>train_bpc</td><td>2.77936</td></tr><tr><td>train_loss</td><td>1.55359</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flowing-oath-12</strong> at: <a href='https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/pakodlrx' target=\"_blank\">https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers/runs/pakodlrx</a><br/> View project at: <a href='https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers' target=\"_blank\">https://wandb.ai/uctresearch/Language%20Modelling%20with%20Transformers</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240822_171145-pakodlrx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Language Modelling with Transformers\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"batch_size\": 0.02,\n",
    "    \"seq_length\": 128,\n",
    "    \"d_m\": 64,\n",
    "    \"num_heads\": 4,\n",
    "    \"num_layers_list\": 1,\n",
    "    \"ff_widening_factor\": 4,\n",
    "    \"LR\": 2e-3,\n",
    "    \"dropout_rate\": 0.1\n",
    "    }\n",
    ")\n",
    "\n",
    "plotlosses = PlotLosses()\n",
    "\n",
    "MAX_STEPS = 5000\n",
    "LOG_EVERY = 200\n",
    "losses = []\n",
    "bpcs = []\n",
    "VOCAB_SIZE = train_dataset.vocab_size\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(MAX_STEPS):\n",
    "    batch = next(train_dataset)\n",
    "    params, optimizer_state, loss, bpc = train_step(\n",
    "        params, optimizer_state, batch, key, lm.apply, optimizer.update)\n",
    "    losses.append(loss)\n",
    "    bpcs.append(bpc)\n",
    "    if epoch % LOG_EVERY == 0:\n",
    "        loss_ = jnp.array(losses).mean()\n",
    "        bpc_ = jnp.array(bpcs).mean()\n",
    "        plotlosses.update(\n",
    "            {\n",
    "                \"loss\": loss_,\n",
    "                 \"bpc\": bpc_,\n",
    "            }\n",
    "        )\n",
    "        wandb.log({\n",
    "            \"train_loss\": loss_,\n",
    "            \"train_bpc\": bpc_,\n",
    "            \"epoch\": epoch,\n",
    "            \"lr\": LR,\n",
    "            })\n",
    "        plotlosses.send()\n",
    "        losses = []\n",
    "        print(f\"epoch: {epoch}, loss: {loss_}, bpc: {bpc_}\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vuOISzyNR5c",
    "outputId": "980fcd1d-b749-4095-c25d-b5da2f11ba50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating batch 1/124\n",
      "Evaluating batch 2/124\n",
      "Evaluating batch 3/124\n",
      "Evaluating batch 4/124\n",
      "Evaluating batch 5/124\n",
      "Evaluating batch 6/124\n",
      "Evaluating batch 7/124\n",
      "Evaluating batch 8/124\n",
      "Evaluating batch 9/124\n",
      "Evaluating batch 10/124\n",
      "Evaluating batch 11/124\n",
      "Evaluating batch 12/124\n",
      "Evaluating batch 13/124\n",
      "Evaluating batch 14/124\n",
      "Evaluating batch 15/124\n",
      "Evaluating batch 16/124\n",
      "Evaluating batch 17/124\n",
      "Evaluating batch 18/124\n",
      "Evaluating batch 19/124\n",
      "Evaluating batch 20/124\n",
      "Evaluating batch 21/124\n",
      "Evaluating batch 22/124\n",
      "Evaluating batch 23/124\n",
      "Evaluating batch 24/124\n",
      "Evaluating batch 25/124\n",
      "Evaluating batch 26/124\n",
      "Evaluating batch 27/124\n",
      "Evaluating batch 28/124\n",
      "Evaluating batch 29/124\n",
      "Evaluating batch 30/124\n",
      "Evaluating batch 31/124\n",
      "Evaluating batch 32/124\n",
      "Evaluating batch 33/124\n",
      "Evaluating batch 34/124\n",
      "Evaluating batch 35/124\n",
      "Evaluating batch 36/124\n",
      "Evaluating batch 37/124\n",
      "Evaluating batch 38/124\n",
      "Evaluating batch 39/124\n",
      "Evaluating batch 40/124\n",
      "Evaluating batch 41/124\n",
      "Evaluating batch 42/124\n",
      "Evaluating batch 43/124\n",
      "Evaluating batch 44/124\n",
      "Evaluating batch 45/124\n",
      "Evaluating batch 46/124\n",
      "Evaluating batch 47/124\n",
      "Evaluating batch 48/124\n",
      "Evaluating batch 49/124\n",
      "Evaluating batch 50/124\n",
      "Evaluating batch 51/124\n",
      "Evaluating batch 52/124\n",
      "Evaluating batch 53/124\n",
      "Evaluating batch 54/124\n",
      "Evaluating batch 55/124\n",
      "Evaluating batch 56/124\n",
      "Evaluating batch 57/124\n",
      "Evaluating batch 58/124\n",
      "Evaluating batch 59/124\n",
      "Evaluating batch 60/124\n",
      "Evaluating batch 61/124\n",
      "Evaluating batch 62/124\n",
      "Evaluating batch 63/124\n",
      "Evaluating batch 64/124\n",
      "Evaluating batch 65/124\n",
      "Evaluating batch 66/124\n",
      "Evaluating batch 67/124\n",
      "Evaluating batch 68/124\n",
      "Evaluating batch 69/124\n",
      "Evaluating batch 70/124\n",
      "Evaluating batch 71/124\n",
      "Evaluating batch 72/124\n",
      "Evaluating batch 73/124\n",
      "Evaluating batch 74/124\n",
      "Evaluating batch 75/124\n",
      "Evaluating batch 76/124\n",
      "Evaluating batch 77/124\n",
      "Evaluating batch 78/124\n",
      "Evaluating batch 79/124\n",
      "Evaluating batch 80/124\n",
      "Evaluating batch 81/124\n",
      "Evaluating batch 82/124\n",
      "Evaluating batch 83/124\n",
      "Evaluating batch 84/124\n",
      "Evaluating batch 85/124\n",
      "Evaluating batch 86/124\n",
      "Evaluating batch 87/124\n",
      "Evaluating batch 88/124\n",
      "Evaluating batch 89/124\n",
      "Evaluating batch 90/124\n",
      "Evaluating batch 91/124\n",
      "Evaluating batch 92/124\n",
      "Evaluating batch 93/124\n",
      "Evaluating batch 94/124\n",
      "Evaluating batch 95/124\n",
      "Evaluating batch 96/124\n",
      "Evaluating batch 97/124\n",
      "Evaluating batch 98/124\n",
      "Evaluating batch 99/124\n",
      "Evaluating batch 100/124\n",
      "Evaluating batch 101/124\n",
      "Evaluating batch 102/124\n",
      "Evaluating batch 103/124\n",
      "Evaluating batch 104/124\n",
      "Evaluating batch 105/124\n",
      "Evaluating batch 106/124\n",
      "Evaluating batch 107/124\n",
      "Evaluating batch 108/124\n",
      "Evaluating batch 109/124\n",
      "Evaluating batch 110/124\n",
      "Evaluating batch 111/124\n",
      "Evaluating batch 112/124\n",
      "Evaluating batch 113/124\n",
      "Evaluating batch 114/124\n",
      "Evaluating batch 115/124\n",
      "Evaluating batch 116/124\n",
      "Evaluating batch 117/124\n",
      "Evaluating batch 118/124\n",
      "Evaluating batch 119/124\n",
      "Evaluating batch 120/124\n",
      "Evaluating batch 121/124\n",
      "Evaluating batch 122/124\n",
      "Evaluating batch 123/124\n",
      "Evaluating batch 124/124\n",
      "================================================================================\n",
      "| End of training | test loss 10.253360748291016 | test bpc 14.880392074584961 \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "lm = LM(num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            d_m=d_m,\n",
    "            vocab_size=vocab_size,\n",
    "            ff_widening_factor=ff_widening_factor,\n",
    "            dropout_rate = dropout_rate,\n",
    "            training=False,\n",
    "            pre_norm = pre_norm,\n",
    "            tie_weights=tie_weights\n",
    "      )\n",
    "\n",
    "# Reinitialize test dataset with the best batch size and sequence length\n",
    "test_dataset = CharacterBasedAsciiDatasetForLLM(\"nchlt_text.xh.test\", batch_size, seq_length)\n",
    "\n",
    "# Evaluate on all batches in the test dataset\n",
    "test_losses = []\n",
    "test_bpcs = []\n",
    "for batch_idx in range(len(test_dataset)):\n",
    "    print(f\"Evaluating batch {batch_idx + 1}/{len(test_dataset)}\")\n",
    "    test_batch = next(test_dataset)\n",
    "    mask = jnp.tril(jnp.ones((test_batch['input'].shape[1], test_batch['input'].shape[1])))\n",
    "    key, test_key = jax.random.split(key)\n",
    "    test_loss, test_bpc = validation_step(params, test_batch, mask, test_key, lm.apply)\n",
    "    test_losses.append(test_loss)\n",
    "    test_bpcs.append(test_bpc)\n",
    "\n",
    "# Compute average test loss and BPC across all batches\n",
    "avg_test_loss = jnp.mean(jnp.array(test_losses))\n",
    "avg_test_bpc = jnp.mean(jnp.array(test_bpcs))\n",
    "\n",
    "# Final Test BPC and Loss log\n",
    "print(\"================================================================================\")\n",
    "print(f\"| End of training | test loss {avg_test_loss} | test bpc {avg_test_bpc} \")\n",
    "print(\"================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZg-DgHWyMww"
   },
   "source": [
    "# Advanced **Extensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wd6YmF1byO8Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "018d646821754a829ecf4a6624004470": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43ed9c9ba4df4fbeadc1dbc2cca11e2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60e558c3b13b4fc99f527e14ce554089": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aba42ab7002142f4882be1c57a64dd1c",
       "IPY_MODEL_d7f2fd2804df4f729c7b2f65d265896d"
      ],
      "layout": "IPY_MODEL_018d646821754a829ecf4a6624004470"
     }
    },
    "aba42ab7002142f4882be1c57a64dd1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd344acc4d824165990cfe0fb71129c3",
      "placeholder": "",
      "style": "IPY_MODEL_43ed9c9ba4df4fbeadc1dbc2cca11e2b",
      "value": "0.016 MB of 0.016 MB uploaded\r"
     }
    },
    "b00b705168ff43ee9d5203a25bb7ecbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cd344acc4d824165990cfe0fb71129c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7f2fd2804df4f729c7b2f65d265896d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dddce3f2fb2848ab8412575ae48510e7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b00b705168ff43ee9d5203a25bb7ecbb",
      "value": 1
     }
    },
    "dddce3f2fb2848ab8412575ae48510e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0bced8aa2944587a6d5351c44267ab7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0fdba02052c04155a7b12fb6cb508100",
       "IPY_MODEL_3130f876ded9491ca19ac8473689699a"
      ],
      "layout": "IPY_MODEL_5eb13287d30e4028995c478a98f40c5e"
     }
    },
    "0fdba02052c04155a7b12fb6cb508100": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cff8736081da46409ad6bb7bd44613d5",
      "placeholder": "",
      "style": "IPY_MODEL_8692dbbd46154f7284d65336e548786d",
      "value": "0.070 MB of 0.070 MB uploaded\r"
     }
    },
    "3130f876ded9491ca19ac8473689699a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_557a034fc53b4acba623ebfffbdd6ef6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab134c0566fd46d7b0eee5d7b2d44ca0",
      "value": 1
     }
    },
    "5eb13287d30e4028995c478a98f40c5e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cff8736081da46409ad6bb7bd44613d5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8692dbbd46154f7284d65336e548786d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "557a034fc53b4acba623ebfffbdd6ef6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab134c0566fd46d7b0eee5d7b2d44ca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
